[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Riccardo Cappuzzo",
    "section": "",
    "text": "Data Science, Machine Learning, Python"
  },
  {
    "objectID": "posts/about-me/intro-blog.html",
    "href": "posts/about-me/intro-blog.html",
    "title": "Something about this blog",
    "section": "",
    "text": "What is this even about?\nThe idea behind the blog is having a single place where I can put posts on stuff that interests me, or things I have learned over time, or weird side projects I embark in for various reasons. I am also doing this as a writing exercise, and mostly for fun. I am not planning to turn this website into something I dedicate a lot of effort to.\nSome of the subjects I might cover include: * Complications encountered at work, developing, doing research, or anything else. * Interesting experimental results. * Posts about small programming stuff I do in my free time. * Updates on my publications. * Miscellaneous nagging.\n\n\nHow often are you going to update the blog?\nI have no idea at the moment. It’ll depend on how often I find something I’d like to write a post about. Depending on how it goes, this may take months, weeks, days, or just never happen again.\n\n\nTell me about yourself\nCheck the About page for a brief story of my recent life."
  },
  {
    "objectID": "posts/data_preparation/fetch-lastfm.html",
    "href": "posts/data_preparation/fetch-lastfm.html",
    "title": "Fetching and preparing Last.fm data for beautiful data visualizations",
    "section": "",
    "text": "One of the things I enjoy the most about my job is having the opportunity of turning data into images, coming up with ways to tell myself and others a story.\nThe story of this post (and a few more down the line) will revolve around the history of my musical taste: favorite artists, favorite tracks, artists loved then forgotten, and new discoveries that stuck around. It’s also a story about timeseries, which is a type of data I haven’t worked on a lot yet, so I took it as an opportunity to learn something new with it."
  },
  {
    "objectID": "posts/data_preparation/fetch-lastfm.html#how-to-get-your-own-musical-history",
    "href": "posts/data_preparation/fetch-lastfm.html#how-to-get-your-own-musical-history",
    "title": "Fetching and preparing Last.fm data for beautiful data visualizations",
    "section": "How to get your own musical history",
    "text": "How to get your own musical history\nHow am I going to do this? To begin with, I need a lot of data, ideally spanning multiple years and tracking anything I might need to cook up some nice figures. In comes my Last.fm account: the place where (most of) the tracks I’ve played over the past 10 years or so can be found.\nIf you’re not familiar with Last.fm, it’s a website that keeps track of the music you listen to on most streaming services (or music players): each track played is recorded as a scrobble, which is simply a record that includes when the track was being played, the track name, and some additional information about it (the artist, album etc.).\nLast.fm then uses that information to give cool stats about your listening habits, and to recommend artists or events that may interest you. I quite like to track my tracks (pun intended), but I’m not very interested in the recommendation. I found out about the service at the end of 2012, and I have been using it ever since, across many generations of devices and music players. As a result, as of the writing of this post I have I have about 124 thousand scrobbles, which is a lot of data to play with.\nSo, at some point I got an idea: what if I get all that data out of the account, and I use it as a sandbox for preparing figures?\nWell, the first thing to figure out was getting that data out, which can usually be done by either requesting for my data (thanks GDPR), or by looking up the API. In this case, the API was freely available and quite easy to use.\n\n\n\n\n\n\nTip\n\n\n\nOut of curiosity, I tried to get my Spotify data, which goes back about as long, but dumping it and sifting through it I couldn’t find anywhere as much information as I could with Last.fm, which is why I stuck with the latter instead.\n\n\nLuckily, I found an online tool made by Last.fm user ghan64, that saved me the need to write my own script for dumping my data.\nThe tool was very convenient, and I got all my listening history in a few minutes. A good starting point, but then I wondered: what if I need additional information? After all, the data I have contains only the timestamp, the names of the song, the artist and the album, and their respective MusicBrainz ID if avabilable (I won’t be going into the MBID in this post, but it may come into play later). No genres or user tags are available, for example, and there is a lot of missing data.\nTo fill in that missing data, I decided to use the Last.fm API myself so that I could make use of all the information that is available on the website, and add it to my own collection of records."
  },
  {
    "objectID": "posts/data_preparation/fetch-lastfm.html#requesting-info-with-the-last.fm-api",
    "href": "posts/data_preparation/fetch-lastfm.html#requesting-info-with-the-last.fm-api",
    "title": "Fetching and preparing Last.fm data for beautiful data visualizations",
    "section": "Requesting info with the Last.fm API",
    "text": "Requesting info with the Last.fm API\nFor the time being, my requesting code is extremely simple: all it does is handling the request generation of the data by providing a handful of functions that send different requests to the API, and converting the resulting data to either json or Python dictionaries.\nMy requesting.py starts by loading the API key for my account from a file I have on disk, then it sets the constants that are used to contatct the API entry point. If you want to follow along, you will have to prepare your own API key: it’s a very quick process and all it takes is having a Last.fm account.\nimport pprint\nimport requests\nimport json\n\ndef get_api_key():\n    with open(\"api-key.id\", \"r\") as file:\n        api_key = file.read().strip()\n    return api_key\napi_key = get_api_key()\napi_url = \"http://ws.audioscrobbler.com/2.0\" \napi_method = \"GET\"  \nThen, I need a requesting function that can send a well-formed request to the API entry point. The parameters needed for the request are saved in params, and depend on the specific call.\ndef send_api_request(url, method='GET', headers=None, params=None, data=None):\n    try:\n        # Send the request to the API\n        response = requests.request(method, url, headers=headers, params=params, json=data)\n        # Raise an exception if the request was unsuccessful\n        response.raise_for_status()\n        # Parse the JSON response\n        json_response = response.json()\n        return json_response\n\n    except requests.exceptions.HTTPError as http_err:\n        print(f\"HTTP error occurred: {http_err}\")\n    except requests.exceptions.RequestException as err:\n        print(f\"Error occurred: {err}\")\n    except json.JSONDecodeError:\n        print(\"Error decoding JSON response\")\nThe send_api_request method is used by the other methods, whose only function is preparing the parameters for the request:\n# Fetch information about an artist given their name\ndef get_artist_data(artist_name):\n    params = {\n        \"artist\": artist_name,\n        \"method\": \"artist.getinfo\",\n        \"api_key\": get_api_key(),\n        \"format\": \"json\"\n    }\n    response = send_api_request(api_url, method=api_method, headers=None, params=params)\n    return response\n# Fetch information about a track given the name and the artist\ndef get_track_data(track_name, artist_name):\n    params = {\n        \"track\": track_name,\n        \"artist\": artist_name,\n        \"method\": \"track.getinfo\",\n        \"api_key\": get_api_key(),\n        \"format\": \"json\"\n    }\n    response = send_api_request(api_url, method=api_method, headers=None, params=params)\n    return response\n\n# Given a user, get information about all the top artists\ndef get_user_top_artists(user_name):\n    params = {\n        \"user\": user_name,\n        \"method\": \"user.gettopartists\",\n        \"api_key\": get_api_key(),\n        \"format\": \"json\"\n    }\n    response = send_api_request(api_url, method=api_method, headers=None, params=params)\n    return response\n\n\n\n\n\n\nImportant\n\n\n\nI do not want to flood the API entry point with requests, so I added a delay to the scripts to avoid hitting rate limits.\n\n\nThe main use I’ve had so far consisted in fetching information about the artists: I am interested particularly in the tags fields, because I can use them to extract genres, which are not available in the main dump of scrobbles. That information will come into play in later posts.\nimport json\nimport time\n\nimport polars as pl\nfrom tqdm import tqdm\n\nfrom src.requesting import get_artist_data\n\ndf= pl.read_csv(\"recent-tracks.csv\")\nall_artists = df[\"artist\"].unique().to_list()\n\nartist_data = []\nfor idx, a in tqdm(enumerate(all_artists), total=len(all_artists), desc=\"Fetching artist data\"):\n    # Adding a delay to avoid hitting the API rate limit\n    if idx % 50 == 0:\n        tqdm.write(f\"Sleeping for 1 second to avoid hitting the API rate limit...\")\n        time.sleep(1)\n    try:\n        _data = get_artist_data(a)\n        artist_data.append(_data)\n    except Exception as e:\n        print(f\"Error fetching data for {a}: {e}\")\n\nwith open(\"artist_data.json\", \"w\") as f:\n    json.dump(artist_data, f)\nPutting all this code together, I now have a barebones set of methods that allow to fetch additional info from the API in case I need it. If I need to, I can add more methods to access more API functions."
  },
  {
    "objectID": "posts/data_preparation/fetch-lastfm.html#to-conclude",
    "href": "posts/data_preparation/fetch-lastfm.html#to-conclude",
    "title": "Fetching and preparing Last.fm data for beautiful data visualizations",
    "section": "To conclude…",
    "text": "To conclude…\nThis post was an introduction to my Last.fm plotting project. I figured some context would be needed before moving on to actually using the data, which is what I will be talking about in the next post in this series.\nTo summarize:\n\nI want to explore the 13 years worth of data stored in my Last.fm account through plotting and timeseries.\nTo do this, I downloaded all my data using a user script, and now I have access to the list of all my scrobbles (i.e., the tracks I played over time).\nFinally, I wrote a set of functions that will help me fetching additional information from the Last.fm API if I need it.\n\nThat’s it for this post! Thanks for reading along.\nIn the next post, I will study and plot my top artists, and I’ll explore a bunch of techniques I’ve seen in other people’s visualizations."
  },
  {
    "objectID": "posts/embdi/intro-embdi.html",
    "href": "posts/embdi/intro-embdi.html",
    "title": "Introducing EmbDI",
    "section": "",
    "text": "Out of all the work I’ve carried out during my PhD, EmbDI is by far the most important contribution. I’ve dedicated about half of PhD years working on designing, developing and implementing the system, and I can say I have been awarded with a lot of satisfaction from the result. Indeed, there is a lot to talk about here, so I’ll be writing a series of posts where I’ll go into more detail than it’s reasonably needed to get into “the weeds” of what it was like to work on EmbDI. This post will act as introduction and as index for later posts."
  },
  {
    "objectID": "posts/embdi/intro-embdi.html#what-is-embdi",
    "href": "posts/embdi/intro-embdi.html#what-is-embdi",
    "title": "Introducing EmbDI",
    "section": "What is EmbDI?",
    "text": "What is EmbDI?\nEmbDI stands for Embeddings for Data Integration, and is a system that generates embeddings for tabular data (specifically, CSV tables) and then uses said embeddings for performing Entity Resolution and Schema Matching. It was developed during my PhD as a solution to the problem of automating those data curation procedures and does so remarkably well, beating previous state of the art systems by a pretty large margin.\nWhile the ideas behind the implementation of the code were developed together my supervisor (Prof. Paolo Papotti), and another researcher from a different institution (Dr. Saravanan Thirumuruganathan), I wrote the vast majority of the code alone. This, of course, means that the code was a mess, then it got a bit better as I kept on hammering on it to clean up bugs and make it available to other researchers. The code has its repository on github, together with a very in-depth readme and instructions on how to run it.\nEmbDI is written in Python and relies on the usual data science libraries in Pandas, Numpy and sklearn, as well as the gensim library for training the embeddings themselves.\nThe code has been thoroughly tested to produce results for a paper that made it to SIGMOD 2020, which can be found here. The slides used for the SIGMOD 2020 presentation are also available here."
  },
  {
    "objectID": "posts/embdi/intro-embdi.html#how-does-embdi-work-in-short",
    "href": "posts/embdi/intro-embdi.html#how-does-embdi-work-in-short",
    "title": "Introducing EmbDI",
    "section": "How does EmbDI work, in short?",
    "text": "How does EmbDI work, in short?\nEmbDI is a Python-based data integration system that takes as input relational tables as CSV files, and returns as output a list of matches of tuples that represent the same entity (a task I’ll call Entity Resolution), or a list of matches between columns that represent the same attribute (Schema Matching).\n\n\n\npng\n\n\nThe procedure followed by EmbDI is the following:\n\nTake a pair of relational tables, then clean them to remove problematic strings and characters.\nConvert the relational tables into a graph.\nTraverse the graph by using random walks, then gather the random walks in a training corpus (that is, a text file that contains all the random walks).\nFeed the training corpus to the word2vec embeddings training algorithm.\nUse the geometric properties of the embeddings to find related tuples and columns.\n\nThis is, of course, extremely summarized. I would like to explore how EmbDI works more in depth over the course of a series of blog posts that will go into far more detail than what I’ll do here, so I’ll make sure to link back to those posts once they’re ready."
  },
  {
    "objectID": "posts/bns/post-bns-intro.html",
    "href": "posts/bns/post-bns-intro.html",
    "title": "Theorycrafting for Blade and Soul: introduction and damage formula",
    "section": "",
    "text": "Videogames are my biggest passion, and trying to play well is something I do whenever I enjoy a game. For me, minmaxing is part of the fun.\nNot all games can be optimized in the same way. In a lot of cases, it’s a combination of multiple factors, from mechanics to theorycrafting. As you might imagine, this post will focus on the latter."
  },
  {
    "objectID": "posts/bns/post-bns-intro.html#what-is-this-about",
    "href": "posts/bns/post-bns-intro.html#what-is-this-about",
    "title": "Theorycrafting for Blade and Soul: introduction and damage formula",
    "section": "What is this about?",
    "text": "What is this about?\nThe idea behind this post and the next posts in this series is trying to optimize how I play a specific game, and describing how I did it. For this series I will be talking about a MMORPG called Blade & Soul NEO. The original Blade and Soul is almost 10 years old, but a “Classic” version was released a few weeks ago, and after clocking more than 4000 hours on the original version, I decided to give the revamp a try.\nOverall, I have been enjoying my time, and as a result I started looking into ways to improve my gameplay. In particular, I looked into how I could improve how fast I can kill bosses in PvE (Player vs Environment). Unsurprisingly, part of the solution is just getting better equipment. However, this got me thinking: how can I optimize the equipment I have to deal as much damage to bosses as I can?\nOne thing led to another, and I ended up writing code that would generate all possible combinations of equipment sets to find the best. I had a lot of fun working on it, and I thought it might be interesting, so I decided to write about it in these posts.\nSo, maybe you’re interested in the game itself, or in how I used Python, Google Docs, and various AI tools to maximize my deepz. Or maybe you’re just interested in the ramblings of a mad man that clearly has way too much time on his hands. In any case, please follow along and dive into the rabbit hole with me!"
  },
  {
    "objectID": "posts/bns/post-bns-intro.html#the-basics-how-does-equipment-affect-a-character",
    "href": "posts/bns/post-bns-intro.html#the-basics-how-does-equipment-affect-a-character",
    "title": "Theorycrafting for Blade and Soul: introduction and damage formula",
    "section": "The basics: how does equipment affect a character?",
    "text": "The basics: how does equipment affect a character?\nLike in many other games, character attributes (or stats) in Blade and Soul are given by a combination of base stats and additional stats. Base stats depend on the character’s class, level and some other sources, and remain mostly constant over time. On the other hand, additional stats are provided by a character’s equipment, and can change very easily simply by swapping different equipment pieces.\nAttributes can be either offensive, or defensive: the first category includes Attack Power (AP), Accuracy, Critical Rate, and Critical Damage; the second HP, Defense, Evasion, and some more. I will focus mostly on the offensive stats, as defensive stats are not as interesting to optimize, and are often treated as “dump stats”.\n\n\n\nA screenshot showing different pieces of equipment.\n\n\nEquipment in BnS can be divided broadly in weapon and accessories, soulshields and badges. All items can have one or more attributes, whose value will be added directly to the base stats of the character.\n\n\n\nA screenshot showing an earring and its stats.\n\n\nThis earring, for example, has 40 Accuracy and 76 Critical Rate (let’s ignore all the other stats for now). By equipping it, the Accuracy and Critical Rate of my character will increase by 40 and 76 respectively.\nSoulshields provide additional attributes, which are added to the base stats like other equipment. For example, this soulshield gives HP (increasing how much damage my character can take), as well as Critical Damage and Defense. \nEach piece of equipment (or soulshield slice) is generated with a random set of statistics, and a random amount for each statistic. As a result, a player may collect a large number of copies of the same piece, and need to choose which one to use. This is particularly important for soulshields, because it is very easy to obtain their copies:\n\n\n\nMultiple soulshield slices\n\n\nGiven this context, the problem I am try to solve is the following:\n\n\n\n\n\n\nImportant\n\n\n\nI have many copies of each accessory, and a huge number of possible combinations of accessories. How can I find the best set of equipment to use to maximize the damage my character deals to bosses?\n\n\nTo start working towards a solution, I first need to define how damage is dealt, and how to integrate that in my problem formulation, so that’s what the next section is for."
  },
  {
    "objectID": "posts/bns/post-bns-intro.html#damage-in-blade-and-soul-why-and-how",
    "href": "posts/bns/post-bns-intro.html#damage-in-blade-and-soul-why-and-how",
    "title": "Theorycrafting for Blade and Soul: introduction and damage formula",
    "section": "Damage in Blade and Soul: why and how",
    "text": "Damage in Blade and Soul: why and how\nIn PvE, the objective is dealing damage to enemy bosses to reduce their HP to 0 before they do the same to the player. In fact, the main gameplay loop involves killing bosses, getting their loot, improving equipment with the loot, then challenging stronger bosses for even better loot.\nTypically, boss fights require players hit the bosses as hard as possible, while executing a set of mechanics (avoiding the boss’ attacks, activating items in the arena etc.). If a player is executing everything correctly, then better attributes lead to better damage. In general, having more damage makes fights easier. Furthermore, some bosses have an enrage timer that will immediately kill the player if they cannot eliminate the boss in time.\nI have been talking about damage to bosses all this time, and it’s now time to define how damage is calculated. For this, I will be using a damage formula that was shared in the community, and that goes as follows: \\[\nDamage = AP \\times (Crit\\% \\times CDmg + (1-Crit\\%))\n\\]\nThe Attack Power (AP) is the main offensive stat, as all damage is multiplied by this value; additionally, skills multiply this value by some modifier that depends on which skill is being used. \\(Crit\\%\\) is the probability to land a “Critical hit”: a Critical hit deals additional damage equal to \\(AP\\times CDmg\\). Since not all hits are critical hits, the “Damage” is given by the damage of critical hits weighted by how frequently they occur, in addition to the damage of hits that do not crit.\nAs an example, let’s consider \\(AP=500\\), \\(Crit\\%=55\\%\\), \\(CDmg=150\\%\\): the equation would become \\[\nDamage = 500 \\times (0.55 \\times 1.5 + (1 - 0.55)) = 687.5\n\\]\nIn a vacuum, a character with 687.5 AP and 0 \\(Crit\\%\\) would deal the same damage as a character 500 AP, 55% of crit chance, and 150% of bonus damage for crits. For simplicity, I am ignoring all sorts of class interactions with crits, and just taking the raw numbers.\nClearly, better Crit% and CDmg would result in a larger value for Damage, and therefore better DPS: in practice, those are the main stats that player try to maximize.\nHowever, what we see on a character’s profile is a raw number for AP, Critical rate and Critical damage. How can we turn those into the multipliers used above? Well, while there are no official numbers for this, the community managed to get an estimate of the formula (source), which is what I am using here: \\[\nCrit\\% = \\frac{Crit \\times 96.98979}{Crit + 1124.069}\n\\] where \\(Crit\\) is the Critical rate attribute. \\[\nCDmg = \\frac{CD \\times 290.8}{CD + 2102.36} + 125\n\\] where \\(CD\\) is the Critical damage attribute.\nUsually, players strive to maximize Attack Power, Critical rate and Critical damage. The problem then becomes understanding how these statistics affect the final damage output, and selecting the equipment that leads to the best returns.\nIn conclusion, now I have a (rough) estimation of how much damage my character deals given a certain amount of AP, Critical rate, and Critical damage. While it is not perfect, it’s still a good starting point to optimize for."
  },
  {
    "objectID": "posts/bns/post-bns-intro.html#to-summarize-and-whats-next",
    "href": "posts/bns/post-bns-intro.html#to-summarize-and-whats-next",
    "title": "Theorycrafting for Blade and Soul: introduction and damage formula",
    "section": "To summarize, and what’s next",
    "text": "To summarize, and what’s next\nThis was a lot! Let’s wrap up:\n\nI am trying to optimize the equipment of my character in the game Blade and Soul.\nMy objective is maximizing the amount of damage I deal to enemy bosses.\nI need a metric that lets me whether a set of equipment is good or not.\nI can use datamined formulas to estimate my damage, and use that as as my “objective function”.\n\nNow that I have a formula, I have a better idea of how to rank sets of equipments, and now I need to figure out how to generate all the combinations. This and some more subjects I’d like to cover include:\n\nDrawing figures that use the damage formulas to draw some preliminary conclusions from.\nWriting some software that can select the best set of soulshields given a list of pieces.\nFiguring out a reliable way of getting the soulshield statistics.\nRe-implementing the code I wrote in Python using a different programming language to exercise.\n\nIf you got this far, I hope you found the post interesting, and thanks a lot for reading!"
  },
  {
    "objectID": "posts/posts.html",
    "href": "posts/posts.html",
    "title": "Posts",
    "section": "",
    "text": "Experiments plotting my top Last.fm artists\n\n\n\n\n\n\n\n\nSep 4, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nEuroSciPy 2025: thinking back on the event and my tutorial\n\n\n\n\n\n\n\n\nAug 26, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nSODA Kickoff: a new slide deck for skrub\n\n\n\nnews\n\n\n\n\n\n\n\n\n\nJun 3, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nNew publication: Retrieve, Merge, Predict\n\n\n\nacademia\n\nnews\n\n\n\n\n\n\n\n\n\nMay 19, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nFetching and preparing Last.fm data for beautiful data visualizations\n\n\n\ncode\n\ndata visualization\n\n\n\n\n\n\n\n\n\nMay 4, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nNew blog post: testing Skrub categorical encoders\n\n\n\nnews\n\nskrub\n\n\n\n\n\n\n\n\n\nMar 30, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nTheorycrafting for Blade and Soul: introduction and damage formula\n\n\n\ncode\n\ngames\n\nblade and soul\n\n\n\n\n\n\n\n\n\nMar 29, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n2024 wrapped: ups, downs, and some changes\n\n\n\nnews\n\n\n\n\n\n\n\n\n\nDec 30, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nMy 2024 (so far) in academia\n\n\n\n\n\n\n\n\n\n\n\n\n\nOct 8, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nMoving up… or sideways?\n\n\n\nnews\n\n\n\n\n\n\n\n\n\nSep 12, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nNew story on Medium: Retrieve, Merge, Predict\n\n\n\nnews\n\n\n\n\n\n\n\n\n\nSep 12, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nNew submission: Retrieve, Merge Predict at VLDB2024\n\n\n\nnews\n\n\n\n\n\n\n\n\n\nFeb 1, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nSome updates\n\n\n\nnews\n\n\n\n\n\n\n\n\n\nFeb 5, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nStudying the temperature readings\n\n\n\nraspberry\n\nIoT\n\n\n\n\n\n\n\n\n\nJul 16, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nCoding the Thermometer\n\n\n\nraspberry\n\n\n\n\n\n\n\n\n\nJul 11, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nPreparing a Raspberry-based thermometer setup\n\n\n\nraspberry\n\narduino\n\n\n\nThe device, and the accessories.\n\n\n\n\n\nJul 10, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nIntroducing EmbDI\n\n\n\nembdi\n\n\n\n\n\n\n\n\n\nApr 12, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nPublications\n\n\n\n\n\n\n\n\nApr 7, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nSomething about this blog\n\n\n\nIntroduction\n\n\n\n\n\n\n\n\n\nMar 29, 2022\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/raspberry/the_setup.html",
    "href": "posts/raspberry/the_setup.html",
    "title": "Preparing a Raspberry-based thermometer setup",
    "section": "",
    "text": "To begin with, let’s consider the material I am working with, to have an idea of what can be done with it and what might be needed to build something clever.\nFirst off, my board is a Raspberry Pi 2 Model B (2015), which features neither a Wifi nor a Bluetooth connection. This did cause me quite a few headaches, since connecting wireless turned out to be more complex than I thought.\nI am running the Pi using a completely unofficial power supply, which regularly leads the board to print a very ominous “Undervoltage detected” message on the CLI.\nI also own an Arduino Uno board, with which I am far less familiar as I cannot use Python to wrestle results out of it. I will still try to get it to work, and report it here.\nFor the Uno, I got an adapter for MicroSD cards, so that it would be possible to use the microcontroller to log sensor data. I also need to get a fairly small MicroSD card because the Uno cannot handle more than 5-ish GBs of storage.\nIn my plans, I wanted to access the Pi remotely via wifi using SSH, which is problematic as the model I am working with does not have an integrated Wifi antenna. For this reason, I went to an electronics store and bought the first USB Wifi antenna, without checking whether it would work on Linux devices. Turns out, it didn’t. Which means that I had to look for a different model, and ended up purchasing a (model-name)[model-link].\nSince the first idea I got was using the Pi as a thermometer, the first sensor I purchased was a DHT22 temperature-humidity sensor. The version I purchased is slightly different from what I have seen on online guides, but the functionality is the same.\nI am looking to purchase another Pi, but since I am still going to use it as a sensor base, I will be getting either a Pi Zero (whenever they become available again!), or a Pico."
  },
  {
    "objectID": "posts/news/euroscipy-2025.html",
    "href": "posts/news/euroscipy-2025.html",
    "title": "EuroSciPy 2025: thinking back on the event and my tutorial",
    "section": "",
    "text": "The week of EuroSciPy 2025 has just ended, and now it’s time to take stock of all the various talks I attended, those that I missed out on, and some other personal observations.\nIn this post, I will be going over my personal experience with the trip and the talk I gave on the first day.\n\nThe city, the venue, and the organization\nThis year, the conference was held in the AGH University of Kraków, Poland. This being my very first time in Poland, I had no idea what to expect. What I found was a beautiful city, great and well maintained infrastructure, and everyone speaking in English. At least the last part was a surprise and a nice change of pace, given my typical experience in France 🙈.\nAt the university we were spoiled with coffee, tea and snacks all day long. I must admit I did not love the food, but I believe that’s mostly to do with my own personal tastes rather than the actual quality.\nI was travelling with three other scikit-learn maintainers: Olivier Grisel, Guillaume Lemaitre and Loic Esteve. I’m thankful they had me tag along: it’s always interesting to listen in to their conversations, and I always learn a lot from just discussing with them.\nOverall, the organizers and volunteers were incredibly helpful, and I think everything went according to plan. At least, from what I could tell.\nMy main gripe with the whole event was that it was held in the week starting on the 18th of August, meaning that we had to travel right on the weekend where everyone goes back from vacation. Being on vacation myself, I did not have the time to prepare my talk as well as I wanted. I really hope that the dates for next year’s event will be better.\n\n\nThe social side\nIt was a great time! I met a lot of interesting and clever people. It was just the kind of geek collection that I’d expect from such a conference, and in general everyone was nice and fun to hang out with. I’m sure I’ll meet a lot of the same people at PyData Paris in a month, looking forwards to that.\nAlso, the social event was a blast and I ate way more pizza (and good pizza) than I had expected. Full marks on that too!\n\n\nMy tutorial: “Timeseries forecasting with skrub”\nYes, that’s right. I was a speaker this time! My first time doing a tutorial in a public conference, at that. Indeed, I was very nervous about it, and while I had a lot of people come and tell me it was great, I still felt it was not as good as it could have been, and I’ve been mulling over how to make it better for the next time. At least, I can say that I am thinking of the next time!\nThe material we used for the tutorial is available in this repository. It was adapted from the masterclass that Olivier and Guillaume prepared for Probabl. (the masterclass is available here), cutting down on a lot of material, and adding a few exercises for the tutorial.\nI feel like the main problem with this tutorial is that it had two complicated parts to it: preparing data for forecasting, and using the skrub Data Ops. In a tutorial like the one we did, this made the 90 minutes extremely dense and hard to follow. I even had the same problem when I followed the masterclass, and I knew what Data Ops were!\nI’m already planning to address that in the next version by just removing the forecasting altogether, and instead using a simpler example to force the audience to deal only with the Data Ops.\nAnother problem was with the exercises: they were too few and far inbetween, and they were way too complicated for the audience to actually solve in the limited time we gave them. A lot of complex operations that would require a bunch of digging in the examples and the documentation.\nI absolutely have to come up with simpler examples, and in general with things that anyone can do given the information provided up to that point. In most of the other tutorials the speakers let us play around with the software for much longer, which I think was a far better strategy than what we ended up doing.\nFinally, and this is entirely due to my nervousness and not having a clue what to say in a tutorial, I completely forgot to introduce myself, which was funny, but also not good. Thankfully, Guillaume covered for me when he did the second section of the tutorial.\nI’m still fairly happy with the outcome, but I still think we could have done better, and I’ll make sure the next version will be better.\nOh, and if you want to judge for yourself whether I’m being too harsh on myself, you can find the full thing on Youtube.\n\n\nConclusions\nOverall, it was a very positive experience: there were a lot of interesting talks, interesting people, and interesting packages. The trip went well and the city was very nice (I still hate travelling, but that’s just me). I finally presented my first tutorial, with somewhat mixed results (at least in my head…), so now I know what to do for the next. I’m really happy I went.\nStay tuned for the next post, where I’ll go over the talks I liked the most and share pointers and links to the material.\nCheers!"
  },
  {
    "objectID": "posts/news/moving-up.html",
    "href": "posts/news/moving-up.html",
    "title": "Moving up… or sideways?",
    "section": "",
    "text": "So, my postdoc is about to end (just a few more weeks left), and it seems like my next position has been decided! I will be a research engineer at P16, a development unit within Inria where I will be working on the Skrub Python library.\nIt’s a short term contract (one year), which means that in a few months I’ll have to go back to looking for a new job. For the time being, however, I’ll enjoy spending more time at Inria.\nI’ll miss Dataiku though, it’s a great company and I met some really nice people there, despite the fact I was there only for one or two days a week."
  },
  {
    "objectID": "posts/news/end-of-2024.html",
    "href": "posts/news/end-of-2024.html",
    "title": "2024 wrapped: ups, downs, and some changes",
    "section": "",
    "text": "It’s the end of the year, so I decided I had to do my semestral wrestling bout with Hugo to go over what happened in the past 12 months.\nTo start with, as I haven’t updated the website in months, I have been welcomed by an error message, which further complicates my relationship with this platform. Seriously though, every time I try to write something here I spend more time debugging Hugo than I do writing.\nOverall, most of the year was devoted to “the paper”, that one article I have been working on for the past two years. Said paper has been rejected three times (so far), and we are still working on it. Supposedly, we should be done with it this January, but who knows what the future actually holds.\nWhat else? Well, my contract as a post-doc ended, and now I am working as a research engineer at Inria, working on Skrub. Overall, I am interacting with a lot of the same people while working on a library that I find interesting and that still lets me run experiments.\nI should start looking for a new position soon enough, but first I’d like to get the paper done with.\nI went to PyData Paris 2024, which was a very interesting experience (though quite tiring, as I went there in the same period I was moving to a different place), meeting a lot of brilliant people in the field I would like to keep on working in.\nI would say that the year has been very stressful, with burnout waiting for me after pretty much every paper deadline, and some extremely disappointing experience in Academia. On the other hand, it was also a very instructive year as I feel like I learned a lot about data science, machine learning, note taking, proper software development, and some more. So, swings and roundabouts?\nIf I have to come up with a theme for 2025, I think it will be something along the lines of “read more sources”. I have been saving all sorts of scholarly articles, blog posts, guides, essays, books, and proceeded to read maybe 1% of the collection. My hope is that by the time this day rolls around next year, I’ll be able to look back and think that I was able to read what I save, rather than just hoard articles.\nThat said, see you around in the next update (whenever that may be, and if Hugo allows…). Happy 2025!"
  },
  {
    "objectID": "posts/news/medium-post.html",
    "href": "posts/news/medium-post.html",
    "title": "New story on Medium: Retrieve, Merge, Predict",
    "section": "",
    "text": "I wrote a blog post on Medium! It summarizes most of the work we did in a more digestible format than an academic paper.\nThe post is available here."
  },
  {
    "objectID": "posts/news/academia_experience.html",
    "href": "posts/news/academia_experience.html",
    "title": "My 2024 (so far) in academia",
    "section": "",
    "text": "Disclaimer: this post will mostly be a rant. The reasons for it should become sufficiently clear as I go over my experience in the past few months.\nFor context, I have been working on a paper for the past year or so. Said paper involved running a few thousands experimental configurations over different baselines, then reporting said experiments in a paper that was initially submitted to VLDB2024 (end of January this year). Said paper was rejected at VLDB 2024, submitted to SIGMOD2025, and was rejected again.\nOver the course of only a few months, I had the privilege of experiencing (very courteous) academic blackmailing, extremely negative reviews, burnout, overtime, and possibly the most bullshit rejection I have heard of.\nLet’s start from top, with the academic blackmail. So, VLDB is a single blind conference, which means that the authors are not aware of who the reviewers are, while the reviewers can instead see the names and affiliations of the authors.\nThis is less annoying to manage than double blind (which instead involve anonymizing everything), but has the fun side effect of receiving emails from the reviewers, who might nicely ask to please add their recent papers to yours in a completely non-threatening way by reaching out in private to one of the authors. Bonus points for asking to add references to papers whose code is not available to run and test.\nThis leads me directly into the rejection. I have my fair share of rejected papers – though by this point, most of my rejections are from this singular paper –, and I am keenly aware of how much of an impact “rolling the right reviewers” has on the final outcome. This rejection was especially fun, however, because it came only after a revision period during which I had to re-run most of the experiments (hence the burnout). But as a man once said, “I’m not done yet”: the reasoning for the rejection was that “we did not test a specific baseline, and said baseline works on my machine” (paraphrased). The kicker? The code for the baseline was not available online, and the reviewer did not provide it to us so that we could test it. As a result, the area chair agreed with it, and the paper was rejected after the revision step.\nWhat was even more of a kick in the nether regions was that we even managed to change the mind of the one reviewer that had marked his review as “weak reject, and I am not going to change my mind”. Unfortunately, we also managed to change the mind of one of the “weak accept” reviews to “reject”, and that somehow stuck.\nSomething interesting to note is that, soon after the paper was rejected, the arXiv version received a new citation. This citation was by one of the authors of the baseline. While I am not one to engage in conspirationsm, the coincidence was a bit too perfect for me to ignore.\nWhile I only briefly mentioned the burnout, it was bad. I got sick after every deadline, and I think I developed some degree of PTSD from looking at terminal screens waiting for experimental runs to end. Not the best, especially considering it did not result in success (yet, at least). That’s not good for morale either.\n\nLessons learned\nWhat is the summary of these experiences? 1) The current reviewing system is broken (which is something literally anyone in the field will confirm); 2) academia is rife with politics just like everything else; 3) effort is not likely to be rewarded, and your fate lies less in your hands and in your results, than in the ethereal roll of the dice that will assign a reviewer rather than another in any given round of paper submission.\nThe result? I will be looking for my next position in industry, rather than academia. Stay tuned for when I will write the equivalent post on my industry misgivings once I have enough experience in that."
  },
  {
    "objectID": "posts/news/blog-post-skrub-encoders.html",
    "href": "posts/news/blog-post-skrub-encoders.html",
    "title": "New blog post: testing Skrub categorical encoders",
    "section": "",
    "text": "I wrote a new blog post on the Skrub materials website!\nThis one is about finding out how to best encode categorical features using the categorical encoders provided by skrub.\nYou can find the post here."
  },
  {
    "objectID": "posts/news/retrieve-merge-predict-short.html",
    "href": "posts/news/retrieve-merge-predict-short.html",
    "title": "New publication: Retrieve, Merge, Predict",
    "section": "",
    "text": "Almost exactly two years in the making, our paper “Retrieve, Merge, Predict - Augmenting tables with data lakes” has finally been published in TMLR: you may find it here.\nI am planning to write a proper blog post to summarize the results, as well as a retrospective on the paper, just to reminisce on the effort it took to finally publish it."
  },
  {
    "objectID": "posts/news/soda-kickoff.html",
    "href": "posts/news/soda-kickoff.html",
    "title": "SODA Kickoff: a new slide deck for skrub",
    "section": "",
    "text": "Today we had the SODA Kickoff, an event organized by some of the PIs (I still don’t know what “PI” stands for, other than that it’s not “private investigator”) of the Inria team I am part of.\nIt was very interesting! I got to hear about what all the other post docs, PhD students, and researcher are working on, and I got to know some people I don’t often talk with.\nAs part of the event, I presented skrub (not too unexpected), and had to prepare a new slide deck to showcase the latest updates to the library. It was very well received, and a lot of people complimented me for the presentation, which is always great :D\nHere it is, together with the rest of the materials on our teaching website."
  },
  {
    "objectID": "posts/news/wherehavibeen.html",
    "href": "posts/news/wherehavibeen.html",
    "title": "Some updates",
    "section": "",
    "text": "It’s been a while, hasn’t it? According the newly added archives page, the latest post in this blog goes back all the way to July 2022… Well, it does not seem like I kept my promise of posting with some frequency.\nNo matter! Something did happen a few months ago (last October), and I have been quite busy because of it. I have moved to Paris! For the next two years (starting last October), I have been/will be working as a Postdoctoral Researcher for Inria Saclay in the SODA Team and at Dataiku in their research department.\nThis is pretty cool! I get to be a part of a strong research group which includes the guys that maintain scikit-learn, and I have contributed with some minor additions to dirtycat: this will definitely be a great learning experience as collaborative programming is something I have very little experience with.\nOn the other side, I get to work in a pretty cool company where everyone looks bright and prepared. That side of the experience will be useful to get an idea of how things work in an enterprise setting, and the kind of problems that enterprise people actually face. And, we get free breakfast every day!\nThus, I am currently splitting the time between these two realities and acting as the connecting thread between the two.\nSo, what is the subject of this research project? Well, we’re still trying to figure that part out. While we started by working on the creation of embeddings of tabular data, now we are moving on to the subject of testing the performance of systems for performing join suggestion. This will be quite the task, given my scarce experience on the subject. We will start from benchmarking a number of different methods, and just figuring that part out will be an adventure.\nI think that’s all for now.\nCheers!"
  },
  {
    "objectID": "posts/news/vldb_2024.html",
    "href": "posts/news/vldb_2024.html",
    "title": "New submission: Retrieve, Merge Predict at VLDB2024",
    "section": "",
    "text": "We submitted a revised version of our benchmarking paper to VLDB 2024, with the title “Retrieve, Merge, Predict: Augmenting Tables with Data Lakes”.\nThe preprint can be found on Arxiv, while the code has its own website, and is available on Github.\nWe also release YADL, the semi-synthetic benchmarking data lake that we used to run our experiments. The repository is also available on Github.\nUPDATE June 2024: the paper was rejected. It was extremely disappointing, and I will write a post to detail what happened. It goes without saying that I am not happy about this outcome."
  },
  {
    "objectID": "posts/raspberry/study-temperature.html",
    "href": "posts/raspberry/study-temperature.html",
    "title": "Studying the temperature readings",
    "section": "",
    "text": "In this notebook, I will take a look at the readings from a DHT22 temperature-humidity sensor that I have left for a few days in a couple of locations in my house. As I’m writing this, we’re in the middle of the summer and there’s a heatwave going on, so the average temperatures will be pretty high.\nBy gathering data over the course of different days and by varying some conditions (e.g. opening certain windows), I’m hoping to find what’s the best way of cooling down the house at night, and keeping it cool during the day.\n\n\nLet’s start by talking about the libraries. Nothing pretty fancy here. pandas is there to put all the log readings into a dataframe, matplotlib and seaborn are graph visualization libraries for plotting data and datetime is there because I am working with timeseries. matplotlib.dates is used to customize some of the plotting parameters when handling dates.\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport datetime\nimport matplotlib.dates as mdates\n\n\n\n\n\nThe temperature/humidity readings are stored in a csv file, together with the timestamp of the reading. Let’s start with the readings coming from the sensor left in the attic.\ndf = pd.read_csv('log.txt', header=0, names=['date','time', 'temperature', 'humidity'])\nThe next few commands are there for turning the datetime into a more manageable format, which helps with plotting later. In particular, since the date and time columns are read as strings, the following lines convert them back into datetime format.\nThen, the two are combined back into a singular datetime object for each row, with the entire timestamp.\ndf['time'] = df['time'].apply(datetime.time.fromisoformat)\ndf['date'] = df['date'].apply(datetime.date.fromisoformat)\n\ndf['datetime'] = pd.to_datetime([datetime.datetime.combine(a, b) for a,b in zip(df['date'], df['time'])])\nNow that I have a unique datetime index for every row, I can reset the index of the dataframe so that it uses the timestamp, rather than the regular ID.\nThe next step is resampling the timeline in order to reduce the number of readings, and to reduce the effect of anomalous readings. In this example, readings are grouped every 30 minutes, then they are aggregated by using the mean function.\nOut of curiosity, and following a procedure I am not completely sure of, I am trying to plot the variance of the measurement over the same interval of time by aggregating with var, then plotting using the fill_between function.\n# Resetting the index so that it uses the datetime, then aggregating according to mean and variance. \ndf_resampled = df.set_index('datetime').resample('30Min').mean()\n\ndf_var = df.set_index('datetime').resample('30Min').var()\nTime for the first plots. Here, I am plotting all the readings from the moment I activated the process, to the moment I saved it on disk to study the readings. To better distinguish between days, I am searching for the timestamps with time (0,0) (i.e. midnight), then I’m saving them in the variable midnight.\nThe command df_resampled.index.time==datetime.time(0,0) looks in the .time attribute of the datetime objects in the index and checks if the value contained therein is the same as datetime.time(0,0).\nThe first plot will show the timeseries of temperature and humidity, along with their (supposed) error bars. Since I’m using the same code for plotting both temperature and humidity, I wrote a simple function that takes as input the dataframe and the variable of interest, then prepares the resulting plot.\ndef plot_overall_var(df, tgt_var, df_var=None):\n\n    def get_bounds(df_base, df_var, column):\n        '''Preparing the upper and lower bounds by adding the variance\n        '''\n        y1 = df_base[column] + df_var[column]\n        y2 = df_base[column] - df_var[column]\n\n        return y1,y2\n\n    # Setting the size of the figure\n    fig = plt.figure(figsize=(16,4))\n\n    # Plotting the main line\n    ax = sns.lineplot(x=df_resampled.index, y=df_resampled[tgt_var])\n    plt.title(tgt_var)\n\n    # If provided, adding the variance\n    if df_var is not None:\n        yhigh, ylow = get_bounds(df_resampled, df_var, tgt_var)\n        plt.fill_between(df_resampled.index, ylow, yhigh, color='orange')\n\n    # Plotting vertical lines for highlighting the change of date\n    midnight = df_resampled.loc[df_resampled.index.time==datetime.time(0,0)].index\n    for day in midnight:\n        plt.axvline(day)\n        \n    # Plotting ticks every 3 hours\n    _ = ax.xaxis.set_major_locator(mdates.HourLocator(interval=3))\n    \n    # Rotating the ticks\n    ax.xaxis.set_tick_params(rotation=60)\n    \n    # Tightening the result\n    plt.tight_layout()\n\nplot_overall_var(df_resampled, 'temperature', df_var)\nplot_overall_var(df_resampled, 'humidity', df_var)\n \n\n\n\nIn the next section, I will reshape the data with the objective of comparing the readings gathered in different days, to look for potential similarities and differences. To do so, I’ll first split the datetime index into columns date and time, then I will pivot the table so that the new index will be time, and the columns will report temperature and humidity for each day. As you’ll notice, a bunch of values are missing and get with the value NaN: this is due to the fact that I started the data collection around 14h on July 13th.\ndf_resampled['date'] = df_resampled.index.date\ndf_resampled['time'] = df_resampled.index.time\n\ndf_pivot=df_resampled.pivot(index='time', columns='date')\n# Displaying only the first 10 rows\ndf_pivot.head(10)\n\n\n\n\n\n\n\n\ntemperature\n\n\nhumidity\n\n\n\n\ndate\n\n\n2022-07-13\n\n\n2022-07-14\n\n\n2022-07-15\n\n\n2022-07-16\n\n\n2022-07-13\n\n\n2022-07-14\n\n\n2022-07-15\n\n\n2022-07-16\n\n\n\n\ntime\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n00:00:00\n\n\nNaN\n\n\n29.640351\n\n\n31.483636\n\n\n31.187500\n\n\nNaN\n\n\n44.256141\n\n\n41.012727\n\n\n38.969643\n\n\n\n\n00:30:00\n\n\nNaN\n\n\n29.321428\n\n\n31.041071\n\n\n30.917544\n\n\nNaN\n\n\n44.341071\n\n\n40.332143\n\n\n38.903509\n\n\n\n\n01:00:00\n\n\nNaN\n\n\n29.025000\n\n\n30.600000\n\n\n30.540741\n\n\nNaN\n\n\n44.517857\n\n\n40.024561\n\n\n39.196297\n\n\n\n\n01:30:00\n\n\nNaN\n\n\n28.775000\n\n\n30.238182\n\n\n30.076786\n\n\nNaN\n\n\n44.776786\n\n\n39.936363\n\n\n39.655357\n\n\n\n\n02:00:00\n\n\nNaN\n\n\n28.519298\n\n\n29.961403\n\n\n29.866071\n\n\nNaN\n\n\n44.966667\n\n\n40.045614\n\n\n39.594642\n\n\n\n\n02:30:00\n\n\nNaN\n\n\n28.323214\n\n\n29.657143\n\n\n29.710527\n\n\nNaN\n\n\n44.798214\n\n\n39.983929\n\n\n40.035088\n\n\n\n\n03:00:00\n\n\nNaN\n\n\n28.083929\n\n\n29.340000\n\n\n29.500000\n\n\nNaN\n\n\n44.835714\n\n\n39.558182\n\n\n40.560714\n\n\n\n\n03:30:00\n\n\nNaN\n\n\n27.839286\n\n\n29.054386\n\n\n29.285714\n\n\nNaN\n\n\n45.023214\n\n\n39.131579\n\n\n42.167857\n\n\n\n\n04:00:00\n\n\nNaN\n\n\n27.514286\n\n\n28.794643\n\n\n29.000000\n\n\nNaN\n\n\n45.291072\n\n\n39.330357\n\n\n44.741072\n\n\n\n\n04:30:00\n\n\nNaN\n\n\n27.251786\n\n\n28.581818\n\n\n28.656364\n\n\nNaN\n\n\n45.671428\n\n\n40.038182\n\n\n45.830909\n\n\n\n\n\nHere I’m plotting the values of temperature and humidity recorded on each day.\ntime_range = pd.timedelta_range(start='1 day', end='2 days', freq='3H')\nfig, axs = plt.subplots(2,1, sharex=True, figsize=(10, 8))\ndf_pivot=df_resampled.pivot(index='time', columns='date')\na=df_pivot['temperature'].plot(title='Temperature', ylabel='Temperature (*C)', ax=axs[0], legend=True)\n\ndf_pivot=df_resampled.pivot(index='time', columns='date')\ndf_pivot['humidity'].plot(ax=axs[1], title='Humidity', ylabel='Humidity (%)', legend=True)\n\n# plt.legend([a.lines])\nplt.tight_layout()\nplt.savefig('temp-humid-attic.png', transparent=False, facecolor='white')\n\n\n\npng\n\n\n\n\n\nIn this section, I will be parsing the data and look for the daily extremes (maximum and minimum) for temperature and humidity, and the time at which they occurred.\n# Simple dictionary for formatting the unit of measurement that will be printed\nd_form = {'temperature': '*C', 'humidity': '%'}\n# Grouping by day\nfor idx, day_g in df_resampled.groupby('date'):\n    print(f'Date: {idx}')\n    \n#     Maximum\n#     idxmax returns the position of the maximum value\n    aggr_i = day_g[['temperature', 'humidity']].aggregate(pd.Series.idxmax)\n    aggr_m = day_g[['temperature', 'humidity']].aggregate(pd.Series.max)\n#     Putting the values together for pretty printing\n    df_t=pd.concat([aggr_i, aggr_m], axis=1)\n    for i, val in df_t.iterrows():\n        print(f'Max {i:&gt;12} = {val[1]:.1f} {d_form[i]:&gt;2} at {val[0].time()}')\n        \n#     Minimum\n#     idxmax returns the position of the maximum value\n    aggr_i = day_g[['temperature', 'humidity']].aggregate(pd.Series.idxmin)\n    aggr_m = day_g[['temperature', 'humidity']].aggregate(pd.Series.min)\n#     Putting the values together for pretty printing\n    df_t=pd.concat([aggr_i, aggr_m], axis=1)\n    for i, val in df_t.iterrows():\n        print(f'Min {i:&gt;12} = {val[1]:.1f} {d_form[i]:&gt;2} at {val[0].time()}')\nDate: 2022-07-13\nMax  temperature = 33.5 *C at 18:00:00\nMax     humidity = 49.8  % at 14:30:00\nMin  temperature = 30.0 *C at 23:30:00\nMin     humidity = 42.6  % at 18:30:00\nDate: 2022-07-14\nMax  temperature = 35.6 *C at 18:00:00\nMax     humidity = 47.4  % at 10:30:00\nMin  temperature = 26.4 *C at 07:30:00\nMin     humidity = 38.8  % at 19:00:00\nDate: 2022-07-15\nMax  temperature = 36.3 *C at 16:00:00\nMax     humidity = 43.7  % at 10:00:00\nMin  temperature = 27.6 *C at 07:00:00\nMin     humidity = 33.4  % at 16:00:00\nDate: 2022-07-16\nMax  temperature = 36.0 *C at 18:00:00\nMax     humidity = 51.1  % at 09:00:00\nMin  temperature = 27.7 *C at 07:30:00\nMin     humidity = 38.9  % at 00:30:00\n\n\n\n\nIn this section, I will be repeating most of what was done in Part 1: the main difference will be the location of the sensor. Rather than keeping it in the attic, in fact, I was keeping this on the desk in my bedroom. In particular, the sensor was exposed to the heat coming from a pretty chonky desktop PC, as well as two monitors, and it is positioned close to a window.\ndf = pd.read_csv('readings-room.csv', header=0, names=['date','time', 'temperature', 'humidity'])\ndf['time'] = df['time'].apply(datetime.time.fromisoformat)\ndf['date'] = df['date'].apply(datetime.date.fromisoformat)\n\ndf['datetime'] = pd.to_datetime([datetime.datetime.combine(a, b) for a,b in zip(df['date'], df['time'])])\n# Resetting the index so that it uses the datetime, then aggregating according to mean and variance. \ndf_resampled = df.set_index('datetime').resample('30Min').mean()\n\ndf_var = df.set_index('datetime').resample('30Min').var()\nBelow, I’m plotting the overall temperature and humidity readings. Unlike in the case of the attic, however, the variance is far more larger, which leads to a plot that is almost unreadable in the case of humidity. For this reason, I am also plotting the graphs without added bands by calling plot_overall_var without the df_var variable.\nplot_overall_var(df_resampled, 'temperature', df_var)\nplot_overall_var(df_resampled, 'humidity', df_var)\n\nplot_overall_var(df_resampled, 'temperature')\nplot_overall_var(df_resampled, 'humidity')\n\n\n\npng\n\n\n\n\n\npng\n\n\n\n\n\npng\n\n\n\n\n\npng\n\n\n\n\ndf_resampled['date'] = df_resampled.index.date\ndf_resampled['time'] = df_resampled.index.time\n\ndf_pivot=df_resampled.pivot(index='time', columns='date')\n# Displaying only the first 10 rows\ndf_pivot.head(10)\n\n\n\n\n\n\n\n\ntemperature\n\n\nhumidity\n\n\n\n\ndate\n\n\n2022-07-06\n\n\n2022-07-07\n\n\n2022-07-08\n\n\n2022-07-06\n\n\n2022-07-07\n\n\n2022-07-08\n\n\n\n\ntime\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n00:00:00\n\n\nNaN\n\n\nNaN\n\n\n24.601786\n\n\nNaN\n\n\nNaN\n\n\n60.225000\n\n\n\n\n00:30:00\n\n\nNaN\n\n\n26.792857\n\n\n23.985965\n\n\nNaN\n\n\n62.985715\n\n\n61.821053\n\n\n\n\n01:00:00\n\n\nNaN\n\n\n26.937255\n\n\n23.458929\n\n\nNaN\n\n\n62.368627\n\n\n63.941071\n\n\n\n\n01:30:00\n\n\nNaN\n\n\n26.825000\n\n\n22.831579\n\n\nNaN\n\n\n62.901786\n\n\n66.178947\n\n\n\n\n02:00:00\n\n\nNaN\n\n\n26.421053\n\n\n21.867273\n\n\nNaN\n\n\n63.638596\n\n\n69.143637\n\n\n\n\n02:30:00\n\n\nNaN\n\n\n26.396428\n\n\n22.728572\n\n\nNaN\n\n\n64.705357\n\n\n65.946429\n\n\n\n\n03:00:00\n\n\nNaN\n\n\n26.171930\n\n\n22.787719\n\n\nNaN\n\n\n64.521053\n\n\n64.991228\n\n\n\n\n03:30:00\n\n\nNaN\n\n\n25.918965\n\n\n22.864285\n\n\nNaN\n\n\n65.108621\n\n\n65.960714\n\n\n\n\n04:00:00\n\n\nNaN\n\n\n25.978571\n\n\n23.108929\n\n\nNaN\n\n\n64.525000\n\n\n66.139285\n\n\n\n\n04:30:00\n\n\nNaN\n\n\n25.794736\n\n\n23.110527\n\n\nNaN\n\n\n65.349123\n\n\n66.324561\n\n\n\n\n\nval_idxmax=df_pivot[['temperature', 'humidity']].apply(pd.Series.idxmax)\nval_max=df_pivot[['temperature', 'humidity']].apply(pd.Series.max)\ndf_concat = pd.concat([val_idxmax, val_max],  axis=1).reset_index().pivot(index='date', columns='level_0')\ndf_concat\n\n\n\n\n\n\n\n\n0\n\n\n1\n\n\n\n\nlevel_0\n\n\nhumidity\n\n\ntemperature\n\n\nhumidity\n\n\ntemperature\n\n\n\n\ndate\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2022-07-06\n\n\n18:30:00\n\n\n17:00:00\n\n\n67.537736\n\n\n28.174419\n\n\n\n\n2022-07-07\n\n\n07:30:00\n\n\n17:00:00\n\n\n70.470175\n\n\n28.641072\n\n\n\n\n2022-07-08\n\n\n05:00:00\n\n\n17:30:00\n\n\n70.805263\n\n\n27.100000\n\n\n\n\n\nmi = pd.MultiIndex.from_product([['humidity', 'temperature'], ['timeat', 'value']])\nrows=[]\nfor idx, row in df_concat.iterrows():\n    t = [row[idx][variable]  for variable in ['humidity', 'temperature'] for idx in [0,1] ]\n    rows.append([idx] + t)\ndf_new = pd.DataFrame.from_records(rows)\ndf_new=df_new.set_index(0)\ndf_new.index.rename('date', inplace=True)\ndf_new.columns = mi\ndf_new\n\n\n\n\n\n\n\n\nhumidity\n\n\ntemperature\n\n\n\n\n\n\ntimeat\n\n\nvalue\n\n\ntimeat\n\n\nvalue\n\n\n\n\ndate\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2022-07-06\n\n\n18:30:00\n\n\n67.537736\n\n\n17:00:00\n\n\n28.174419\n\n\n\n\n2022-07-07\n\n\n07:30:00\n\n\n70.470175\n\n\n17:00:00\n\n\n28.641072\n\n\n\n\n2022-07-08\n\n\n05:00:00\n\n\n70.805263\n\n\n17:30:00\n\n\n27.100000\n\n\n\n\n\ntime_range = pd.timedelta_range(start='1 day', end='2 days', freq='3H')\nfig, axs = plt.subplots(2,1, sharex=True, figsize=(10, 8))\ndf_pivot=df_resampled.pivot(index='time', columns='date')\na=df_pivot['temperature'].plot(title='Temperature', ylabel='Temperature (*C)', ax=axs[0], legend=True)\n\ndf_pivot=df_resampled.pivot(index='time', columns='date')\ndf_pivot['humidity'].plot(ax=axs[1], title='Humidity', ylabel='Humidity (%)', legend=True)\n\n# plt.legend([a.lines])\nplt.tight_layout()\nplt.savefig('temp-humid-bedroom.png', transparent=False, facecolor='white')\n\n\n\npng\n\n\n\n\n\n# Simple dictionary for formatting the unit of measurement that will be printed\nd_form = {'temperature': '*C', 'humidity': '%'}\n# Grouping by day\nfor idx, day_g in df_resampled.groupby('date'):\n    print(f'Date: {idx}')\n    \n#     Maximum\n#     idxmax returns the position of the maximum value\n    aggr_i = day_g[['temperature', 'humidity']].aggregate(pd.Series.idxmax)\n    aggr_m = day_g[['temperature', 'humidity']].aggregate(pd.Series.max)\n#     Putting the values together for pretty printing\n    df_t=pd.concat([aggr_i, aggr_m], axis=1)\n    for i, val in df_t.iterrows():\n        print(f'Max {i:&gt;12} = {val[1]:.1f} {d_form[i]:&gt;2} at {val[0].time()}')\n        \n#     Minimum\n#     idxmax returns the position of the maximum value\n    aggr_i = day_g[['temperature', 'humidity']].aggregate(pd.Series.idxmin)\n    aggr_m = day_g[['temperature', 'humidity']].aggregate(pd.Series.min)\n#     Putting the values together for pretty printing\n    df_t=pd.concat([aggr_i, aggr_m], axis=1)\n    for i, val in df_t.iterrows():\n        print(f'Min {i:&gt;12} = {val[1]:.1f} {d_form[i]:&gt;2} at {val[0].time()}')\nDate: 2022-07-06\nMax  temperature = 28.2 *C at 17:00:00\nMax     humidity = 67.5  % at 18:30:00\nMin  temperature = 26.7 *C at 21:30:00\nMin     humidity = 60.8  % at 22:00:00\nDate: 2022-07-07\nMax  temperature = 28.6 *C at 17:00:00\nMax     humidity = 70.5  % at 07:30:00\nMin  temperature = 24.4 *C at 23:00:00\nMin     humidity = 58.5  % at 23:30:00\nDate: 2022-07-08\nMax  temperature = 27.1 *C at 17:30:00\nMax     humidity = 70.8  % at 05:00:00\nMin  temperature = 21.7 *C at 05:00:00\nMin     humidity = 60.2  % at 00:00:00\n\n\n\n\nLet’s wrap up the post by summarizing what was shown, then making some observations.\nOverall, this was an interesting exercise to play around with timeseries, which is a format I haven’t really considered during my PhD. I had to figure out how to handle them using the functions provided by pandas, then how to use matplotlib to prepare the graphs while getting around the shortcomings of my data-gathering setup. In particular, seaborn was far more stubborn than I expected and I wasn’t able to find a proper way of wrangling the correct plots with the library.\nRegarding the actual data, it’s interesting to see the differences between the readings coming from the two locations. They were gathered over different time periods, and it was far hotter while I was recording the temperature in the attic: this makes direct comparisons not very meaningful, due to how different the external conditions were. Still, the difference in readings between the room and the attic can be explained by the fact that there is no activity in the attic; on the other hand, the sensor in the bedroom picked up on both my activity there: this involved the use of a desktop PC and multiple monitors and additional devices, coupled with the fact that windows can generate a noticeable breeze that goes through the entire top floor of the building. This goes to show that a good ventilato helps a lot with cooling down a hot interior, and that electronic devices work directly against that!\nI’m still working on building a different temperature sensor, and possibly something more complex that would allow to gather further information (a weather station maybe?), so I will definitely come back to this subject at some point in the future.\n\n\n\nIn this section I’m gathering some additional methods I tried while working on the points shown above. I’m adding these both as a note to myself of what doesn’t work, and for readers to see what can happen while trying stuff out.\nFrom my understanding, part of the reason why these methods did not work quite as well as I hoped is due to the fact that I’m working with datetime objects, rather than “regular” indexing. This results in some funky behavior in some cases.\n\n\nThe pandas.DataFrame.melt function allows to obtain a “long form” version of a dataframe that pivots around some columns, while keeping other columns as variables. This can be extremely useful for plotting different variables on the same plot using libraries such as seaborn, which is what I will be trying in a moment.\ndf_melted = df_resampled.melt(id_vars=['time', 'date'], value_vars=['temperature'])\ndf_melted['time'] = df_melted['time'].apply(datetime.time.isoformat)\ndisplay(df_melted)\n\n# df_melted = df_melted.sort_values(['date', 'time'])\n\n\n\n\n\n\n\n\ntime\n\n\ndate\n\n\nvariable\n\n\nvalue\n\n\n\n\n\n\n0\n\n\n14:30:00\n\n\n2022-07-13\n\n\ntemperature\n\n\n31.266667\n\n\n\n\n1\n\n\n15:00:00\n\n\n2022-07-13\n\n\ntemperature\n\n\n32.048214\n\n\n\n\n2\n\n\n15:30:00\n\n\n2022-07-13\n\n\ntemperature\n\n\n32.547368\n\n\n\n\n3\n\n\n16:00:00\n\n\n2022-07-13\n\n\ntemperature\n\n\n32.926316\n\n\n\n\n4\n\n\n16:30:00\n\n\n2022-07-13\n\n\ntemperature\n\n\n33.198245\n\n\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n\n\n147\n\n\n16:00:00\n\n\n2022-07-16\n\n\ntemperature\n\n\n35.548214\n\n\n\n\n148\n\n\n16:30:00\n\n\n2022-07-16\n\n\ntemperature\n\n\n35.810526\n\n\n\n\n149\n\n\n17:00:00\n\n\n2022-07-16\n\n\ntemperature\n\n\n35.903573\n\n\n\n\n150\n\n\n17:30:00\n\n\n2022-07-16\n\n\ntemperature\n\n\n35.987719\n\n\n\n\n151\n\n\n18:00:00\n\n\n2022-07-16\n\n\ntemperature\n\n\n36.000000\n\n\n\n\n\n152 rows × 4 columns\n\n\nA “melted” dataframe can be plotted fairly easily using a command such as the one below. Besides the x- and y-axes, it is possible to distinguish the lines by hue (in this case, different colors for different dates) and by style (different line style for different variables). Here, it does not make a lot of sense to plot humidity and temperature in the same plot, but it is useful for the sake of the example.\nSomething else to note in the plot below is that the x-axis starts at 14:30, then continues for 24 hours until 14:00 the next day: this is due to the fact that the data captured by the log is incomplete for the day of 2022-07-13. After searching around for a while, I wasn’t able to reset the ticks on the x-axis to make them work properly, so I had to look for some workarounds.\nIndeed, the current plot shows a pretty major inconsistency at 0:00, which is when the date shifts from July 13th to July 14th, which is also why the orange line “jumps” below and seems to be a continuation of the blue line: this is exactly what that is!\nfig = plt.figure(figsize=(12,4))\nax = sns.lineplot(data = df_melted, x='time', y='value', hue='date', style='variable')\nax.tick_params(rotation=90)\n\n\n\npng\n\n\nIt is possible to build a plot by using the full datetime object, rather than splitting by day, and the result is shown below:\ndf_melted = df_resampled.reset_index().melt(id_vars=['datetime'], value_vars=['temperature'])\ndisplay(df_melted)\n\nfig = plt.figure(figsize=(12,4))\nax = sns.lineplot(data = df_melted, x='datetime', y='value')\nax.tick_params(rotation=90)\n\n\n\n\n\n\n\n\ndatetime\n\n\nvariable\n\n\nvalue\n\n\n\n\n\n\n0\n\n\n2022-07-13 14:30:00\n\n\ntemperature\n\n\n31.266667\n\n\n\n\n1\n\n\n2022-07-13 15:00:00\n\n\ntemperature\n\n\n32.048214\n\n\n\n\n2\n\n\n2022-07-13 15:30:00\n\n\ntemperature\n\n\n32.547368\n\n\n\n\n3\n\n\n2022-07-13 16:00:00\n\n\ntemperature\n\n\n32.926316\n\n\n\n\n4\n\n\n2022-07-13 16:30:00\n\n\ntemperature\n\n\n33.198245\n\n\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n\n\n147\n\n\n2022-07-16 16:00:00\n\n\ntemperature\n\n\n35.548214\n\n\n\n\n148\n\n\n2022-07-16 16:30:00\n\n\ntemperature\n\n\n35.810526\n\n\n\n\n149\n\n\n2022-07-16 17:00:00\n\n\ntemperature\n\n\n35.903573\n\n\n\n\n150\n\n\n2022-07-16 17:30:00\n\n\ntemperature\n\n\n35.987719\n\n\n\n\n151\n\n\n2022-07-16 18:00:00\n\n\ntemperature\n\n\n36.000000\n\n\n\n\n\n152 rows × 3 columns\n\n\n\n\n\npng\n\n\nNow, this plot looks much clearer, however it’s not exactly what I wanted. For this reason, I decided to get rid of the offending data series (i.e. the incomplete series of readings from the first day) and plot the data for the remaining days. This isn’t really the best way of doing it, which is why I went for the solution in the first section of this post, but it gets the job done at least.\nSo, now my objective is selecting only the readings that start after the first day has ended, which can be done using either one of the two commands below, whose output is exactly the same.\ndf_resampled.index returns the index of the dataframe, which is in datetime format: this allows to compare it with a different datetime object that I am declaring (either fromisoformat(...) or date(...)). The comparison allows to find the datetime indices that occur after the given value, and keep only those.\ndf_1 =df_resampled.loc[df_resampled.index &gt;= datetime.datetime.fromisoformat('2022-07-14 00:00:00')]\n\ndf_2 =df_resampled.loc[df_resampled.index.date &gt; datetime.date(2022, 7,13)]\n\n# Checking that the two indices are equal\nall(df_1.index == df_2.index)\nTrue\nNow that there is a cleaner dataframe to plot, the usual lineplot function can be used to prepare the temperature and humidity comparisons. |\ndf_melted = df_1.melt(id_vars=['time', 'date'], value_vars=['temperature'], value_name='temp')\ndf_melted['time'] = df_melted['time'].apply(datetime.time.isoformat)\ndf_melted = df_melted.sort_values(['date', 'time'])\n\nfig = plt.figure(figsize=(10,4))\nax = sns.lineplot(data = df_melted, x='time', y='temp', hue='date')\nax.tick_params(rotation=90)\n\n\n\npng\n\n\ndf_melted = df_1.melt(id_vars=['time', 'date'], value_vars=['humidity'], value_name='hum')\ndf_melted['time'] = df_melted['time'].apply(datetime.time.isoformat)\ndf_melted = df_melted.sort_values(['date', 'time'])\n\nfig = plt.figure(figsize=(10,4))\nax = sns.lineplot(data = df_melted, x='time', y='hum', hue='date')\nax.tick_params(rotation=90)\n\n\n\npng\n\n\n\n\n\nAfter splitting the readings on a day-by-day basis using pivot, I looked for a way of getting the extremes using the same table. Turns out, this was far more complex than I thought, especially for printing them in an easy-to-read way.\nLet’s start by taking a look at df_pivot again:\ndisplay(df_pivot.head())\n\n\n\n\n\n\n\n\ntemperature\n\n\nhumidity\n\n\n\n\ndate\n\n\n2022-07-13\n\n\n2022-07-14\n\n\n2022-07-15\n\n\n2022-07-16\n\n\n2022-07-13\n\n\n2022-07-14\n\n\n2022-07-15\n\n\n2022-07-16\n\n\n\n\ntime\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n00:00:00\n\n\nNaN\n\n\n29.640351\n\n\n31.483636\n\n\n31.187500\n\n\nNaN\n\n\n44.256141\n\n\n41.012727\n\n\n38.969643\n\n\n\n\n00:30:00\n\n\nNaN\n\n\n29.321428\n\n\n31.041071\n\n\n30.917544\n\n\nNaN\n\n\n44.341071\n\n\n40.332143\n\n\n38.903509\n\n\n\n\n01:00:00\n\n\nNaN\n\n\n29.025000\n\n\n30.600000\n\n\n30.540741\n\n\nNaN\n\n\n44.517857\n\n\n40.024561\n\n\n39.196297\n\n\n\n\n01:30:00\n\n\nNaN\n\n\n28.775000\n\n\n30.238182\n\n\n30.076786\n\n\nNaN\n\n\n44.776786\n\n\n39.936363\n\n\n39.655357\n\n\n\n\n02:00:00\n\n\nNaN\n\n\n28.519298\n\n\n29.961403\n\n\n29.866071\n\n\nNaN\n\n\n44.966667\n\n\n40.045614\n\n\n39.594642\n\n\n\n\n\nThis dataframe has a two-level header, with one half of the column covering the temperature on different days, and the other half recording humidity in the same periods of time. The index is a timeseries (not a datetime anymore, since the date is recorded in the columns).\n# Extracting the extremes and saving them in dataframes\nval_idxmax=df_pivot[['temperature', 'humidity']].apply(pd.Series.idxmax)\nval_max=df_pivot[['temperature', 'humidity']].apply(pd.Series.max)\n# Concatenating dataframes\ndf_concat = pd.concat([val_idxmax, val_max],  axis=1).reset_index().pivot(index='date', columns='level_0')\ndf_concat\n\n\n\n\n\n\n\n\n0\n\n\n1\n\n\n\n\nlevel_0\n\n\nhumidity\n\n\ntemperature\n\n\nhumidity\n\n\ntemperature\n\n\n\n\ndate\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2022-07-13\n\n\n14:30:00\n\n\n18:00:00\n\n\n49.774510\n\n\n33.494737\n\n\n\n\n2022-07-14\n\n\n10:30:00\n\n\n18:00:00\n\n\n47.358182\n\n\n35.562499\n\n\n\n\n2022-07-15\n\n\n10:00:00\n\n\n16:00:00\n\n\n43.696364\n\n\n36.289285\n\n\n\n\n2022-07-16\n\n\n09:00:00\n\n\n18:00:00\n\n\n51.060715\n\n\n36.000000\n\n\n\n\n\nThe result above kind of works, but is not very understandable: on the left, we have the time at which the extreme was measured and on the right we have the actual measured value.\nIt would be nicer to have a format similar to the original df_pivot, with temperature and humidity on top and the two variables timeat and measured value below. This can be done using a MultiIndex, which in this case I am building with the function .from_product. This function executes the cartesian product of the iterables passed as inputs and produces a MultiIndex object where each iterable represents a level in the multiindex.\nmi = pd.MultiIndex.from_product([['humidity', 'temperature'], ['timeat', 'value']])\nmi\nMultiIndex([(   'humidity', 'timeat'),\n            (   'humidity',  'value'),\n            ('temperature', 'timeat'),\n            ('temperature',  'value')],\n           )\nAt this point, I was looking for a way of massaging the columns and the index in order to make them match the multiindex, and this is what I came up with. Pretty ugly, but it works.\nFor each row in df_concat, I select the two columns of the multiindex (timeat and value, with labels 0 and 1 respectively), and then the specific variable (either humidity or temperature), so that the content of the list is:\n[row[timeat][humidity], row[value, humidity], row[timeat][temperature], row[value][temperature]]\nFinally, I add the index (i.e. the date) to the list, then I build the dataframe starting from those records. In the new dataframe, the index is then set to use the date column added to the list.\nThis is one way of printing the values, however I later realized that this makes actual pretty printing far more difficult, and the entire process thus far was quite clunky and “unpythonic”, which is why I decided to something else.\nrows=[]\nfor idx, row in df_concat.iterrows():\n    t = [row[i_][variable]  for variable in ['humidity', 'temperature'] for i_ in [0,1] ]\n    rows.append([idx] + t)\ndf_new = pd.DataFrame.from_records(rows)\ndf_new=df_new.set_index(0)\ndf_new.index.rename('date', inplace=True)\ndf_new.columns = mi\ndf_new\n\n\n\n\n\n\n\n\nhumidity\n\n\ntemperature\n\n\n\n\n\n\ntimeat\n\n\nvalue\n\n\ntimeat\n\n\nvalue\n\n\n\n\ndate\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2022-07-13\n\n\n14:30:00\n\n\n49.774510\n\n\n18:00:00\n\n\n33.494737\n\n\n\n\n2022-07-14\n\n\n10:30:00\n\n\n47.358182\n\n\n18:00:00\n\n\n35.562499\n\n\n\n\n2022-07-15\n\n\n10:00:00\n\n\n43.696364\n\n\n16:00:00\n\n\n36.289285\n\n\n\n\n2022-07-16\n\n\n09:00:00\n\n\n51.060715\n\n\n18:00:00\n\n\n36.000000"
  },
  {
    "objectID": "posts/raspberry/study-temperature.html#part-1-the-attic",
    "href": "posts/raspberry/study-temperature.html#part-1-the-attic",
    "title": "Studying the temperature readings",
    "section": "",
    "text": "The temperature/humidity readings are stored in a csv file, together with the timestamp of the reading. Let’s start with the readings coming from the sensor left in the attic.\ndf = pd.read_csv('log.txt', header=0, names=['date','time', 'temperature', 'humidity'])\nThe next few commands are there for turning the datetime into a more manageable format, which helps with plotting later. In particular, since the date and time columns are read as strings, the following lines convert them back into datetime format.\nThen, the two are combined back into a singular datetime object for each row, with the entire timestamp.\ndf['time'] = df['time'].apply(datetime.time.fromisoformat)\ndf['date'] = df['date'].apply(datetime.date.fromisoformat)\n\ndf['datetime'] = pd.to_datetime([datetime.datetime.combine(a, b) for a,b in zip(df['date'], df['time'])])\nNow that I have a unique datetime index for every row, I can reset the index of the dataframe so that it uses the timestamp, rather than the regular ID.\nThe next step is resampling the timeline in order to reduce the number of readings, and to reduce the effect of anomalous readings. In this example, readings are grouped every 30 minutes, then they are aggregated by using the mean function.\nOut of curiosity, and following a procedure I am not completely sure of, I am trying to plot the variance of the measurement over the same interval of time by aggregating with var, then plotting using the fill_between function.\n# Resetting the index so that it uses the datetime, then aggregating according to mean and variance. \ndf_resampled = df.set_index('datetime').resample('30Min').mean()\n\ndf_var = df.set_index('datetime').resample('30Min').var()\nTime for the first plots. Here, I am plotting all the readings from the moment I activated the process, to the moment I saved it on disk to study the readings. To better distinguish between days, I am searching for the timestamps with time (0,0) (i.e. midnight), then I’m saving them in the variable midnight.\nThe command df_resampled.index.time==datetime.time(0,0) looks in the .time attribute of the datetime objects in the index and checks if the value contained therein is the same as datetime.time(0,0).\nThe first plot will show the timeseries of temperature and humidity, along with their (supposed) error bars. Since I’m using the same code for plotting both temperature and humidity, I wrote a simple function that takes as input the dataframe and the variable of interest, then prepares the resulting plot.\ndef plot_overall_var(df, tgt_var, df_var=None):\n\n    def get_bounds(df_base, df_var, column):\n        '''Preparing the upper and lower bounds by adding the variance\n        '''\n        y1 = df_base[column] + df_var[column]\n        y2 = df_base[column] - df_var[column]\n\n        return y1,y2\n\n    # Setting the size of the figure\n    fig = plt.figure(figsize=(16,4))\n\n    # Plotting the main line\n    ax = sns.lineplot(x=df_resampled.index, y=df_resampled[tgt_var])\n    plt.title(tgt_var)\n\n    # If provided, adding the variance\n    if df_var is not None:\n        yhigh, ylow = get_bounds(df_resampled, df_var, tgt_var)\n        plt.fill_between(df_resampled.index, ylow, yhigh, color='orange')\n\n    # Plotting vertical lines for highlighting the change of date\n    midnight = df_resampled.loc[df_resampled.index.time==datetime.time(0,0)].index\n    for day in midnight:\n        plt.axvline(day)\n        \n    # Plotting ticks every 3 hours\n    _ = ax.xaxis.set_major_locator(mdates.HourLocator(interval=3))\n    \n    # Rotating the ticks\n    ax.xaxis.set_tick_params(rotation=60)\n    \n    # Tightening the result\n    plt.tight_layout()\n\nplot_overall_var(df_resampled, 'temperature', df_var)\nplot_overall_var(df_resampled, 'humidity', df_var)\n \n\n\n\nIn the next section, I will reshape the data with the objective of comparing the readings gathered in different days, to look for potential similarities and differences. To do so, I’ll first split the datetime index into columns date and time, then I will pivot the table so that the new index will be time, and the columns will report temperature and humidity for each day. As you’ll notice, a bunch of values are missing and get with the value NaN: this is due to the fact that I started the data collection around 14h on July 13th.\ndf_resampled['date'] = df_resampled.index.date\ndf_resampled['time'] = df_resampled.index.time\n\ndf_pivot=df_resampled.pivot(index='time', columns='date')\n# Displaying only the first 10 rows\ndf_pivot.head(10)\n\n\n\n\n\n\n\n\ntemperature\n\n\nhumidity\n\n\n\n\ndate\n\n\n2022-07-13\n\n\n2022-07-14\n\n\n2022-07-15\n\n\n2022-07-16\n\n\n2022-07-13\n\n\n2022-07-14\n\n\n2022-07-15\n\n\n2022-07-16\n\n\n\n\ntime\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n00:00:00\n\n\nNaN\n\n\n29.640351\n\n\n31.483636\n\n\n31.187500\n\n\nNaN\n\n\n44.256141\n\n\n41.012727\n\n\n38.969643\n\n\n\n\n00:30:00\n\n\nNaN\n\n\n29.321428\n\n\n31.041071\n\n\n30.917544\n\n\nNaN\n\n\n44.341071\n\n\n40.332143\n\n\n38.903509\n\n\n\n\n01:00:00\n\n\nNaN\n\n\n29.025000\n\n\n30.600000\n\n\n30.540741\n\n\nNaN\n\n\n44.517857\n\n\n40.024561\n\n\n39.196297\n\n\n\n\n01:30:00\n\n\nNaN\n\n\n28.775000\n\n\n30.238182\n\n\n30.076786\n\n\nNaN\n\n\n44.776786\n\n\n39.936363\n\n\n39.655357\n\n\n\n\n02:00:00\n\n\nNaN\n\n\n28.519298\n\n\n29.961403\n\n\n29.866071\n\n\nNaN\n\n\n44.966667\n\n\n40.045614\n\n\n39.594642\n\n\n\n\n02:30:00\n\n\nNaN\n\n\n28.323214\n\n\n29.657143\n\n\n29.710527\n\n\nNaN\n\n\n44.798214\n\n\n39.983929\n\n\n40.035088\n\n\n\n\n03:00:00\n\n\nNaN\n\n\n28.083929\n\n\n29.340000\n\n\n29.500000\n\n\nNaN\n\n\n44.835714\n\n\n39.558182\n\n\n40.560714\n\n\n\n\n03:30:00\n\n\nNaN\n\n\n27.839286\n\n\n29.054386\n\n\n29.285714\n\n\nNaN\n\n\n45.023214\n\n\n39.131579\n\n\n42.167857\n\n\n\n\n04:00:00\n\n\nNaN\n\n\n27.514286\n\n\n28.794643\n\n\n29.000000\n\n\nNaN\n\n\n45.291072\n\n\n39.330357\n\n\n44.741072\n\n\n\n\n04:30:00\n\n\nNaN\n\n\n27.251786\n\n\n28.581818\n\n\n28.656364\n\n\nNaN\n\n\n45.671428\n\n\n40.038182\n\n\n45.830909\n\n\n\n\n\nHere I’m plotting the values of temperature and humidity recorded on each day.\ntime_range = pd.timedelta_range(start='1 day', end='2 days', freq='3H')\nfig, axs = plt.subplots(2,1, sharex=True, figsize=(10, 8))\ndf_pivot=df_resampled.pivot(index='time', columns='date')\na=df_pivot['temperature'].plot(title='Temperature', ylabel='Temperature (*C)', ax=axs[0], legend=True)\n\ndf_pivot=df_resampled.pivot(index='time', columns='date')\ndf_pivot['humidity'].plot(ax=axs[1], title='Humidity', ylabel='Humidity (%)', legend=True)\n\n# plt.legend([a.lines])\nplt.tight_layout()\nplt.savefig('temp-humid-attic.png', transparent=False, facecolor='white')\n\n\n\npng\n\n\n\n\n\nIn this section, I will be parsing the data and look for the daily extremes (maximum and minimum) for temperature and humidity, and the time at which they occurred.\n# Simple dictionary for formatting the unit of measurement that will be printed\nd_form = {'temperature': '*C', 'humidity': '%'}\n# Grouping by day\nfor idx, day_g in df_resampled.groupby('date'):\n    print(f'Date: {idx}')\n    \n#     Maximum\n#     idxmax returns the position of the maximum value\n    aggr_i = day_g[['temperature', 'humidity']].aggregate(pd.Series.idxmax)\n    aggr_m = day_g[['temperature', 'humidity']].aggregate(pd.Series.max)\n#     Putting the values together for pretty printing\n    df_t=pd.concat([aggr_i, aggr_m], axis=1)\n    for i, val in df_t.iterrows():\n        print(f'Max {i:&gt;12} = {val[1]:.1f} {d_form[i]:&gt;2} at {val[0].time()}')\n        \n#     Minimum\n#     idxmax returns the position of the maximum value\n    aggr_i = day_g[['temperature', 'humidity']].aggregate(pd.Series.idxmin)\n    aggr_m = day_g[['temperature', 'humidity']].aggregate(pd.Series.min)\n#     Putting the values together for pretty printing\n    df_t=pd.concat([aggr_i, aggr_m], axis=1)\n    for i, val in df_t.iterrows():\n        print(f'Min {i:&gt;12} = {val[1]:.1f} {d_form[i]:&gt;2} at {val[0].time()}')\nDate: 2022-07-13\nMax  temperature = 33.5 *C at 18:00:00\nMax     humidity = 49.8  % at 14:30:00\nMin  temperature = 30.0 *C at 23:30:00\nMin     humidity = 42.6  % at 18:30:00\nDate: 2022-07-14\nMax  temperature = 35.6 *C at 18:00:00\nMax     humidity = 47.4  % at 10:30:00\nMin  temperature = 26.4 *C at 07:30:00\nMin     humidity = 38.8  % at 19:00:00\nDate: 2022-07-15\nMax  temperature = 36.3 *C at 16:00:00\nMax     humidity = 43.7  % at 10:00:00\nMin  temperature = 27.6 *C at 07:00:00\nMin     humidity = 33.4  % at 16:00:00\nDate: 2022-07-16\nMax  temperature = 36.0 *C at 18:00:00\nMax     humidity = 51.1  % at 09:00:00\nMin  temperature = 27.7 *C at 07:30:00\nMin     humidity = 38.9  % at 00:30:00"
  },
  {
    "objectID": "posts/raspberry/study-temperature.html#part-2-the-bedroom",
    "href": "posts/raspberry/study-temperature.html#part-2-the-bedroom",
    "title": "Studying the temperature readings",
    "section": "",
    "text": "In this section, I will be repeating most of what was done in Part 1: the main difference will be the location of the sensor. Rather than keeping it in the attic, in fact, I was keeping this on the desk in my bedroom. In particular, the sensor was exposed to the heat coming from a pretty chonky desktop PC, as well as two monitors, and it is positioned close to a window.\ndf = pd.read_csv('readings-room.csv', header=0, names=['date','time', 'temperature', 'humidity'])\ndf['time'] = df['time'].apply(datetime.time.fromisoformat)\ndf['date'] = df['date'].apply(datetime.date.fromisoformat)\n\ndf['datetime'] = pd.to_datetime([datetime.datetime.combine(a, b) for a,b in zip(df['date'], df['time'])])\n# Resetting the index so that it uses the datetime, then aggregating according to mean and variance. \ndf_resampled = df.set_index('datetime').resample('30Min').mean()\n\ndf_var = df.set_index('datetime').resample('30Min').var()\nBelow, I’m plotting the overall temperature and humidity readings. Unlike in the case of the attic, however, the variance is far more larger, which leads to a plot that is almost unreadable in the case of humidity. For this reason, I am also plotting the graphs without added bands by calling plot_overall_var without the df_var variable.\nplot_overall_var(df_resampled, 'temperature', df_var)\nplot_overall_var(df_resampled, 'humidity', df_var)\n\nplot_overall_var(df_resampled, 'temperature')\nplot_overall_var(df_resampled, 'humidity')\n\n\n\npng\n\n\n\n\n\npng\n\n\n\n\n\npng\n\n\n\n\n\npng\n\n\n\n\ndf_resampled['date'] = df_resampled.index.date\ndf_resampled['time'] = df_resampled.index.time\n\ndf_pivot=df_resampled.pivot(index='time', columns='date')\n# Displaying only the first 10 rows\ndf_pivot.head(10)\n\n\n\n\n\n\n\n\ntemperature\n\n\nhumidity\n\n\n\n\ndate\n\n\n2022-07-06\n\n\n2022-07-07\n\n\n2022-07-08\n\n\n2022-07-06\n\n\n2022-07-07\n\n\n2022-07-08\n\n\n\n\ntime\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n00:00:00\n\n\nNaN\n\n\nNaN\n\n\n24.601786\n\n\nNaN\n\n\nNaN\n\n\n60.225000\n\n\n\n\n00:30:00\n\n\nNaN\n\n\n26.792857\n\n\n23.985965\n\n\nNaN\n\n\n62.985715\n\n\n61.821053\n\n\n\n\n01:00:00\n\n\nNaN\n\n\n26.937255\n\n\n23.458929\n\n\nNaN\n\n\n62.368627\n\n\n63.941071\n\n\n\n\n01:30:00\n\n\nNaN\n\n\n26.825000\n\n\n22.831579\n\n\nNaN\n\n\n62.901786\n\n\n66.178947\n\n\n\n\n02:00:00\n\n\nNaN\n\n\n26.421053\n\n\n21.867273\n\n\nNaN\n\n\n63.638596\n\n\n69.143637\n\n\n\n\n02:30:00\n\n\nNaN\n\n\n26.396428\n\n\n22.728572\n\n\nNaN\n\n\n64.705357\n\n\n65.946429\n\n\n\n\n03:00:00\n\n\nNaN\n\n\n26.171930\n\n\n22.787719\n\n\nNaN\n\n\n64.521053\n\n\n64.991228\n\n\n\n\n03:30:00\n\n\nNaN\n\n\n25.918965\n\n\n22.864285\n\n\nNaN\n\n\n65.108621\n\n\n65.960714\n\n\n\n\n04:00:00\n\n\nNaN\n\n\n25.978571\n\n\n23.108929\n\n\nNaN\n\n\n64.525000\n\n\n66.139285\n\n\n\n\n04:30:00\n\n\nNaN\n\n\n25.794736\n\n\n23.110527\n\n\nNaN\n\n\n65.349123\n\n\n66.324561\n\n\n\n\n\nval_idxmax=df_pivot[['temperature', 'humidity']].apply(pd.Series.idxmax)\nval_max=df_pivot[['temperature', 'humidity']].apply(pd.Series.max)\ndf_concat = pd.concat([val_idxmax, val_max],  axis=1).reset_index().pivot(index='date', columns='level_0')\ndf_concat\n\n\n\n\n\n\n\n\n0\n\n\n1\n\n\n\n\nlevel_0\n\n\nhumidity\n\n\ntemperature\n\n\nhumidity\n\n\ntemperature\n\n\n\n\ndate\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2022-07-06\n\n\n18:30:00\n\n\n17:00:00\n\n\n67.537736\n\n\n28.174419\n\n\n\n\n2022-07-07\n\n\n07:30:00\n\n\n17:00:00\n\n\n70.470175\n\n\n28.641072\n\n\n\n\n2022-07-08\n\n\n05:00:00\n\n\n17:30:00\n\n\n70.805263\n\n\n27.100000\n\n\n\n\n\nmi = pd.MultiIndex.from_product([['humidity', 'temperature'], ['timeat', 'value']])\nrows=[]\nfor idx, row in df_concat.iterrows():\n    t = [row[idx][variable]  for variable in ['humidity', 'temperature'] for idx in [0,1] ]\n    rows.append([idx] + t)\ndf_new = pd.DataFrame.from_records(rows)\ndf_new=df_new.set_index(0)\ndf_new.index.rename('date', inplace=True)\ndf_new.columns = mi\ndf_new\n\n\n\n\n\n\n\n\nhumidity\n\n\ntemperature\n\n\n\n\n\n\ntimeat\n\n\nvalue\n\n\ntimeat\n\n\nvalue\n\n\n\n\ndate\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2022-07-06\n\n\n18:30:00\n\n\n67.537736\n\n\n17:00:00\n\n\n28.174419\n\n\n\n\n2022-07-07\n\n\n07:30:00\n\n\n70.470175\n\n\n17:00:00\n\n\n28.641072\n\n\n\n\n2022-07-08\n\n\n05:00:00\n\n\n70.805263\n\n\n17:30:00\n\n\n27.100000\n\n\n\n\n\ntime_range = pd.timedelta_range(start='1 day', end='2 days', freq='3H')\nfig, axs = plt.subplots(2,1, sharex=True, figsize=(10, 8))\ndf_pivot=df_resampled.pivot(index='time', columns='date')\na=df_pivot['temperature'].plot(title='Temperature', ylabel='Temperature (*C)', ax=axs[0], legend=True)\n\ndf_pivot=df_resampled.pivot(index='time', columns='date')\ndf_pivot['humidity'].plot(ax=axs[1], title='Humidity', ylabel='Humidity (%)', legend=True)\n\n# plt.legend([a.lines])\nplt.tight_layout()\nplt.savefig('temp-humid-bedroom.png', transparent=False, facecolor='white')\n\n\n\npng\n\n\n\n\n\n# Simple dictionary for formatting the unit of measurement that will be printed\nd_form = {'temperature': '*C', 'humidity': '%'}\n# Grouping by day\nfor idx, day_g in df_resampled.groupby('date'):\n    print(f'Date: {idx}')\n    \n#     Maximum\n#     idxmax returns the position of the maximum value\n    aggr_i = day_g[['temperature', 'humidity']].aggregate(pd.Series.idxmax)\n    aggr_m = day_g[['temperature', 'humidity']].aggregate(pd.Series.max)\n#     Putting the values together for pretty printing\n    df_t=pd.concat([aggr_i, aggr_m], axis=1)\n    for i, val in df_t.iterrows():\n        print(f'Max {i:&gt;12} = {val[1]:.1f} {d_form[i]:&gt;2} at {val[0].time()}')\n        \n#     Minimum\n#     idxmax returns the position of the maximum value\n    aggr_i = day_g[['temperature', 'humidity']].aggregate(pd.Series.idxmin)\n    aggr_m = day_g[['temperature', 'humidity']].aggregate(pd.Series.min)\n#     Putting the values together for pretty printing\n    df_t=pd.concat([aggr_i, aggr_m], axis=1)\n    for i, val in df_t.iterrows():\n        print(f'Min {i:&gt;12} = {val[1]:.1f} {d_form[i]:&gt;2} at {val[0].time()}')\nDate: 2022-07-06\nMax  temperature = 28.2 *C at 17:00:00\nMax     humidity = 67.5  % at 18:30:00\nMin  temperature = 26.7 *C at 21:30:00\nMin     humidity = 60.8  % at 22:00:00\nDate: 2022-07-07\nMax  temperature = 28.6 *C at 17:00:00\nMax     humidity = 70.5  % at 07:30:00\nMin  temperature = 24.4 *C at 23:00:00\nMin     humidity = 58.5  % at 23:30:00\nDate: 2022-07-08\nMax  temperature = 27.1 *C at 17:30:00\nMax     humidity = 70.8  % at 05:00:00\nMin  temperature = 21.7 *C at 05:00:00\nMin     humidity = 60.2  % at 00:00:00"
  },
  {
    "objectID": "posts/raspberry/study-temperature.html#part-3-concluding",
    "href": "posts/raspberry/study-temperature.html#part-3-concluding",
    "title": "Studying the temperature readings",
    "section": "",
    "text": "Let’s wrap up the post by summarizing what was shown, then making some observations.\nOverall, this was an interesting exercise to play around with timeseries, which is a format I haven’t really considered during my PhD. I had to figure out how to handle them using the functions provided by pandas, then how to use matplotlib to prepare the graphs while getting around the shortcomings of my data-gathering setup. In particular, seaborn was far more stubborn than I expected and I wasn’t able to find a proper way of wrangling the correct plots with the library.\nRegarding the actual data, it’s interesting to see the differences between the readings coming from the two locations. They were gathered over different time periods, and it was far hotter while I was recording the temperature in the attic: this makes direct comparisons not very meaningful, due to how different the external conditions were. Still, the difference in readings between the room and the attic can be explained by the fact that there is no activity in the attic; on the other hand, the sensor in the bedroom picked up on both my activity there: this involved the use of a desktop PC and multiple monitors and additional devices, coupled with the fact that windows can generate a noticeable breeze that goes through the entire top floor of the building. This goes to show that a good ventilato helps a lot with cooling down a hot interior, and that electronic devices work directly against that!\nI’m still working on building a different temperature sensor, and possibly something more complex that would allow to gather further information (a weather station maybe?), so I will definitely come back to this subject at some point in the future."
  },
  {
    "objectID": "posts/raspberry/study-temperature.html#appendix-other-unsuccessful-attempts",
    "href": "posts/raspberry/study-temperature.html#appendix-other-unsuccessful-attempts",
    "title": "Studying the temperature readings",
    "section": "",
    "text": "In this section I’m gathering some additional methods I tried while working on the points shown above. I’m adding these both as a note to myself of what doesn’t work, and for readers to see what can happen while trying stuff out.\nFrom my understanding, part of the reason why these methods did not work quite as well as I hoped is due to the fact that I’m working with datetime objects, rather than “regular” indexing. This results in some funky behavior in some cases.\n\n\nThe pandas.DataFrame.melt function allows to obtain a “long form” version of a dataframe that pivots around some columns, while keeping other columns as variables. This can be extremely useful for plotting different variables on the same plot using libraries such as seaborn, which is what I will be trying in a moment.\ndf_melted = df_resampled.melt(id_vars=['time', 'date'], value_vars=['temperature'])\ndf_melted['time'] = df_melted['time'].apply(datetime.time.isoformat)\ndisplay(df_melted)\n\n# df_melted = df_melted.sort_values(['date', 'time'])\n\n\n\n\n\n\n\n\ntime\n\n\ndate\n\n\nvariable\n\n\nvalue\n\n\n\n\n\n\n0\n\n\n14:30:00\n\n\n2022-07-13\n\n\ntemperature\n\n\n31.266667\n\n\n\n\n1\n\n\n15:00:00\n\n\n2022-07-13\n\n\ntemperature\n\n\n32.048214\n\n\n\n\n2\n\n\n15:30:00\n\n\n2022-07-13\n\n\ntemperature\n\n\n32.547368\n\n\n\n\n3\n\n\n16:00:00\n\n\n2022-07-13\n\n\ntemperature\n\n\n32.926316\n\n\n\n\n4\n\n\n16:30:00\n\n\n2022-07-13\n\n\ntemperature\n\n\n33.198245\n\n\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n\n\n147\n\n\n16:00:00\n\n\n2022-07-16\n\n\ntemperature\n\n\n35.548214\n\n\n\n\n148\n\n\n16:30:00\n\n\n2022-07-16\n\n\ntemperature\n\n\n35.810526\n\n\n\n\n149\n\n\n17:00:00\n\n\n2022-07-16\n\n\ntemperature\n\n\n35.903573\n\n\n\n\n150\n\n\n17:30:00\n\n\n2022-07-16\n\n\ntemperature\n\n\n35.987719\n\n\n\n\n151\n\n\n18:00:00\n\n\n2022-07-16\n\n\ntemperature\n\n\n36.000000\n\n\n\n\n\n152 rows × 4 columns\n\n\nA “melted” dataframe can be plotted fairly easily using a command such as the one below. Besides the x- and y-axes, it is possible to distinguish the lines by hue (in this case, different colors for different dates) and by style (different line style for different variables). Here, it does not make a lot of sense to plot humidity and temperature in the same plot, but it is useful for the sake of the example.\nSomething else to note in the plot below is that the x-axis starts at 14:30, then continues for 24 hours until 14:00 the next day: this is due to the fact that the data captured by the log is incomplete for the day of 2022-07-13. After searching around for a while, I wasn’t able to reset the ticks on the x-axis to make them work properly, so I had to look for some workarounds.\nIndeed, the current plot shows a pretty major inconsistency at 0:00, which is when the date shifts from July 13th to July 14th, which is also why the orange line “jumps” below and seems to be a continuation of the blue line: this is exactly what that is!\nfig = plt.figure(figsize=(12,4))\nax = sns.lineplot(data = df_melted, x='time', y='value', hue='date', style='variable')\nax.tick_params(rotation=90)\n\n\n\npng\n\n\nIt is possible to build a plot by using the full datetime object, rather than splitting by day, and the result is shown below:\ndf_melted = df_resampled.reset_index().melt(id_vars=['datetime'], value_vars=['temperature'])\ndisplay(df_melted)\n\nfig = plt.figure(figsize=(12,4))\nax = sns.lineplot(data = df_melted, x='datetime', y='value')\nax.tick_params(rotation=90)\n\n\n\n\n\n\n\n\ndatetime\n\n\nvariable\n\n\nvalue\n\n\n\n\n\n\n0\n\n\n2022-07-13 14:30:00\n\n\ntemperature\n\n\n31.266667\n\n\n\n\n1\n\n\n2022-07-13 15:00:00\n\n\ntemperature\n\n\n32.048214\n\n\n\n\n2\n\n\n2022-07-13 15:30:00\n\n\ntemperature\n\n\n32.547368\n\n\n\n\n3\n\n\n2022-07-13 16:00:00\n\n\ntemperature\n\n\n32.926316\n\n\n\n\n4\n\n\n2022-07-13 16:30:00\n\n\ntemperature\n\n\n33.198245\n\n\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n\n\n147\n\n\n2022-07-16 16:00:00\n\n\ntemperature\n\n\n35.548214\n\n\n\n\n148\n\n\n2022-07-16 16:30:00\n\n\ntemperature\n\n\n35.810526\n\n\n\n\n149\n\n\n2022-07-16 17:00:00\n\n\ntemperature\n\n\n35.903573\n\n\n\n\n150\n\n\n2022-07-16 17:30:00\n\n\ntemperature\n\n\n35.987719\n\n\n\n\n151\n\n\n2022-07-16 18:00:00\n\n\ntemperature\n\n\n36.000000\n\n\n\n\n\n152 rows × 3 columns\n\n\n\n\n\npng\n\n\nNow, this plot looks much clearer, however it’s not exactly what I wanted. For this reason, I decided to get rid of the offending data series (i.e. the incomplete series of readings from the first day) and plot the data for the remaining days. This isn’t really the best way of doing it, which is why I went for the solution in the first section of this post, but it gets the job done at least.\nSo, now my objective is selecting only the readings that start after the first day has ended, which can be done using either one of the two commands below, whose output is exactly the same.\ndf_resampled.index returns the index of the dataframe, which is in datetime format: this allows to compare it with a different datetime object that I am declaring (either fromisoformat(...) or date(...)). The comparison allows to find the datetime indices that occur after the given value, and keep only those.\ndf_1 =df_resampled.loc[df_resampled.index &gt;= datetime.datetime.fromisoformat('2022-07-14 00:00:00')]\n\ndf_2 =df_resampled.loc[df_resampled.index.date &gt; datetime.date(2022, 7,13)]\n\n# Checking that the two indices are equal\nall(df_1.index == df_2.index)\nTrue\nNow that there is a cleaner dataframe to plot, the usual lineplot function can be used to prepare the temperature and humidity comparisons. |\ndf_melted = df_1.melt(id_vars=['time', 'date'], value_vars=['temperature'], value_name='temp')\ndf_melted['time'] = df_melted['time'].apply(datetime.time.isoformat)\ndf_melted = df_melted.sort_values(['date', 'time'])\n\nfig = plt.figure(figsize=(10,4))\nax = sns.lineplot(data = df_melted, x='time', y='temp', hue='date')\nax.tick_params(rotation=90)\n\n\n\npng\n\n\ndf_melted = df_1.melt(id_vars=['time', 'date'], value_vars=['humidity'], value_name='hum')\ndf_melted['time'] = df_melted['time'].apply(datetime.time.isoformat)\ndf_melted = df_melted.sort_values(['date', 'time'])\n\nfig = plt.figure(figsize=(10,4))\nax = sns.lineplot(data = df_melted, x='time', y='hum', hue='date')\nax.tick_params(rotation=90)\n\n\n\npng\n\n\n\n\n\nAfter splitting the readings on a day-by-day basis using pivot, I looked for a way of getting the extremes using the same table. Turns out, this was far more complex than I thought, especially for printing them in an easy-to-read way.\nLet’s start by taking a look at df_pivot again:\ndisplay(df_pivot.head())\n\n\n\n\n\n\n\n\ntemperature\n\n\nhumidity\n\n\n\n\ndate\n\n\n2022-07-13\n\n\n2022-07-14\n\n\n2022-07-15\n\n\n2022-07-16\n\n\n2022-07-13\n\n\n2022-07-14\n\n\n2022-07-15\n\n\n2022-07-16\n\n\n\n\ntime\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n00:00:00\n\n\nNaN\n\n\n29.640351\n\n\n31.483636\n\n\n31.187500\n\n\nNaN\n\n\n44.256141\n\n\n41.012727\n\n\n38.969643\n\n\n\n\n00:30:00\n\n\nNaN\n\n\n29.321428\n\n\n31.041071\n\n\n30.917544\n\n\nNaN\n\n\n44.341071\n\n\n40.332143\n\n\n38.903509\n\n\n\n\n01:00:00\n\n\nNaN\n\n\n29.025000\n\n\n30.600000\n\n\n30.540741\n\n\nNaN\n\n\n44.517857\n\n\n40.024561\n\n\n39.196297\n\n\n\n\n01:30:00\n\n\nNaN\n\n\n28.775000\n\n\n30.238182\n\n\n30.076786\n\n\nNaN\n\n\n44.776786\n\n\n39.936363\n\n\n39.655357\n\n\n\n\n02:00:00\n\n\nNaN\n\n\n28.519298\n\n\n29.961403\n\n\n29.866071\n\n\nNaN\n\n\n44.966667\n\n\n40.045614\n\n\n39.594642\n\n\n\n\n\nThis dataframe has a two-level header, with one half of the column covering the temperature on different days, and the other half recording humidity in the same periods of time. The index is a timeseries (not a datetime anymore, since the date is recorded in the columns).\n# Extracting the extremes and saving them in dataframes\nval_idxmax=df_pivot[['temperature', 'humidity']].apply(pd.Series.idxmax)\nval_max=df_pivot[['temperature', 'humidity']].apply(pd.Series.max)\n# Concatenating dataframes\ndf_concat = pd.concat([val_idxmax, val_max],  axis=1).reset_index().pivot(index='date', columns='level_0')\ndf_concat\n\n\n\n\n\n\n\n\n0\n\n\n1\n\n\n\n\nlevel_0\n\n\nhumidity\n\n\ntemperature\n\n\nhumidity\n\n\ntemperature\n\n\n\n\ndate\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2022-07-13\n\n\n14:30:00\n\n\n18:00:00\n\n\n49.774510\n\n\n33.494737\n\n\n\n\n2022-07-14\n\n\n10:30:00\n\n\n18:00:00\n\n\n47.358182\n\n\n35.562499\n\n\n\n\n2022-07-15\n\n\n10:00:00\n\n\n16:00:00\n\n\n43.696364\n\n\n36.289285\n\n\n\n\n2022-07-16\n\n\n09:00:00\n\n\n18:00:00\n\n\n51.060715\n\n\n36.000000\n\n\n\n\n\nThe result above kind of works, but is not very understandable: on the left, we have the time at which the extreme was measured and on the right we have the actual measured value.\nIt would be nicer to have a format similar to the original df_pivot, with temperature and humidity on top and the two variables timeat and measured value below. This can be done using a MultiIndex, which in this case I am building with the function .from_product. This function executes the cartesian product of the iterables passed as inputs and produces a MultiIndex object where each iterable represents a level in the multiindex.\nmi = pd.MultiIndex.from_product([['humidity', 'temperature'], ['timeat', 'value']])\nmi\nMultiIndex([(   'humidity', 'timeat'),\n            (   'humidity',  'value'),\n            ('temperature', 'timeat'),\n            ('temperature',  'value')],\n           )\nAt this point, I was looking for a way of massaging the columns and the index in order to make them match the multiindex, and this is what I came up with. Pretty ugly, but it works.\nFor each row in df_concat, I select the two columns of the multiindex (timeat and value, with labels 0 and 1 respectively), and then the specific variable (either humidity or temperature), so that the content of the list is:\n[row[timeat][humidity], row[value, humidity], row[timeat][temperature], row[value][temperature]]\nFinally, I add the index (i.e. the date) to the list, then I build the dataframe starting from those records. In the new dataframe, the index is then set to use the date column added to the list.\nThis is one way of printing the values, however I later realized that this makes actual pretty printing far more difficult, and the entire process thus far was quite clunky and “unpythonic”, which is why I decided to something else.\nrows=[]\nfor idx, row in df_concat.iterrows():\n    t = [row[i_][variable]  for variable in ['humidity', 'temperature'] for i_ in [0,1] ]\n    rows.append([idx] + t)\ndf_new = pd.DataFrame.from_records(rows)\ndf_new=df_new.set_index(0)\ndf_new.index.rename('date', inplace=True)\ndf_new.columns = mi\ndf_new\n\n\n\n\n\n\n\n\nhumidity\n\n\ntemperature\n\n\n\n\n\n\ntimeat\n\n\nvalue\n\n\ntimeat\n\n\nvalue\n\n\n\n\ndate\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2022-07-13\n\n\n14:30:00\n\n\n49.774510\n\n\n18:00:00\n\n\n33.494737\n\n\n\n\n2022-07-14\n\n\n10:30:00\n\n\n47.358182\n\n\n18:00:00\n\n\n35.562499\n\n\n\n\n2022-07-15\n\n\n10:00:00\n\n\n43.696364\n\n\n16:00:00\n\n\n36.289285\n\n\n\n\n2022-07-16\n\n\n09:00:00\n\n\n51.060715\n\n\n18:00:00\n\n\n36.000000"
  },
  {
    "objectID": "posts/raspberry/coding-the-thermometer.html",
    "href": "posts/raspberry/coding-the-thermometer.html",
    "title": "Coding the Thermometer",
    "section": "",
    "text": "In this post, I will describe the code I have developed to run the thermometer, log the data and save it in such a way that it can be used to carry out some (limited) analytics. At this point, the thermometer has already been installed  \nPart of the code I am showing here was taken from the Ardumotive - How to use the DHT22 I have already referenced. I then expanded on it to add some simple logging.\n\nLibraries\nTo run the code, only three libraries are needed: the Adafruit_DHT library,  time library (for sleep), and the datetime library for better date/time utilities.\nsleep is needed to slow down the program, so that the sensor is able to gather the information (in fact, it cannot be accessed more than once every two seconds).\n# Library for reading from the DHT sensor\nimport Adafruit_DHT as dht\nfrom time import sleep\nfrom datetime import datetime as dt\n\n\nThe main loop\nThe main loop of the progam features a while True loop, which will let the program run until it gets interrupted by the user. As soon as the program runs, the log file is opened in append mode, so that it is possible to add more lines without deleting previous readings (this is pretty useful when debugging!). The value of DHT_data is set to be the pin that is connected to the DATA output of the DHT22 sensor.\nNext, the while loop is executed. In each iteration of the loop, the timestamp of the iteration is saved in now, then the sensor is read. The values of temperature and humidity are printed on screen, then put together with the timestamp in variable data_line and added to the log file.\nThe loop is wrapped in a try-except block, so that it is possible to interrupt it “gracefully” using Ctrl+C. When this happens, the log file is closed and the program terminates.\n#Set DATA pin (this is the pin I will be connecting the DATA output of the chip to)\nDHT_data = 4 \n\n# Open the log file in append mode, so that previous readings will not be deleted \n# when the program restarts.\nfw = open('log.txt', 'a')\n\ntry: \n    # Run forever, until the program is killed through Ctrl+C\n    while True:\n        # Getting timestamp for the current measurement\n        now = dt.now()\n        # Reading Humidity and Temperature from DHT22\n        h,t = dht.read_retry(dht.DHT22, DHT_data)\n\n        # Print Temperature and Humidity on Shell window\n        print('Time={0}\\tTemp={1:0.1f}*C\\tHumidity={2:0.1f}%'.format(now, t, h))\n\n        # Preparing the datetime object for logging, splitting date and time for convenience.\n        this_date = now.date()\n        this_time = now.time()\n        # Writing the data in CSV format.\n        data_line = \"{},{},{},{}\\n\".format(this_date, this_time, t,h)\n        fw.write(data_line)\n\n        sleep(30) #Wait 30 seconds and read again\n\nexcept KeyboardInterrupt:\n    fw.close()\n    print('Ending the program.')\nWhen all is said and done, this is a possible output of the program:\nTime=2022-07-12 00:57:15.430657 Temp=25.0*C     Humidity=58.2%\nTime=2022-07-12 00:57:56.118202 Temp=25.0*C     Humidity=58.3%\nTime=2022-07-12 00:58:29.214395 Temp=25.0*C     Humidity=58.3%\nTime=2022-07-12 00:58:59.776892 Temp=25.0*C     Humidity=58.5%\nTime=2022-07-12 00:59:30.340131 Temp=25.1*C     Humidity=58.5%\nTime=2022-07-12 01:00:13.552538 Temp=25.1*C     Humidity=58.3%\nTime=2022-07-12 01:00:46.646864 Temp=25.1*C     Humidity=58.4%\nTime=2022-07-12 01:01:17.210069 Temp=25.1*C     Humidity=58.4%\nTime=2022-07-12 01:01:52.816956 Temp=25.1*C     Humidity=58.5%\nNote that there is a bug in the line\nprint('Time={0}\\tTemp={1:0.1f}*C\\tHumidity={2:0.1f}%'.format(now, t, h))\nwhich is caused by the temperature sensor returning invalid data: this raises an exception in the print function and interrupts the program. I was not able to reproduce it (after admittedly not a lot of testing) to add a fix, but a simple solution to the problem is simply deleting the offending line. At least, until I figure out a fix.\nSo there we go! This is how it is possible to code a simple log of temperature and humidity by using a Raspberry Pi and a DHT22 sensor. In a future post, I will be play around with some data visualisation and plot the temperature and humidity over time.\nThanks for reading!"
  },
  {
    "objectID": "posts/data_preparation/study_recent_tracks.html",
    "href": "posts/data_preparation/study_recent_tracks.html",
    "title": "Experiments plotting my top Last.fm artists",
    "section": "",
    "text": "Following from my previous post on fetching my Last.fm scrobbles (check it out here if you want to see how I did it), I now have more than ten years’ worth of data and no idea what to do with it.\nMy first thought was to visualize my top artists, so I decided to experiment a bit with how I could do that. I recently found a very nice website (aptly) named The Python Graph Gallery, which has a lot of interesting examples of how to visualize data using Python.\nOut of all the examples, there were a couple I found particularly interesting, so I decided to apply them to my own data. However, before getting to that point, I had to do a lot of data wrangling to get my data into the proper format.\nThis was all done using Python and the Polars library, so if you’re not interested in the plots but are curious to see some funky Polars code, do keep reading."
  },
  {
    "objectID": "posts/data_preparation/study_recent_tracks.html#preparing-the-data",
    "href": "posts/data_preparation/study_recent_tracks.html#preparing-the-data",
    "title": "Experiments plotting my top Last.fm artists",
    "section": "Preparing the data",
    "text": "Preparing the data\nFirst, I imported the various libraries I needed: - datetime was needed for some datetime manipulation. - matplotlib because I wanted to push it further than I usually do. - polars for all data manipulation. - skrub for some cleaning and for the TableReport (also, it’s the library I work on). - scipy.ndimage for data smoothing to make the first plot prettier.\n\nfrom datetime import datetime\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport polars as pl\nfrom skrub import Cleaner, TableReport\nfrom scipy.ndimage import gaussian_filter1d\n\nFirst, I read the data from the CSV file I generated in the previous post, then I pass it to the Cleaner class from skrub to clean it up a bit.\nStarting from skrub version 0.6.0, the Cleaner has the datetime_format argument, which I am using here to parse directly the timestamp without having to use polars.\nThen, I use the TableReport to look at the data and see what it looks like.\n\ndf = pl.read_csv(\"data/recent-tracks.csv\")\ncleaner = Cleaner()\ndf = Cleaner(datetime_format=\"%d %b %Y, %H:%M\").fit_transform(df)\nTableReport(df, verbose=0)\n\n\n    \n\n    \n    \n\n    \n\n\n\n    Please enable javascript\n    \n        The skrub table reports need javascript to display correctly. If you are\n        displaying a report in a Jupyter notebook and you see this message, you may need to\n        re-execute the cell or to trust the notebook (button on the top right or\n        \"File &gt; Trust notebook\").\n    \n\n\n\n\n\nThe TableReport is a pretty neat tool with a lot of features. In this case, I was particularly interested in the Stats tab, which shows various high-level statistics about each column in the table. Here, it immediately showed me that some columns include a lot of missing values: the artist_mbid column has about 24% missing values, while both the album_mbid and track_mbid columns have more than 40% missing values.\nWhat’s the mbid? It’s the MusicBrainz Identifier, a unique identifier for each artist, album, and track in the MusicBrainz database.\nWhy does it matter? Because it allows me to link artists and tracks with the MusicBrainz database, which contains additional information about them, such as the genre, the release date, the country of origin, and so on. In other words, missing the mbid means I am missing out on a lot of additional information that I could use for new plots. This is a shame, and I already have plans on how to fix this.\nIn this post, however, I will focus on the artists, the tracks, and the timestamps of the scrobbles to take a look at the evolution of my favorite artists over the years. Originally, I wanted to include the genres as well, as they have a larger granularity and would therefore be easier to visualize, but the data dump I have does not include them, and even querying the Last.fm API did not help with filling in the gaps for most of the less popular artists in my library.\nIn any case, lets move on to the actual data wrangling."
  },
  {
    "objectID": "posts/data_preparation/study_recent_tracks.html#finding-the-top-artists",
    "href": "posts/data_preparation/study_recent_tracks.html#finding-the-top-artists",
    "title": "Experiments plotting my top Last.fm artists",
    "section": "Finding the top artists",
    "text": "Finding the top artists\nI decided to extract the top 15 artists from the data based on the number of scrobbles to have some variety, while remaining able to plot all of them at the same time. I also want to treat them separately from all the other artists: this comes into play later for the stackplot.\n\ntop_k = 15\ntop_artists = (\n    df.group_by(\"artist\") # group by artist\n    .agg(pl.len().alias(\"count\")) # count the number of scrobbles per artist\n    .top_k(k=top_k, by=\"count\") # get the top k artists\n)\ntop_artists\n\n\nshape: (15, 2)\n\n\n\nartist\ncount\n\n\nstr\nu32\n\n\n\n\n\"Muse\"\n11017\n\n\n\"Coldplay\"\n5470\n\n\n\"Jovanotti\"\n5444\n\n\n\"Negrita\"\n4767\n\n\n\"Zucchero\"\n3879\n\n\n…\n…\n\n\n\"Daft Punk\"\n1479\n\n\n\"Avicii\"\n1470\n\n\n\"U2\"\n1276\n\n\n\"YOASOBI\"\n1203\n\n\n\"Mika\"\n1023\n\n\n\n\n\n\nNow for the nasty wrangling part. Since my objective is to visualize the evolution of my favorite artists over time, I can do this by counting the number of scrobbles per artist each month.\nHowever, what I want from my stackplot is to show how many of the scrobbles in each month come from one of the top 15 artists, and how many come from the rest. For this, I need to find the fraction of scrobbles per month per artist. To avoid skewing the results in months where I listened to very few of the top artists, I will also include the total number of scrobbles for “everyone else” in the fraction.\n\ndf_prep = (\n    df.with_columns(\n        pl.when(pl.col(\"artist\").is_in(top_artists[\"artist\"])) # check if artist is in top artists\n        .then(pl.col(\"artist\")) # keep the artist name\n        .otherwise(pl.lit(\"Everyone else\")) # otherwise label as \"Everyone else\"\n        .alias(\"artist\") # replace the artist column with this new one\n    )\n)\n\nNow that I have the top artists and “everyone else”, I can count the number of scrobbles per artist each month. For this, I first select only the artist and the timestamp columns, then I truncate the timestamp to the month, and finally I group the data by artist and month, counting the number of scrobbles for each group, i.e., for each artist in each month.\n\ndf_prep = (df_prep\n    .select( \n        [\n            pl.col(\"artist\"),\n            pl.col(\"utc_time\").dt.truncate(\"1mo\"), # truncate the time to the month\n        ]\n    )\n    .group_by([\"artist\", \"utc_time\"])\n    .agg(count=pl.len())\n    .sort([\"utc_time\", \"count\"])\n)\ndf_prep\n\n\nshape: (1_427, 3)\n\n\n\nartist\nutc_time\ncount\n\n\nstr\ndatetime[μs]\nu32\n\n\n\n\n\"Mika\"\n2012-12-01 00:00:00\n1\n\n\n\"U2\"\n2012-12-01 00:00:00\n1\n\n\n\"Caparezza\"\n2012-12-01 00:00:00\n2\n\n\n\"Zucchero\"\n2012-12-01 00:00:00\n13\n\n\n\"Negrita\"\n2012-12-01 00:00:00\n15\n\n\n…\n…\n…\n\n\n\"Hoshimachi Suisei\"\n2025-04-01 00:00:00\n53\n\n\n\"Coldplay\"\n2025-04-01 00:00:00\n73\n\n\n\"Negrita\"\n2025-04-01 00:00:00\n81\n\n\n\"Ado\"\n2025-04-01 00:00:00\n87\n\n\n\"Everyone else\"\n2025-04-01 00:00:00\n108\n\n\n\n\n\n\nEach row in df_prep now contains the artist name, the month of the scrobble, and the number of scrobbles for that artist in that month.\nNow, I want to calculate the fraction of scrobbles per artist each month, that is, how many of the scrobbles for a given month were made with the specific artist: this is what I want to put in the stackplot. Will it be a good choice? Looking back, it probably wasn’t.\nIn any case, for this I need to group the data by month to find the total number of scrobbles for each month, then add a new column where I store the fraction.\n\ndf_prep = df_prep.join( \n        df_prep # self-join to add the total number of scrobbles per month\n        .group_by(\"utc_time\") # group by month\n        .agg(pl.sum(\"count\").alias(\"total_scrobbles\")), # total number of scrobbles per month\n        on=\"utc_time\" \n    ).with_columns(\n        frac=pl.col(\"count\") / pl.col(\"total_scrobbles\") * 100 # calculate the fraction \n)\ndf_prep\n\n\ndf_prep\n\n\nshape: (1_427, 5)\n\n\n\nartist\nutc_time\ncount\ntotal_scrobbles\nfrac\n\n\nstr\ndatetime[μs]\nu32\nu32\nf64\n\n\n\n\n\"Mika\"\n2012-12-01 00:00:00\n1\n334\n0.299401\n\n\n\"U2\"\n2012-12-01 00:00:00\n1\n334\n0.299401\n\n\n\"Caparezza\"\n2012-12-01 00:00:00\n2\n334\n0.598802\n\n\n\"Zucchero\"\n2012-12-01 00:00:00\n13\n334\n3.892216\n\n\n\"Negrita\"\n2012-12-01 00:00:00\n15\n334\n4.491018\n\n\n…\n…\n…\n…\n…\n\n\n\"Hoshimachi Suisei\"\n2025-04-01 00:00:00\n53\n405\n13.08642\n\n\n\"Coldplay\"\n2025-04-01 00:00:00\n73\n405\n18.024691\n\n\n\"Negrita\"\n2025-04-01 00:00:00\n81\n405\n20.0\n\n\n\"Ado\"\n2025-04-01 00:00:00\n87\n405\n21.481481\n\n\n\"Everyone else\"\n2025-04-01 00:00:00\n108\n405\n26.666667\n\n\n\n\n\n\nNow I have a dataframe that contains all the information I need: for each month, I have the artists I listened to, the number of songs played, the number of total scrobbles in that month, and finally the fraction of scrobbles for each artist.\n\nGetting a fixed order for my top artists\nA small detail I wanted to get done was getting a fixed order for my artists, with the “Everyone else” label on top. This is easy to do:\n\nranking = df_prep.group_by(\"artist\").agg(pl.sum(\"count\")).sort(\"count\", descending=True)\norder = ranking[\"artist\"].to_list()\norder\n\nNow for the final, and possibly worst part of the entire preparation: massaging the data into a format that should be easier to plot iteratively with the stackplot.\nThe rough idea is having a dictionary where each key is one of the top artists, and each artist is mapped to a list of values for each month (including the months in which I did not listen to them); then, plotting each artist separately becomes easier. Getting there, however, is anything but.\n\n\nWhich artists am I always listening to?\nThe main problem here is that no artist (other than the “Everyone else” label) was played every month since the start of the data collection:\n\ndf_prep.select(\"artist\", \"utc_time\").group_by(\"artist\").agg(pl.len().alias(\"Months played\")).sort(\"Months played\", descending=True)\n\n\nshape: (16, 2)\n\n\n\nartist\nMonths played\n\n\nstr\nu32\n\n\n\n\n\"Everyone else\"\n145\n\n\n\"Muse\"\n134\n\n\n\"Coldplay\"\n128\n\n\n\"Jovanotti\"\n122\n\n\n\"Negrita\"\n119\n\n\n…\n…\n\n\n\"Daft Punk\"\n76\n\n\n\"Mika\"\n43\n\n\n\"Hoshimachi Suisei\"\n27\n\n\n\"Ado\"\n26\n\n\n\"YOASOBI\"\n25\n\n\n\n\n\n\nSo at the top we have Muse, Coldplay, Jovanotti and Negrita that showed up almost every month; even they, however, are missing for about one year throughout.\nAt the other end of the ladder sit Hoshimachi Suisei, Ado and YOASOBI, three Japanese singers that I discovered about two years ago, and that have been a staple of my playlists ever since. They’ll get their own post in due time.\nClosing this small parenthesis to explain why there are a bunch of missing values, how did I deal with them?\nFirst off, I defined a datetime range that started on the first month of data, and that finished at the end of the data range, with a point every month.\n\nall_time = pl.DataFrame(\n    {\n        \"time\": pl.datetime_range(\n            start=datetime(2012, 12, 1),\n            end=datetime(2025, 4, 20),\n            interval=\"1mo\",\n            eager=True,\n        )\n    }\n)\n\n# Defining the dictionaries to hold the data for the plot\ndict_frac = {}\ndict_abs = {}\ndict_cumulative = {}\nlabels = []\n\nNow the fun part. I group again by artist, and right join on the all_time range defined above. The result is having a new group that has a line for every month in the range, and null values on all rows where the artist is missing.\nThen, I fill in the null values with 0’s (I could also fill in the artist name, but I’m not using it here).\nI then fill each dictionary with a numpy array taken from each column, and conclude by sorting the dictionaries according to the order of artists defined before.\n\nfor gidx, g in df_prep.group_by(\"artist\"):\n    group = (\n        g.with_columns(pl.col(\"utc_time\").dt.truncate(\"1mo\"))\n        .join(all_time, left_on=\"utc_time\", right_on=\"time\", how=\"right\")\n        .with_columns(pl.col(\"frac\").fill_null(0), pl.col(\"count\").fill_null(0))\n    )\n\n    dict_frac[gidx[0]] = group[\"frac\"].to_numpy()\n    dict_abs[gidx[0]] = group[\"count\"].to_numpy()\n    dict_cumulative[gidx[0]] = group[\"count\"].to_numpy()\n    labels.append(gidx[0])\n\ndict_frac = {k: dict_frac[k] for k in order}\ndict_abs = {k: dict_abs[k] for k in order}\ndict_cumulative = {k: dict_cumulative[k] for k in order}\ndict_total = dict(ranking.rows())\n\nWhew. This was not pretty, but one of the positives of this approach is that now I have, for each of the top artists, three things I can play with, and plot: - for each month, the number of songs played - how much of my listening was monopolized by the artist - the cumulative number of scrobbles, showing me the evolution month by month\nThis is a lot of information! In the next section, I’ll use the first and second points, and in later posts I’ll explore the third."
  },
  {
    "objectID": "posts/data_preparation/study_recent_tracks.html#plotting-the-data",
    "href": "posts/data_preparation/study_recent_tracks.html#plotting-the-data",
    "title": "Experiments plotting my top Last.fm artists",
    "section": "Plotting the data",
    "text": "Plotting the data\nAs explained just above, it’s time to put everything into something that’s “good to look at”. Whether I pulled that off, I’ll let the reader decide.\n\nFirst attempt: stackplot\nTo start with, let’s try to prepare the stackplot without any smoothing.\n\nx = all_time[\"time\"].to_numpy()\ny = np.array(list(dict_frac.values()))\ncmap = plt.cm.tab20c\ncolors = cmap(np.linspace(0.1, 0.9, len(order)))\nfig, ax = plt.subplots(figsize=(12, 6))\nax.stackplot(\n    x,\n    y,\n    labels=order,\n    colors=colors,\n)\nfig.legend(loc=\"center right\", ncol=1, fontsize=12, bbox_to_anchor=(1.1, 0.5))\n\nax.set_xlim(x[0], x[-1])\nax.set_ylim(30, 100)\n\nax.set_xlabel(\"Date\", fontsize=14)\nax.set_ylabel(\"Percentage of total plays\", fontsize=14)\n\nText(0, 0.5, 'Percentage of total plays')\n\n\n\n\n\n\n\n\n\nOof. This does not look great. Better smooth it out.\nNote: unfortunately, I lost track of whatever source I used for preparing the following snippet of code, but it definitely started in the stackplot section of the Graph Gallery linked before.\nThe gist of it is adding a gaussian filter to the data, so that most of the jaggedness is removed. I’m also removing any month that has no data at all using a mask.\n\nx = all_time[\"time\"].to_numpy()\ny = np.array(list(dict_frac.values()))\nsigma = 2\nmask = ~(y.sum(axis=0) == 0)\ny = y[:, mask]\nx = x[mask]\ny = gaussian_filter1d(y, sigma=sigma)\n\nI define a specific colormap based on the artists:\n\ncmap = plt.cm.tab20c\ncolors = cmap(np.linspace(0.1, 0.9, len(order)))\n\nNow I can use the matplotlib stackplot function to finally draw my data. Some labels to make the data pretty, but nothing particularly fancy to add here.\n\nfig, ax = plt.subplots(figsize=(12, 6))\nax.stackplot(\n    x,\n    y,\n    labels=order,\n    colors=colors,\n)\nfig.legend(loc=\"center right\", ncol=1, fontsize=12, bbox_to_anchor=(1.1, 0.5))\n\nax.set_xlim(x[0], x[-1])\nax.set_ylim(30, 100)\n\nax.set_xlabel(\"Date\", fontsize=14)\nax.set_ylabel(\"Percentage of total plays\", fontsize=14)\nax.set_title(f\"My Top {top_k} artists over 10 years of Last.fm scrobbles\", fontsize=16)\n\n\n\n\n\n\n\n\nTo be honest, I am not satisfied with this plot at all. It’s hard to read, the color palette is not clear and there isn’t a clear pattern to it. For example, having a color that’s related to the nationality of the artist would make it easier to pick on patterns (shame that the nationality is not readily available from the last.fm data…).\nThe only thing that can somewhat be gleamed is that some of the artists are far more consistent through the years, while others were more common at a certain point and then progressively fell off. Another thing that’s pretty clear is that a couple of years ago my tastes evolved pretty sharply, with Japanese singers taking up a far larger fraction of everything I was listening to.\nAnyway, I am not happy with this plot, so I looked a bit more for alternatives and landed on something that ended up being far more satisfying.\n\n\nSecond attempt: multiple line chart\nThe second plot was heavily inspired by this plot from the gallery.\nI really like that plot: it’s informative and shows the information in an interesting way. Moreover, I have never played around with facecolors and fonts, and that plot inspired me to try something different from the usual “default font on white background”.\nFor this plot, I’m only interested in the actual artists, no need to have “Everyone else” in the way.\n\n# del dict_abs[\"Everyone else\"]\n\n# Defining a new colormap\ncmap = plt.cm.Reds\ncolors = cmap(np.linspace(0.3, 0.9, len(order))[::-1])\n\n# 15 subplots, one for each artist \nfig, axs = plt.subplots(3, 5, figsize=(20, 10), sharex=True, sharey=True)\n# changing the background color\nbackground_color = \"#001219\"\nfig.set_facecolor(background_color)\n\n# Plotting each artist in its own subplot\nfor i, (k, v) in enumerate(dict_abs.items()):\n    # Each plot has its own color \n    color = colors[i]\n    # Finding the right subplot\n    ax = axs[i // 5, i % 5]\n    # Setting the background color of each subplot (it's separate from the fig background)\n    ax.set_facecolor(background_color)\n    # reusing the same mask as before\n    v = v[mask]\n    \n    # Plotting the current artist with the given artist \n    ax.plot(x, v, color=color, zorder=3, linewidth=2)\n    # Here I can set the font \n    ax.set_title(f\"{k}\", color=color, fontsize=14, fontweight=\"bold\", fontname=\"Futura\")\n    \n    # fixing the x and y limits for the current subplot\n    ax.set_xlim(x[0], x[-1])\n    ax.set_ylim(0, 900)\n\n    # changing the color of the axes to fit the dark background\n    ax.tick_params(axis=\"x\", colors=\"white\")\n    ax.tick_params(axis=\"y\", colors=\"white\")\n\n    # removing the top and right spines    \n    ax.spines['top'].set_visible(False)\n    ax.spines['right'].set_visible(False)\n    \n    # Adding the total number of scrobbles in the top right corner\n    ax.text(\n        0.97,\n        0.97,\n        f\"{dict_total[k]} scrobbles\",\n        ha=\"right\",\n        va=\"top\",\n        fontsize=10,\n        color=color,\n        fontname=\"Futura\",\n        transform=ax.transAxes,\n        )\n\n    # Plotting the other artists in grey in the background\n    for _k, v in dict_abs.items():\n        if _k == k:\n            continue\n        v = v[mask]\n        ax.plot(x, v, color=\"grey\", alpha=0.1)\n\n# Adding a main title and a description\nfig.suptitle(\n    \"Weekly scrobbles for the top 15 artists in my Last.fm library\",\n    fontsize=16,\n    color=\"white\",\n    fontname=\"Futura\",\n)\n\ndesc = \"\"\"The data is from my Last.fm profile, Th3Cap. \nScrobbles are aggregated over one month intervals, and the top 15 artists \nare selected based on the total number of scrobbles.\"\"\"\n\nfig.text(\n    0.1,\n    0.98,\n    desc,\n    ha=\"left\",\n    va=\"top\",\n    fontsize=8,\n    color=\"white\",\n    fontname=\"Futura\",\n)\n\n\n\n\n\n\n\n\nNow, this is a plot I am happy about. It might not be the most compact way to represent the data, but I don’t necessarily care. I really like the “dark mode” theme, I like the color map, and plotting the other artists in the background is a nice way to show indirectly how a specific artist compares to the competition on a given month.\nIt’s also interesting to see how some artists appear to be more consistent, while some others are more likely to be a fad that I go back to after some time."
  },
  {
    "objectID": "posts/data_preparation/study_recent_tracks.html#conclusions",
    "href": "posts/data_preparation/study_recent_tracks.html#conclusions",
    "title": "Experiments plotting my top Last.fm artists",
    "section": "Conclusions",
    "text": "Conclusions\nThis post took forever and a half to write, in no small part because of how disappointed I was in the stackplot and how annoying the data wrangling part was.\nI am quite happy with the final plot, however, and at least now I have a way of representing the evolution of an artist over the years. I’m already working on another plot focusing specifically on Muse, and I hope I’ll get that one out much quicker than I did this.\nIn any case, thanks for sticking around until the end, and I hope you enjoyed the journey and maybe learned something from it."
  },
  {
    "objectID": "posts/about-me/pubs.html",
    "href": "posts/about-me/pubs.html",
    "title": "Publications",
    "section": "",
    "text": "Here are my main publications:\n\nRiccardo Cappuzzo, Gael Varoquaux, Aimee Coelho, Paolo Papotti: Retrieve, Merge, Predict: Augmenting Tables with Data Lakes Pre-print, submitted at VLDB2024, PDF\nRiccardo Cappuzzo, Paolo Papotti, and Saravanan Thirumuruganathan: Relational Data Imputation with Graph Neural Networks EDBT2024, PDF\nRiccardo Cappuzzo, Paolo Papotti, and Saravanan Thirumuruganathan: Creating Embeddings of Heterogeneous Relational Datasets for Data Integration Tasks. SIGMOD 2020, PDF\nRiccardo Cappuzzo, Paolo Papotti, Saravanan Thirumuruganathan: EmbDI: Generating Embeddings for Relational Data Integration (Discussion Paper). SEBD 2021 PDF\nRiccardo Cappuzzo, under the supervision of Paolo Papotti, Elena Baralis: Clustering of Categorical Data for Anonymization and Anomaly Detection. Master Thesis at Politecnico di Torino PDF\nRiccardo Cappuzzo, under the supervision of Paolo Papotti: Deep Learning Models for Tabular Data Curation. PhD Thesis at EURECOM/Sorbonne University PDF"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this website\nHi! I’m Riccardo, and you’re on my personal website. I will be using it to share updates on what I am up to, as well as some blog posts on stuff I may find interesting.\n\n\nAbout myself\nI am a research engineer employed by P16, a project within Inria whose objective is developing promising scientific libraries in Python. I am one of the core devs of the Skrub library, and among other things I am working on implementing in the library some of the code I worked on during my postdoc.\nSaid postdoc involved figuring out the “best practices” to integrate information coming from data lakes with tables of interest. I split my time between between the SODA Team at\nInria Saclay and the R&D department of Dataiku.\nBefore that, I obtained a PhD in Computer Science from EURECOM, in Sophia Antipolis (near Nice in the south of France). The subject of the PhD was automating methods to integrate data and impute missing values. I published some of my findings at SIGMOD2020 and EDBT2024.\nAll my publications are available here.\nOn the personal side, I am a big fan of gaming, anime/manga, technology and history.\nI have been living in France since 2016, but for various reasons I am still not as fluent in French as I would like.\nMy personal CV is available here."
  }
]
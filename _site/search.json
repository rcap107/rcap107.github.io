[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this website\nHi! I’m Riccardo, and you’re on my personal website. I will be using it to share updates on what I am up to, as well as some blog posts on stuff I may find interesting.\n\n\nAbout myself\nI am a research engineer employed by P16, a project within Inria whose objective is developing promising scientific libraries in Python. I am one of the core devs of the Skrub library, and among other things I am working on implementing in the library some of the code I worked on during my postdoc.\nSaid postdoc involved figuring out the “best practices” to integrate information coming from data lakes with tables of interest. I split my time between between the SODA Team at\nInria Saclay and the R&D department of Dataiku.\nBefore that, I obtained a PhD in Computer Science from EURECOM, in Sophia Antipolis (near Nice in the south of France). The subject of the PhD was automating methods to integrate data and impute missing values. I published some of my findings at SIGMOD2020 and EDBT2024.\nAll my publications are available here.\nOn the personal side, I am a big fan of gaming, anime/manga, technology and history.\nI have been living in France since 2016, but for various reasons I am still not as fluent in French as I would like.\nMy personal CV is available here."
  },
  {
    "objectID": "posts/posts.html",
    "href": "posts/posts.html",
    "title": "Posts",
    "section": "",
    "text": "SODA Kickoff: a new slide deck for skrub\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\nJun 3, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nNew publication: Retrieve, Merge, Predict\n\n\n\n\n\n\nacademia\n\n\nnews\n\n\n\n\n\n\n\n\n\nMay 19, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nFetching and preparing Last.fm data for beautiful data visualizations\n\n\n\n\n\n\ncode\n\n\ndata visualization\n\n\n\n\n\n\n\n\n\nMay 4, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nNew blog post: testing Skrub categorical encoders\n\n\n\n\n\n\nnews\n\n\nskrub\n\n\n\n\n\n\n\n\n\nMar 30, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nTheorycrafting for Blade and Soul: introduction and damage formula\n\n\n\n\n\n\ncode\n\n\ngames\n\n\nblade and soul\n\n\n\n\n\n\n\n\n\nMar 29, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n2024 wrapped: ups, downs, and some changes\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\nDec 30, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nMy 2024 (so far) in academia\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOct 8, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nMoving up… or sideways?\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\nSep 13, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nNew story on Medium: Retrieve, Merge, Predict\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\nSep 12, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nNew submission: Retrieve, Merge Predict at VLDB2024\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\nFeb 1, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nSome updates\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\nFeb 5, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nStudying the temperature readings\n\n\n\n\n\n\nraspberry\n\n\nIoT\n\n\n\n\n\n\n\n\n\nJul 17, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nCoding the Thermometer\n\n\n\n\n\n\nraspberry\n\n\n\n\n\n\n\n\n\nJul 12, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nPreparing a Raspberry-based thermometer setup\n\n\n\n\n\n\nraspberry\n\n\narduino\n\n\n\nThe device, and the accessories.\n\n\n\n\n\nJul 11, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nIntroducing EmbDI\n\n\n\n\n\n\nembdi\n\n\n\n\n\n\n\n\n\nApr 12, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nPublications\n\n\n\n\n\n\n\n\n\n\n\nApr 7, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nSomething about this blog\n\n\n\n\n\n\nIntroduction\n\n\n\n\n\n\n\n\n\nMar 29, 2022\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/news/academia_experience.html",
    "href": "posts/news/academia_experience.html",
    "title": "My 2024 (so far) in academia",
    "section": "",
    "text": "Disclaimer: this post will mostly be a rant. The reasons for it should become sufficiently clear as I go over my experience in the past few months.\nFor context, I have been working on a paper for the past year or so. Said paper involved running a few thousands experimental configurations over different baselines, then reporting said experiments in a paper that was initially submitted to VLDB2024 (end of January this year). Said paper was rejected at VLDB 2024, submitted to SIGMOD2025, and was rejected again.\nOver the course of only a few months, I had the privilege of experiencing (very courteous) academic blackmailing, extremely negative reviews, burnout, overtime, and possibly the most bullshit rejection I have heard of.\nLet’s start from top, with the academic blackmail. So, VLDB is a single blind conference, which means that the authors are not aware of who the reviewers are, while the reviewers can instead see the names and affiliations of the authors.\nThis is less annoying to manage than double blind (which instead involve anonymizing everything), but has the fun side effect of receiving emails from the reviewers, who might nicely ask to please add their recent papers to yours in a completely non-threatening way by reaching out in private to one of the authors. Bonus points for asking to add references to papers whose code is not available to run and test.\nThis leads me directly into the rejection. I have my fair share of rejected papers – though by this point, most of my rejections are from this singular paper –, and I am keenly aware of how much of an impact “rolling the right reviewers” has on the final outcome. This rejection was especially fun, however, because it came only after a revision period during which I had to re-run most of the experiments (hence the burnout). But as a man once said, “I’m not done yet”: the reasoning for the rejection was that “we did not test a specific baseline, and said baseline works on my machine” (paraphrased). The kicker? The code for the baseline was not available online, and the reviewer did not provide it to us so that we could test it. As a result, the area chair agreed with it, and the paper was rejected after the revision step.\nWhat was even more of a kick in the nether regions was that we even managed to change the mind of the one reviewer that had marked his review as “weak reject, and I am not going to change my mind”. Unfortunately, we also managed to change the mind of one of the “weak accept” reviews to “reject”, and that somehow stuck.\nSomething interesting to note is that, soon after the paper was rejected, the arXiv version received a new citation. This citation was by one of the authors of the baseline. While I am not one to engage in conspirationsm, the coincidence was a bit too perfect for me to ignore.\nWhile I only briefly mentioned the burnout, it was bad. I got sick after every deadline, and I think I developed some degree of PTSD from looking at terminal screens waiting for experimental runs to end. Not the best, especially considering it did not result in success (yet, at least). That’s not good for morale either.\n\nLessons learned\nWhat is the summary of these experiences? 1) The current reviewing system is broken (which is something literally anyone in the field will confirm); 2) academia is rife with politics just like everything else; 3) effort is not likely to be rewarded, and your fate lies less in your hands and in your results, than in the ethereal roll of the dice that will assign a reviewer rather than another in any given round of paper submission.\nThe result? I will be looking for my next position in industry, rather than academia. Stay tuned for when I will write the equivalent post on my industry misgivings once I have enough experience in that."
  },
  {
    "objectID": "posts/news/medium-post.html",
    "href": "posts/news/medium-post.html",
    "title": "New story on Medium: Retrieve, Merge, Predict",
    "section": "",
    "text": "I wrote a blog post on Medium! It summarizes most of the work we did in a more digestible format than an academic paper.\nThe post is available here."
  },
  {
    "objectID": "posts/news/vldb_2024.html",
    "href": "posts/news/vldb_2024.html",
    "title": "New submission: Retrieve, Merge Predict at VLDB2024",
    "section": "",
    "text": "We submitted a revised version of our benchmarking paper to VLDB 2024, with the title “Retrieve, Merge, Predict: Augmenting Tables with Data Lakes”.\nThe preprint can be found on Arxiv, while the code has its own website, and is available on Github.\nWe also release YADL, the semi-synthetic benchmarking data lake that we used to run our experiments. The repository is also available on Github.\nUPDATE June 2024: the paper was rejected. It was extremely disappointing, and I will write a post to detail what happened. It goes without saying that I am not happy about this outcome."
  },
  {
    "objectID": "posts/news/blog-post-skrub-encoders.html",
    "href": "posts/news/blog-post-skrub-encoders.html",
    "title": "New blog post: testing Skrub categorical encoders",
    "section": "",
    "text": "I wrote a new blog post on the Skrub materials website!\nThis one is about finding out how to best encode categorical features using the categorical encoders provided by skrub.\nYou can find the post here."
  },
  {
    "objectID": "posts/news/wherehavibeen.html",
    "href": "posts/news/wherehavibeen.html",
    "title": "Some updates",
    "section": "",
    "text": "It’s been a while, hasn’t it? According the newly added archives page, the latest post in this blog goes back all the way to July 2022… Well, it does not seem like I kept my promise of posting with some frequency.\nNo matter! Something did happen a few months ago (last October), and I have been quite busy because of it. I have moved to Paris! For the next two years (starting last October), I have been/will be working as a Postdoctoral Researcher for Inria Saclay in the SODA Team and at Dataiku in their research department.\nThis is pretty cool! I get to be a part of a strong research group which includes the guys that maintain scikit-learn, and I have contributed with some minor additions to dirtycat: this will definitely be a great learning experience as collaborative programming is something I have very little experience with.\nOn the other side, I get to work in a pretty cool company where everyone looks bright and prepared. That side of the experience will be useful to get an idea of how things work in an enterprise setting, and the kind of problems that enterprise people actually face. And, we get free breakfast every day!\nThus, I am currently splitting the time between these two realities and acting as the connecting thread between the two.\nSo, what is the subject of this research project? Well, we’re still trying to figure that part out. While we started by working on the creation of embeddings of tabular data, now we are moving on to the subject of testing the performance of systems for performing join suggestion. This will be quite the task, given my scarce experience on the subject. We will start from benchmarking a number of different methods, and just figuring that part out will be an adventure.\nI think that’s all for now.\nCheers!"
  },
  {
    "objectID": "posts/about-me/intro-blog.html",
    "href": "posts/about-me/intro-blog.html",
    "title": "Something about this blog",
    "section": "",
    "text": "What is this even about?\nThe idea behind the blog is having a single place where I can put posts on stuff that interests me, or things I have learned over time, or weird side projects I embark in for various reasons. I am also doing this as a writing exercise, and mostly for fun. I am not planning to turn this website into something I dedicate a lot of effort to.\nSome of the subjects I might cover include: * Complications encountered at work, developing, doing research, or anything else. * Interesting experimental results. * Posts about small programming stuff I do in my free time. * Updates on my publications. * Miscellaneous nagging.\n\n\nHow often are you going to update the blog?\nI have no idea at the moment. It’ll depend on how often I find something I’d like to write a post about. Depending on how it goes, this may take months, weeks, days, or just never happen again.\n\n\nTell me about yourself\nCheck the About page for a brief story of my recent life."
  },
  {
    "objectID": "posts/data_preparation/fetch-lastfm.html",
    "href": "posts/data_preparation/fetch-lastfm.html",
    "title": "Fetching and preparing Last.fm data for beautiful data visualizations",
    "section": "",
    "text": "One of the things I enjoy the most about my job is having the opportunity of turning data into images, coming up with ways to tell myself and others a story.\nThe story of this post (and a few more down the line) will revolve around the history of my musical taste: favorite artists, favorite tracks, artists loved then forgotten, and new discoveries that stuck around. It’s also a story about timeseries, which is a type of data I haven’t worked on a lot yet, so I took it as an opportunity to learn something new with it."
  },
  {
    "objectID": "posts/data_preparation/fetch-lastfm.html#how-to-get-your-own-musical-history",
    "href": "posts/data_preparation/fetch-lastfm.html#how-to-get-your-own-musical-history",
    "title": "Fetching and preparing Last.fm data for beautiful data visualizations",
    "section": "How to get your own musical history",
    "text": "How to get your own musical history\nHow am I going to do this? To begin with, I need a lot of data, ideally spanning multiple years and tracking anything I might need to cook up some nice figures. In comes my Last.fm account: the place where (most of) the tracks I’ve played over the past 10 years or so can be found.\nIf you’re not familiar with Last.fm, it’s a website that keeps track of the music you listen to on most streaming services (or music players): each track played is recorded as a scrobble, which is simply a record that includes when the track was being played, the track name, and some additional information about it (the artist, album etc.).\nLast.fm then uses that information to give cool stats about your listening habits, and to recommend artists or events that may interest you. I quite like to track my tracks (pun intended), but I’m not very interested in the recommendation. I found out about the service at the end of 2012, and I have been using it ever since, across many generations of devices and music players. As a result, as of the writing of this post I have I have about 124 thousand scrobbles, which is a lot of data to play with.\nSo, at some point I got an idea: what if I get all that data out of the account, and I use it as a sandbox for preparing figures?\nWell, the first thing to figure out was getting that data out, which can usually be done by either requesting for my data (thanks GDPR), or by looking up the API. In this case, the API was freely available and quite easy to use.\n\n\n\n\n\n\nTip\n\n\n\nOut of curiosity, I tried to get my Spotify data, which goes back about as long, but dumping it and sifting through it I couldn’t find anywhere as much information as I could with Last.fm, which is why I stuck with the latter instead.\n\n\nLuckily, I found an online tool made by Last.fm user ghan64, that saved me the need to write my own script for dumping my data.\nThe tool was very convenient, and I got all my listening history in a few minutes. A good starting point, but then I wondered: what if I need additional information? After all, the data I have contains only the timestamp, the names of the song, the artist and the album, and their respective MusicBrainz ID if avabilable (I won’t be going into the MBID in this post, but it may come into play later). No genres or user tags are available, for example, and there is a lot of missing data.\nTo fill in that missing data, I decided to use the Last.fm API myself so that I could make use of all the information that is available on the website, and add it to my own collection of records."
  },
  {
    "objectID": "posts/data_preparation/fetch-lastfm.html#requesting-info-with-the-last.fm-api",
    "href": "posts/data_preparation/fetch-lastfm.html#requesting-info-with-the-last.fm-api",
    "title": "Fetching and preparing Last.fm data for beautiful data visualizations",
    "section": "Requesting info with the Last.fm API",
    "text": "Requesting info with the Last.fm API\nFor the time being, my requesting code is extremely simple: all it does is handling the request generation of the data by providing a handful of functions that send different requests to the API, and converting the resulting data to either json or Python dictionaries.\nMy requesting.py starts by loading the API key for my account from a file I have on disk, then it sets the constants that are used to contatct the API entry point. If you want to follow along, you will have to prepare your own API key: it’s a very quick process and all it takes is having a Last.fm account.\nimport pprint\nimport requests\nimport json\n\ndef get_api_key():\n    with open(\"api-key.id\", \"r\") as file:\n        api_key = file.read().strip()\n    return api_key\napi_key = get_api_key()\napi_url = \"http://ws.audioscrobbler.com/2.0\" \napi_method = \"GET\"  \nThen, I need a requesting function that can send a well-formed request to the API entry point. The parameters needed for the request are saved in params, and depend on the specific call.\ndef send_api_request(url, method='GET', headers=None, params=None, data=None):\n    try:\n        # Send the request to the API\n        response = requests.request(method, url, headers=headers, params=params, json=data)\n        # Raise an exception if the request was unsuccessful\n        response.raise_for_status()\n        # Parse the JSON response\n        json_response = response.json()\n        return json_response\n\n    except requests.exceptions.HTTPError as http_err:\n        print(f\"HTTP error occurred: {http_err}\")\n    except requests.exceptions.RequestException as err:\n        print(f\"Error occurred: {err}\")\n    except json.JSONDecodeError:\n        print(\"Error decoding JSON response\")\nThe send_api_request method is used by the other methods, whose only function is preparing the parameters for the request:\n# Fetch information about an artist given their name\ndef get_artist_data(artist_name):\n    params = {\n        \"artist\": artist_name,\n        \"method\": \"artist.getinfo\",\n        \"api_key\": get_api_key(),\n        \"format\": \"json\"\n    }\n    response = send_api_request(api_url, method=api_method, headers=None, params=params)\n    return response\n# Fetch information about a track given the name and the artist\ndef get_track_data(track_name, artist_name):\n    params = {\n        \"track\": track_name,\n        \"artist\": artist_name,\n        \"method\": \"track.getinfo\",\n        \"api_key\": get_api_key(),\n        \"format\": \"json\"\n    }\n    response = send_api_request(api_url, method=api_method, headers=None, params=params)\n    return response\n\n# Given a user, get information about all the top artists\ndef get_user_top_artists(user_name):\n    params = {\n        \"user\": user_name,\n        \"method\": \"user.gettopartists\",\n        \"api_key\": get_api_key(),\n        \"format\": \"json\"\n    }\n    response = send_api_request(api_url, method=api_method, headers=None, params=params)\n    return response\n\n\n\n\n\n\nImportant\n\n\n\nI do not want to flood the API entry point with requests, so I added a delay to the scripts to avoid hitting rate limits.\n\n\nThe main use I’ve had so far consisted in fetching information about the artists: I am interested particularly in the tags fields, because I can use them to extract genres, which are not available in the main dump of scrobbles. That information will come into play in later posts.\nimport json\nimport time\n\nimport polars as pl\nfrom tqdm import tqdm\n\nfrom src.requesting import get_artist_data\n\ndf= pl.read_csv(\"recent-tracks.csv\")\nall_artists = df[\"artist\"].unique().to_list()\n\nartist_data = []\nfor idx, a in tqdm(enumerate(all_artists), total=len(all_artists), desc=\"Fetching artist data\"):\n    # Adding a delay to avoid hitting the API rate limit\n    if idx % 50 == 0:\n        tqdm.write(f\"Sleeping for 1 second to avoid hitting the API rate limit...\")\n        time.sleep(1)\n    try:\n        _data = get_artist_data(a)\n        artist_data.append(_data)\n    except Exception as e:\n        print(f\"Error fetching data for {a}: {e}\")\n\nwith open(\"artist_data.json\", \"w\") as f:\n    json.dump(artist_data, f)\nPutting all this code together, I now have a barebones set of methods that allow to fetch additional info from the API in case I need it. If I need to, I can add more methods to access more API functions."
  },
  {
    "objectID": "posts/data_preparation/fetch-lastfm.html#to-conclude",
    "href": "posts/data_preparation/fetch-lastfm.html#to-conclude",
    "title": "Fetching and preparing Last.fm data for beautiful data visualizations",
    "section": "To conclude…",
    "text": "To conclude…\nThis post was an introduction to my Last.fm plotting project. I figured some context would be needed before moving on to actually using the data, which is what I will be talking about in the next post in this series.\nTo summarize:\n\nI want to explore the 13 years worth of data stored in my Last.fm account through plotting and timeseries.\nTo do this, I downloaded all my data using a user script, and now I have access to the list of all my scrobbles (i.e., the tracks I played over time).\nFinally, I wrote a set of functions that will help me fetching additional information from the Last.fm API if I need it.\n\nThat’s it for this post! Thanks for reading along.\nIn the next post, I will study and plot my top artists, and I’ll explore a bunch of techniques I’ve seen in other people’s visualizations."
  },
  {
    "objectID": "posts/raspberry/the_setup.html",
    "href": "posts/raspberry/the_setup.html",
    "title": "Preparing a Raspberry-based thermometer setup",
    "section": "",
    "text": "To begin with, let’s consider the material I am working with, to have an idea of what can be done with it and what might be needed to build something clever.\nFirst off, my board is a Raspberry Pi 2 Model B (2015), which features neither a Wifi nor a Bluetooth connection. This did cause me quite a few headaches, since connecting wireless turned out to be more complex than I thought.\nI am running the Pi using a completely unofficial power supply, which regularly leads the board to print a very ominous “Undervoltage detected” message on the CLI.\nI also own an Arduino Uno board, with which I am far less familiar as I cannot use Python to wrestle results out of it. I will still try to get it to work, and report it here.\nFor the Uno, I got an adapter for MicroSD cards, so that it would be possible to use the microcontroller to log sensor data. I also need to get a fairly small MicroSD card because the Uno cannot handle more than 5-ish GBs of storage.\nIn my plans, I wanted to access the Pi remotely via wifi using SSH, which is problematic as the model I am working with does not have an integrated Wifi antenna. For this reason, I went to an electronics store and bought the first USB Wifi antenna, without checking whether it would work on Linux devices. Turns out, it didn’t. Which means that I had to look for a different model, and ended up purchasing a (model-name)[model-link].\nSince the first idea I got was using the Pi as a thermometer, the first sensor I purchased was a DHT22 temperature-humidity sensor. The version I purchased is slightly different from what I have seen on online guides, but the functionality is the same.\nI am looking to purchase another Pi, but since I am still going to use it as a sensor base, I will be getting either a Pi Zero (whenever they become available again!), or a Pico."
  },
  {
    "objectID": "posts/bns/post-bns-intro.html",
    "href": "posts/bns/post-bns-intro.html",
    "title": "Theorycrafting for Blade and Soul: introduction and damage formula",
    "section": "",
    "text": "Videogames are my biggest passion, and trying to play well is something I do whenever I enjoy a game. For me, minmaxing is part of the fun.\nNot all games can be optimized in the same way. In a lot of cases, it’s a combination of multiple factors, from mechanics to theorycrafting. As you might imagine, this post will focus on the latter."
  },
  {
    "objectID": "posts/bns/post-bns-intro.html#what-is-this-about",
    "href": "posts/bns/post-bns-intro.html#what-is-this-about",
    "title": "Theorycrafting for Blade and Soul: introduction and damage formula",
    "section": "What is this about?",
    "text": "What is this about?\nThe idea behind this post and the next posts in this series is trying to optimize how I play a specific game, and describing how I did it. For this series I will be talking about a MMORPG called Blade & Soul NEO. The original Blade and Soul is almost 10 years old, but a “Classic” version was released a few weeks ago, and after clocking more than 4000 hours on the original version, I decided to give the revamp a try.\nOverall, I have been enjoying my time, and as a result I started looking into ways to improve my gameplay. In particular, I looked into how I could improve how fast I can kill bosses in PvE (Player vs Environment). Unsurprisingly, part of the solution is just getting better equipment. However, this got me thinking: how can I optimize the equipment I have to deal as much damage to bosses as I can?\nOne thing led to another, and I ended up writing code that would generate all possible combinations of equipment sets to find the best. I had a lot of fun working on it, and I thought it might be interesting, so I decided to write about it in these posts.\nSo, maybe you’re interested in the game itself, or in how I used Python, Google Docs, and various AI tools to maximize my deepz. Or maybe you’re just interested in the ramblings of a mad man that clearly has way too much time on his hands. In any case, please follow along and dive into the rabbit hole with me!"
  },
  {
    "objectID": "posts/bns/post-bns-intro.html#the-basics-how-does-equipment-affect-a-character",
    "href": "posts/bns/post-bns-intro.html#the-basics-how-does-equipment-affect-a-character",
    "title": "Theorycrafting for Blade and Soul: introduction and damage formula",
    "section": "The basics: how does equipment affect a character?",
    "text": "The basics: how does equipment affect a character?\nLike in many other games, character attributes (or stats) in Blade and Soul are given by a combination of base stats and additional stats. Base stats depend on the character’s class, level and some other sources, and remain mostly constant over time. On the other hand, additional stats are provided by a character’s equipment, and can change very easily simply by swapping different equipment pieces.\nAttributes can be either offensive, or defensive: the first category includes Attack Power (AP), Accuracy, Critical Rate, and Critical Damage; the second HP, Defense, Evasion, and some more. I will focus mostly on the offensive stats, as defensive stats are not as interesting to optimize, and are often treated as “dump stats”.\n\n\n\nA screenshot showing different pieces of equipment.\n\n\nEquipment in BnS can be divided broadly in weapon and accessories, soulshields and badges. All items can have one or more attributes, whose value will be added directly to the base stats of the character.\n\n\n\nA screenshot showing an earring and its stats.\n\n\nThis earring, for example, has 40 Accuracy and 76 Critical Rate (let’s ignore all the other stats for now). By equipping it, the Accuracy and Critical Rate of my character will increase by 40 and 76 respectively.\nSoulshields provide additional attributes, which are added to the base stats like other equipment. For example, this soulshield gives HP (increasing how much damage my character can take), as well as Critical Damage and Defense. \nEach piece of equipment (or soulshield slice) is generated with a random set of statistics, and a random amount for each statistic. As a result, a player may collect a large number of copies of the same piece, and need to choose which one to use. This is particularly important for soulshields, because it is very easy to obtain their copies:\n\n\n\nMultiple soulshield slices\n\n\nGiven this context, the problem I am try to solve is the following:\n\n\n\n\n\n\nImportant\n\n\n\nI have many copies of each accessory, and a huge number of possible combinations of accessories. How can I find the best set of equipment to use to maximize the damage my character deals to bosses?\n\n\nTo start working towards a solution, I first need to define how damage is dealt, and how to integrate that in my problem formulation, so that’s what the next section is for."
  },
  {
    "objectID": "posts/bns/post-bns-intro.html#damage-in-blade-and-soul-why-and-how",
    "href": "posts/bns/post-bns-intro.html#damage-in-blade-and-soul-why-and-how",
    "title": "Theorycrafting for Blade and Soul: introduction and damage formula",
    "section": "Damage in Blade and Soul: why and how",
    "text": "Damage in Blade and Soul: why and how\nIn PvE, the objective is dealing damage to enemy bosses to reduce their HP to 0 before they do the same to the player. In fact, the main gameplay loop involves killing bosses, getting their loot, improving equipment with the loot, then challenging stronger bosses for even better loot.\nTypically, boss fights require players hit the bosses as hard as possible, while executing a set of mechanics (avoiding the boss’ attacks, activating items in the arena etc.). If a player is executing everything correctly, then better attributes lead to better damage. In general, having more damage makes fights easier. Furthermore, some bosses have an enrage timer that will immediately kill the player if they cannot eliminate the boss in time.\nI have been talking about damage to bosses all this time, and it’s now time to define how damage is calculated. For this, I will be using a damage formula that was shared in the community, and that goes as follows: \\[\nDamage = AP \\times (Crit\\% \\times CDmg + (1-Crit\\%))\n\\]\nThe Attack Power (AP) is the main offensive stat, as all damage is multiplied by this value; additionally, skills multiply this value by some modifier that depends on which skill is being used. \\(Crit\\%\\) is the probability to land a “Critical hit”: a Critical hit deals additional damage equal to \\(AP\\times CDmg\\). Since not all hits are critical hits, the “Damage” is given by the damage of critical hits weighted by how frequently they occur, in addition to the damage of hits that do not crit.\nAs an example, let’s consider \\(AP=500\\), \\(Crit\\%=55\\%\\), \\(CDmg=150\\%\\): the equation would become \\[\nDamage = 500 \\times (0.55 \\times 1.5 + (1 - 0.55)) = 687.5\n\\]\nIn a vacuum, a character with 687.5 AP and 0 \\(Crit\\%\\) would deal the same damage as a character 500 AP, 55% of crit chance, and 150% of bonus damage for crits. For simplicity, I am ignoring all sorts of class interactions with crits, and just taking the raw numbers.\nClearly, better Crit% and CDmg would result in a larger value for Damage, and therefore better DPS: in practice, those are the main stats that player try to maximize.\nHowever, what we see on a character’s profile is a raw number for AP, Critical rate and Critical damage. How can we turn those into the multipliers used above? Well, while there are no official numbers for this, the community managed to get an estimate of the formula (source), which is what I am using here: \\[\nCrit\\% = \\frac{Crit \\times 96.98979}{Crit + 1124.069}\n\\] where \\(Crit\\) is the Critical rate attribute. \\[\nCDmg = \\frac{CD \\times 290.8}{CD + 2102.36} + 125\n\\] where \\(CD\\) is the Critical damage attribute.\nUsually, players strive to maximize Attack Power, Critical rate and Critical damage. The problem then becomes understanding how these statistics affect the final damage output, and selecting the equipment that leads to the best returns.\nIn conclusion, now I have a (rough) estimation of how much damage my character deals given a certain amount of AP, Critical rate, and Critical damage. While it is not perfect, it’s still a good starting point to optimize for."
  },
  {
    "objectID": "posts/bns/post-bns-intro.html#to-summarize-and-whats-next",
    "href": "posts/bns/post-bns-intro.html#to-summarize-and-whats-next",
    "title": "Theorycrafting for Blade and Soul: introduction and damage formula",
    "section": "To summarize, and what’s next",
    "text": "To summarize, and what’s next\nThis was a lot! Let’s wrap up:\n\nI am trying to optimize the equipment of my character in the game Blade and Soul.\nMy objective is maximizing the amount of damage I deal to enemy bosses.\nI need a metric that lets me whether a set of equipment is good or not.\nI can use datamined formulas to estimate my damage, and use that as as my “objective function”.\n\nNow that I have a formula, I have a better idea of how to rank sets of equipments, and now I need to figure out how to generate all the combinations. This and some more subjects I’d like to cover include:\n\nDrawing figures that use the damage formulas to draw some preliminary conclusions from.\nWriting some software that can select the best set of soulshields given a list of pieces.\nFiguring out a reliable way of getting the soulshield statistics.\nRe-implementing the code I wrote in Python using a different programming language to exercise.\n\nIf you got this far, I hope you found the post interesting, and thanks a lot for reading!"
  },
  {
    "objectID": "posts/embdi/intro-embdi.html",
    "href": "posts/embdi/intro-embdi.html",
    "title": "Introducing EmbDI",
    "section": "",
    "text": "Out of all the work I’ve carried out during my PhD, EmbDI is by far the most important contribution. I’ve dedicated about half of PhD years working on designing, developing and implementing the system, and I can say I have been awarded with a lot of satisfaction from the result. Indeed, there is a lot to talk about here, so I’ll be writing a series of posts where I’ll go into more detail than it’s reasonably needed to get into “the weeds” of what it was like to work on EmbDI. This post will act as introduction and as index for later posts."
  },
  {
    "objectID": "posts/embdi/intro-embdi.html#what-is-embdi",
    "href": "posts/embdi/intro-embdi.html#what-is-embdi",
    "title": "Introducing EmbDI",
    "section": "What is EmbDI?",
    "text": "What is EmbDI?\nEmbDI stands for Embeddings for Data Integration, and is a system that generates embeddings for tabular data (specifically, CSV tables) and then uses said embeddings for performing Entity Resolution and Schema Matching. It was developed during my PhD as a solution to the problem of automating those data curation procedures and does so remarkably well, beating previous state of the art systems by a pretty large margin.\nWhile the ideas behind the implementation of the code were developed together my supervisor (Prof. Paolo Papotti), and another researcher from a different institution (Dr. Saravanan Thirumuruganathan), I wrote the vast majority of the code alone. This, of course, means that the code was a mess, then it got a bit better as I kept on hammering on it to clean up bugs and make it available to other researchers. The code has its repository on github, together with a very in-depth readme and instructions on how to run it.\nEmbDI is written in Python and relies on the usual data science libraries in Pandas, Numpy and sklearn, as well as the gensim library for training the embeddings themselves.\nThe code has been thoroughly tested to produce results for a paper that made it to SIGMOD 2020, which can be found here. The slides used for the SIGMOD 2020 presentation are also available here."
  },
  {
    "objectID": "posts/embdi/intro-embdi.html#how-does-embdi-work-in-short",
    "href": "posts/embdi/intro-embdi.html#how-does-embdi-work-in-short",
    "title": "Introducing EmbDI",
    "section": "How does EmbDI work, in short?",
    "text": "How does EmbDI work, in short?\nEmbDI is a Python-based data integration system that takes as input relational tables as CSV files, and returns as output a list of matches of tuples that represent the same entity (a task I’ll call Entity Resolution), or a list of matches between columns that represent the same attribute (Schema Matching).\n\n\n\npng\n\n\nThe procedure followed by EmbDI is the following:\n\nTake a pair of relational tables, then clean them to remove problematic strings and characters.\nConvert the relational tables into a graph.\nTraverse the graph by using random walks, then gather the random walks in a training corpus (that is, a text file that contains all the random walks).\nFeed the training corpus to the word2vec embeddings training algorithm.\nUse the geometric properties of the embeddings to find related tuples and columns.\n\nThis is, of course, extremely summarized. I would like to explore how EmbDI works more in depth over the course of a series of blog posts that will go into far more detail than what I’ll do here, so I’ll make sure to link back to those posts once they’re ready."
  },
  {
    "objectID": "posts/raspberry/study-temperature.html",
    "href": "posts/raspberry/study-temperature.html",
    "title": "Studying the temperature readings",
    "section": "",
    "text": "Before we begin, a couple of notes on plots dataframes: this post has been prepared by converting a jupyter notebook into a markdown file using nbconvert. As it turns out, the default output format used by nbconvert to display dataframes isn’t rendered correctly by the Hugo backend I’m using for the blog. I was not able to find a “clean” solution for it, so for this post (and until I’ll find the actual fix) all tables are represented in raw html, rather than in a more decent format. I also need to figure out a better way of saving the plots, which allow to specify a facecolor, so that figures have a white background both in light and dark mode. For the time being, it’s possible to switch to light mode to fix that particular problem.\nThe nbconvert-generated version of this post is also available here."
  },
  {
    "objectID": "posts/raspberry/study-temperature.html#part-1-the-attic",
    "href": "posts/raspberry/study-temperature.html#part-1-the-attic",
    "title": "Studying the temperature readings",
    "section": "Part 1: the attic",
    "text": "Part 1: the attic\n\nReading and cleaning the data\nThe temperature/humidity readings are stored in a csv file, together with the timestamp of the reading. Let’s start with the readings coming from the sensor left in the attic.\ndf = pd.read_csv('log.txt', header=0, names=['date','time', 'temperature', 'humidity'])\nThe next few commands are there for turning the datetime into a more manageable format, which helps with plotting later. In particular, since the date and time columns are read as strings, the following lines convert them back into datetime format.\nThen, the two are combined back into a singular datetime object for each row, with the entire timestamp.\ndf['time'] = df['time'].apply(datetime.time.fromisoformat)\ndf['date'] = df['date'].apply(datetime.date.fromisoformat)\n\ndf['datetime'] = pd.to_datetime([datetime.datetime.combine(a, b) for a,b in zip(df['date'], df['time'])])\nNow that I have a unique datetime index for every row, I can reset the index of the dataframe so that it uses the timestamp, rather than the regular ID.\nThe next step is resampling the timeline in order to reduce the number of readings, and to reduce the effect of anomalous readings. In this example, readings are grouped every 30 minutes, then they are aggregated by using the mean function.\nOut of curiosity, and following a procedure I am not completely sure of, I am trying to plot the variance of the measurement over the same interval of time by aggregating with var, then plotting using the fill_between function.\n# Resetting the index so that it uses the datetime, then aggregating according to mean and variance. \ndf_resampled = df.set_index('datetime').resample('30Min').mean()\n\ndf_var = df.set_index('datetime').resample('30Min').var()\nTime for the first plots. Here, I am plotting all the readings from the moment I activated the process, to the moment I saved it on disk to study the readings. To better distinguish between days, I am searching for the timestamps with time (0,0) (i.e. midnight), then I’m saving them in the variable midnight.\nThe command df_resampled.index.time==datetime.time(0,0) looks in the .time attribute of the datetime objects in the index and checks if the value contained therein is the same as datetime.time(0,0).\nThe first plot will show the timeseries of temperature and humidity, along with their (supposed) error bars. Since I’m using the same code for plotting both temperature and humidity, I wrote a simple function that takes as input the dataframe and the variable of interest, then prepares the resulting plot.\ndef plot_overall_var(df, tgt_var, df_var=None):\n\n    def get_bounds(df_base, df_var, column):\n        '''Preparing the upper and lower bounds by adding the variance\n        '''\n        y1 = df_base[column] + df_var[column]\n        y2 = df_base[column] - df_var[column]\n\n        return y1,y2\n\n    # Setting the size of the figure\n    fig = plt.figure(figsize=(16,4))\n\n    # Plotting the main line\n    ax = sns.lineplot(x=df_resampled.index, y=df_resampled[tgt_var])\n    plt.title(tgt_var)\n\n    # If provided, adding the variance\n    if df_var is not None:\n        yhigh, ylow = get_bounds(df_resampled, df_var, tgt_var)\n        plt.fill_between(df_resampled.index, ylow, yhigh, color='orange')\n\n    # Plotting vertical lines for highlighting the change of date\n    midnight = df_resampled.loc[df_resampled.index.time==datetime.time(0,0)].index\n    for day in midnight:\n        plt.axvline(day)\n        \n    # Plotting ticks every 3 hours\n    _ = ax.xaxis.set_major_locator(mdates.HourLocator(interval=3))\n    \n    # Rotating the ticks\n    ax.xaxis.set_tick_params(rotation=60)\n    \n    # Tightening the result\n    plt.tight_layout()\n\nplot_overall_var(df_resampled, 'temperature', df_var)\nplot_overall_var(df_resampled, 'humidity', df_var)\n \n\n\nPlotting results by day\nIn the next section, I will reshape the data with the objective of comparing the readings gathered in different days, to look for potential similarities and differences. To do so, I’ll first split the datetime index into columns date and time, then I will pivot the table so that the new index will be time, and the columns will report temperature and humidity for each day. As you’ll notice, a bunch of values are missing and get with the value NaN: this is due to the fact that I started the data collection around 14h on July 13th.\ndf_resampled['date'] = df_resampled.index.date\ndf_resampled['time'] = df_resampled.index.time\n\ndf_pivot=df_resampled.pivot(index='time', columns='date')\n# Displaying only the first 10 rows\ndf_pivot.head(10)\n\n\n\n\n\n\n\n\ntemperature\n\n\nhumidity\n\n\n\n\ndate\n\n\n2022-07-13\n\n\n2022-07-14\n\n\n2022-07-15\n\n\n2022-07-16\n\n\n2022-07-13\n\n\n2022-07-14\n\n\n2022-07-15\n\n\n2022-07-16\n\n\n\n\ntime\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n00:00:00\n\n\nNaN\n\n\n29.640351\n\n\n31.483636\n\n\n31.187500\n\n\nNaN\n\n\n44.256141\n\n\n41.012727\n\n\n38.969643\n\n\n\n\n00:30:00\n\n\nNaN\n\n\n29.321428\n\n\n31.041071\n\n\n30.917544\n\n\nNaN\n\n\n44.341071\n\n\n40.332143\n\n\n38.903509\n\n\n\n\n01:00:00\n\n\nNaN\n\n\n29.025000\n\n\n30.600000\n\n\n30.540741\n\n\nNaN\n\n\n44.517857\n\n\n40.024561\n\n\n39.196297\n\n\n\n\n01:30:00\n\n\nNaN\n\n\n28.775000\n\n\n30.238182\n\n\n30.076786\n\n\nNaN\n\n\n44.776786\n\n\n39.936363\n\n\n39.655357\n\n\n\n\n02:00:00\n\n\nNaN\n\n\n28.519298\n\n\n29.961403\n\n\n29.866071\n\n\nNaN\n\n\n44.966667\n\n\n40.045614\n\n\n39.594642\n\n\n\n\n02:30:00\n\n\nNaN\n\n\n28.323214\n\n\n29.657143\n\n\n29.710527\n\n\nNaN\n\n\n44.798214\n\n\n39.983929\n\n\n40.035088\n\n\n\n\n03:00:00\n\n\nNaN\n\n\n28.083929\n\n\n29.340000\n\n\n29.500000\n\n\nNaN\n\n\n44.835714\n\n\n39.558182\n\n\n40.560714\n\n\n\n\n03:30:00\n\n\nNaN\n\n\n27.839286\n\n\n29.054386\n\n\n29.285714\n\n\nNaN\n\n\n45.023214\n\n\n39.131579\n\n\n42.167857\n\n\n\n\n04:00:00\n\n\nNaN\n\n\n27.514286\n\n\n28.794643\n\n\n29.000000\n\n\nNaN\n\n\n45.291072\n\n\n39.330357\n\n\n44.741072\n\n\n\n\n04:30:00\n\n\nNaN\n\n\n27.251786\n\n\n28.581818\n\n\n28.656364\n\n\nNaN\n\n\n45.671428\n\n\n40.038182\n\n\n45.830909\n\n\n\n\n\nHere I’m plotting the values of temperature and humidity recorded on each day.\ntime_range = pd.timedelta_range(start='1 day', end='2 days', freq='3H')\nfig, axs = plt.subplots(2,1, sharex=True, figsize=(10, 8))\ndf_pivot=df_resampled.pivot(index='time', columns='date')\na=df_pivot['temperature'].plot(title='Temperature', ylabel='Temperature (*C)', ax=axs[0], legend=True)\n\ndf_pivot=df_resampled.pivot(index='time', columns='date')\ndf_pivot['humidity'].plot(ax=axs[1], title='Humidity', ylabel='Humidity (%)', legend=True)\n\n# plt.legend([a.lines])\nplt.tight_layout()\nplt.savefig('temp-humid-attic.png', transparent=False, facecolor='white')\n\n\n\npng\n\n\n\n\nFinding the daily extremes\nIn this section, I will be parsing the data and look for the daily extremes (maximum and minimum) for temperature and humidity, and the time at which they occurred.\n# Simple dictionary for formatting the unit of measurement that will be printed\nd_form = {'temperature': '*C', 'humidity': '%'}\n# Grouping by day\nfor idx, day_g in df_resampled.groupby('date'):\n    print(f'Date: {idx}')\n    \n#     Maximum\n#     idxmax returns the position of the maximum value\n    aggr_i = day_g[['temperature', 'humidity']].aggregate(pd.Series.idxmax)\n    aggr_m = day_g[['temperature', 'humidity']].aggregate(pd.Series.max)\n#     Putting the values together for pretty printing\n    df_t=pd.concat([aggr_i, aggr_m], axis=1)\n    for i, val in df_t.iterrows():\n        print(f'Max {i:&gt;12} = {val[1]:.1f} {d_form[i]:&gt;2} at {val[0].time()}')\n        \n#     Minimum\n#     idxmax returns the position of the maximum value\n    aggr_i = day_g[['temperature', 'humidity']].aggregate(pd.Series.idxmin)\n    aggr_m = day_g[['temperature', 'humidity']].aggregate(pd.Series.min)\n#     Putting the values together for pretty printing\n    df_t=pd.concat([aggr_i, aggr_m], axis=1)\n    for i, val in df_t.iterrows():\n        print(f'Min {i:&gt;12} = {val[1]:.1f} {d_form[i]:&gt;2} at {val[0].time()}')\nDate: 2022-07-13\nMax  temperature = 33.5 *C at 18:00:00\nMax     humidity = 49.8  % at 14:30:00\nMin  temperature = 30.0 *C at 23:30:00\nMin     humidity = 42.6  % at 18:30:00\nDate: 2022-07-14\nMax  temperature = 35.6 *C at 18:00:00\nMax     humidity = 47.4  % at 10:30:00\nMin  temperature = 26.4 *C at 07:30:00\nMin     humidity = 38.8  % at 19:00:00\nDate: 2022-07-15\nMax  temperature = 36.3 *C at 16:00:00\nMax     humidity = 43.7  % at 10:00:00\nMin  temperature = 27.6 *C at 07:00:00\nMin     humidity = 33.4  % at 16:00:00\nDate: 2022-07-16\nMax  temperature = 36.0 *C at 18:00:00\nMax     humidity = 51.1  % at 09:00:00\nMin  temperature = 27.7 *C at 07:30:00\nMin     humidity = 38.9  % at 00:30:00"
  },
  {
    "objectID": "posts/raspberry/study-temperature.html#part-2-the-bedroom",
    "href": "posts/raspberry/study-temperature.html#part-2-the-bedroom",
    "title": "Studying the temperature readings",
    "section": "Part 2: the bedroom",
    "text": "Part 2: the bedroom\nIn this section, I will be repeating most of what was done in Part 1: the main difference will be the location of the sensor. Rather than keeping it in the attic, in fact, I was keeping this on the desk in my bedroom. In particular, the sensor was exposed to the heat coming from a pretty chonky desktop PC, as well as two monitors, and it is positioned close to a window.\ndf = pd.read_csv('readings-room.csv', header=0, names=['date','time', 'temperature', 'humidity'])\ndf['time'] = df['time'].apply(datetime.time.fromisoformat)\ndf['date'] = df['date'].apply(datetime.date.fromisoformat)\n\ndf['datetime'] = pd.to_datetime([datetime.datetime.combine(a, b) for a,b in zip(df['date'], df['time'])])\n# Resetting the index so that it uses the datetime, then aggregating according to mean and variance. \ndf_resampled = df.set_index('datetime').resample('30Min').mean()\n\ndf_var = df.set_index('datetime').resample('30Min').var()\nBelow, I’m plotting the overall temperature and humidity readings. Unlike in the case of the attic, however, the variance is far more larger, which leads to a plot that is almost unreadable in the case of humidity. For this reason, I am also plotting the graphs without added bands by calling plot_overall_var without the df_var variable.\nplot_overall_var(df_resampled, 'temperature', df_var)\nplot_overall_var(df_resampled, 'humidity', df_var)\n\nplot_overall_var(df_resampled, 'temperature')\nplot_overall_var(df_resampled, 'humidity')\n\n\n\npng\n\n\n\n\n\npng\n\n\n\n\n\npng\n\n\n\n\n\npng\n\n\n\nPlotting results by day\ndf_resampled['date'] = df_resampled.index.date\ndf_resampled['time'] = df_resampled.index.time\n\ndf_pivot=df_resampled.pivot(index='time', columns='date')\n# Displaying only the first 10 rows\ndf_pivot.head(10)\n\n\n\n\n\n\n\n\ntemperature\n\n\nhumidity\n\n\n\n\ndate\n\n\n2022-07-06\n\n\n2022-07-07\n\n\n2022-07-08\n\n\n2022-07-06\n\n\n2022-07-07\n\n\n2022-07-08\n\n\n\n\ntime\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n00:00:00\n\n\nNaN\n\n\nNaN\n\n\n24.601786\n\n\nNaN\n\n\nNaN\n\n\n60.225000\n\n\n\n\n00:30:00\n\n\nNaN\n\n\n26.792857\n\n\n23.985965\n\n\nNaN\n\n\n62.985715\n\n\n61.821053\n\n\n\n\n01:00:00\n\n\nNaN\n\n\n26.937255\n\n\n23.458929\n\n\nNaN\n\n\n62.368627\n\n\n63.941071\n\n\n\n\n01:30:00\n\n\nNaN\n\n\n26.825000\n\n\n22.831579\n\n\nNaN\n\n\n62.901786\n\n\n66.178947\n\n\n\n\n02:00:00\n\n\nNaN\n\n\n26.421053\n\n\n21.867273\n\n\nNaN\n\n\n63.638596\n\n\n69.143637\n\n\n\n\n02:30:00\n\n\nNaN\n\n\n26.396428\n\n\n22.728572\n\n\nNaN\n\n\n64.705357\n\n\n65.946429\n\n\n\n\n03:00:00\n\n\nNaN\n\n\n26.171930\n\n\n22.787719\n\n\nNaN\n\n\n64.521053\n\n\n64.991228\n\n\n\n\n03:30:00\n\n\nNaN\n\n\n25.918965\n\n\n22.864285\n\n\nNaN\n\n\n65.108621\n\n\n65.960714\n\n\n\n\n04:00:00\n\n\nNaN\n\n\n25.978571\n\n\n23.108929\n\n\nNaN\n\n\n64.525000\n\n\n66.139285\n\n\n\n\n04:30:00\n\n\nNaN\n\n\n25.794736\n\n\n23.110527\n\n\nNaN\n\n\n65.349123\n\n\n66.324561\n\n\n\n\n\nval_idxmax=df_pivot[['temperature', 'humidity']].apply(pd.Series.idxmax)\nval_max=df_pivot[['temperature', 'humidity']].apply(pd.Series.max)\ndf_concat = pd.concat([val_idxmax, val_max],  axis=1).reset_index().pivot(index='date', columns='level_0')\ndf_concat\n\n\n\n\n\n\n\n\n0\n\n\n1\n\n\n\n\nlevel_0\n\n\nhumidity\n\n\ntemperature\n\n\nhumidity\n\n\ntemperature\n\n\n\n\ndate\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2022-07-06\n\n\n18:30:00\n\n\n17:00:00\n\n\n67.537736\n\n\n28.174419\n\n\n\n\n2022-07-07\n\n\n07:30:00\n\n\n17:00:00\n\n\n70.470175\n\n\n28.641072\n\n\n\n\n2022-07-08\n\n\n05:00:00\n\n\n17:30:00\n\n\n70.805263\n\n\n27.100000\n\n\n\n\n\nmi = pd.MultiIndex.from_product([['humidity', 'temperature'], ['timeat', 'value']])\nrows=[]\nfor idx, row in df_concat.iterrows():\n    t = [row[idx][variable]  for variable in ['humidity', 'temperature'] for idx in [0,1] ]\n    rows.append([idx] + t)\ndf_new = pd.DataFrame.from_records(rows)\ndf_new=df_new.set_index(0)\ndf_new.index.rename('date', inplace=True)\ndf_new.columns = mi\ndf_new\n\n\n\n\n\n\n\n\nhumidity\n\n\ntemperature\n\n\n\n\n\n\ntimeat\n\n\nvalue\n\n\ntimeat\n\n\nvalue\n\n\n\n\ndate\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2022-07-06\n\n\n18:30:00\n\n\n67.537736\n\n\n17:00:00\n\n\n28.174419\n\n\n\n\n2022-07-07\n\n\n07:30:00\n\n\n70.470175\n\n\n17:00:00\n\n\n28.641072\n\n\n\n\n2022-07-08\n\n\n05:00:00\n\n\n70.805263\n\n\n17:30:00\n\n\n27.100000\n\n\n\n\n\ntime_range = pd.timedelta_range(start='1 day', end='2 days', freq='3H')\nfig, axs = plt.subplots(2,1, sharex=True, figsize=(10, 8))\ndf_pivot=df_resampled.pivot(index='time', columns='date')\na=df_pivot['temperature'].plot(title='Temperature', ylabel='Temperature (*C)', ax=axs[0], legend=True)\n\ndf_pivot=df_resampled.pivot(index='time', columns='date')\ndf_pivot['humidity'].plot(ax=axs[1], title='Humidity', ylabel='Humidity (%)', legend=True)\n\n# plt.legend([a.lines])\nplt.tight_layout()\nplt.savefig('temp-humid-bedroom.png', transparent=False, facecolor='white')\n\n\n\npng\n\n\n\n\nFinding the daily extremes\n# Simple dictionary for formatting the unit of measurement that will be printed\nd_form = {'temperature': '*C', 'humidity': '%'}\n# Grouping by day\nfor idx, day_g in df_resampled.groupby('date'):\n    print(f'Date: {idx}')\n    \n#     Maximum\n#     idxmax returns the position of the maximum value\n    aggr_i = day_g[['temperature', 'humidity']].aggregate(pd.Series.idxmax)\n    aggr_m = day_g[['temperature', 'humidity']].aggregate(pd.Series.max)\n#     Putting the values together for pretty printing\n    df_t=pd.concat([aggr_i, aggr_m], axis=1)\n    for i, val in df_t.iterrows():\n        print(f'Max {i:&gt;12} = {val[1]:.1f} {d_form[i]:&gt;2} at {val[0].time()}')\n        \n#     Minimum\n#     idxmax returns the position of the maximum value\n    aggr_i = day_g[['temperature', 'humidity']].aggregate(pd.Series.idxmin)\n    aggr_m = day_g[['temperature', 'humidity']].aggregate(pd.Series.min)\n#     Putting the values together for pretty printing\n    df_t=pd.concat([aggr_i, aggr_m], axis=1)\n    for i, val in df_t.iterrows():\n        print(f'Min {i:&gt;12} = {val[1]:.1f} {d_form[i]:&gt;2} at {val[0].time()}')\nDate: 2022-07-06\nMax  temperature = 28.2 *C at 17:00:00\nMax     humidity = 67.5  % at 18:30:00\nMin  temperature = 26.7 *C at 21:30:00\nMin     humidity = 60.8  % at 22:00:00\nDate: 2022-07-07\nMax  temperature = 28.6 *C at 17:00:00\nMax     humidity = 70.5  % at 07:30:00\nMin  temperature = 24.4 *C at 23:00:00\nMin     humidity = 58.5  % at 23:30:00\nDate: 2022-07-08\nMax  temperature = 27.1 *C at 17:30:00\nMax     humidity = 70.8  % at 05:00:00\nMin  temperature = 21.7 *C at 05:00:00\nMin     humidity = 60.2  % at 00:00:00"
  },
  {
    "objectID": "posts/raspberry/study-temperature.html#part-3-concluding",
    "href": "posts/raspberry/study-temperature.html#part-3-concluding",
    "title": "Studying the temperature readings",
    "section": "Part 3: Concluding",
    "text": "Part 3: Concluding\nLet’s wrap up the post by summarizing what was shown, then making some observations.\nOverall, this was an interesting exercise to play around with timeseries, which is a format I haven’t really considered during my PhD. I had to figure out how to handle them using the functions provided by pandas, then how to use matplotlib to prepare the graphs while getting around the shortcomings of my data-gathering setup. In particular, seaborn was far more stubborn than I expected and I wasn’t able to find a proper way of wrangling the correct plots with the library.\nRegarding the actual data, it’s interesting to see the differences between the readings coming from the two locations. They were gathered over different time periods, and it was far hotter while I was recording the temperature in the attic: this makes direct comparisons not very meaningful, due to how different the external conditions were. Still, the difference in readings between the room and the attic can be explained by the fact that there is no activity in the attic; on the other hand, the sensor in the bedroom picked up on both my activity there: this involved the use of a desktop PC and multiple monitors and additional devices, coupled with the fact that windows can generate a noticeable breeze that goes through the entire top floor of the building. This goes to show that a good ventilato helps a lot with cooling down a hot interior, and that electronic devices work directly against that!\nI’m still working on building a different temperature sensor, and possibly something more complex that would allow to gather further information (a weather station maybe?), so I will definitely come back to this subject at some point in the future."
  },
  {
    "objectID": "posts/raspberry/study-temperature.html#appendix-other-unsuccessful-attempts",
    "href": "posts/raspberry/study-temperature.html#appendix-other-unsuccessful-attempts",
    "title": "Studying the temperature readings",
    "section": "Appendix: Other (unsuccessful) attempts",
    "text": "Appendix: Other (unsuccessful) attempts\nIn this section I’m gathering some additional methods I tried while working on the points shown above. I’m adding these both as a note to myself of what doesn’t work, and for readers to see what can happen while trying stuff out.\nFrom my understanding, part of the reason why these methods did not work quite as well as I hoped is due to the fact that I’m working with datetime objects, rather than “regular” indexing. This results in some funky behavior in some cases.\n\nPlotting struggles\nThe pandas.DataFrame.melt function allows to obtain a “long form” version of a dataframe that pivots around some columns, while keeping other columns as variables. This can be extremely useful for plotting different variables on the same plot using libraries such as seaborn, which is what I will be trying in a moment.\ndf_melted = df_resampled.melt(id_vars=['time', 'date'], value_vars=['temperature'])\ndf_melted['time'] = df_melted['time'].apply(datetime.time.isoformat)\ndisplay(df_melted)\n\n# df_melted = df_melted.sort_values(['date', 'time'])\n\n\n\n\n\n\n\n\ntime\n\n\ndate\n\n\nvariable\n\n\nvalue\n\n\n\n\n\n\n0\n\n\n14:30:00\n\n\n2022-07-13\n\n\ntemperature\n\n\n31.266667\n\n\n\n\n1\n\n\n15:00:00\n\n\n2022-07-13\n\n\ntemperature\n\n\n32.048214\n\n\n\n\n2\n\n\n15:30:00\n\n\n2022-07-13\n\n\ntemperature\n\n\n32.547368\n\n\n\n\n3\n\n\n16:00:00\n\n\n2022-07-13\n\n\ntemperature\n\n\n32.926316\n\n\n\n\n4\n\n\n16:30:00\n\n\n2022-07-13\n\n\ntemperature\n\n\n33.198245\n\n\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n\n\n147\n\n\n16:00:00\n\n\n2022-07-16\n\n\ntemperature\n\n\n35.548214\n\n\n\n\n148\n\n\n16:30:00\n\n\n2022-07-16\n\n\ntemperature\n\n\n35.810526\n\n\n\n\n149\n\n\n17:00:00\n\n\n2022-07-16\n\n\ntemperature\n\n\n35.903573\n\n\n\n\n150\n\n\n17:30:00\n\n\n2022-07-16\n\n\ntemperature\n\n\n35.987719\n\n\n\n\n151\n\n\n18:00:00\n\n\n2022-07-16\n\n\ntemperature\n\n\n36.000000\n\n\n\n\n\n152 rows × 4 columns\n\n\nA “melted” dataframe can be plotted fairly easily using a command such as the one below. Besides the x- and y-axes, it is possible to distinguish the lines by hue (in this case, different colors for different dates) and by style (different line style for different variables). Here, it does not make a lot of sense to plot humidity and temperature in the same plot, but it is useful for the sake of the example.\nSomething else to note in the plot below is that the x-axis starts at 14:30, then continues for 24 hours until 14:00 the next day: this is due to the fact that the data captured by the log is incomplete for the day of 2022-07-13. After searching around for a while, I wasn’t able to reset the ticks on the x-axis to make them work properly, so I had to look for some workarounds.\nIndeed, the current plot shows a pretty major inconsistency at 0:00, which is when the date shifts from July 13th to July 14th, which is also why the orange line “jumps” below and seems to be a continuation of the blue line: this is exactly what that is!\nfig = plt.figure(figsize=(12,4))\nax = sns.lineplot(data = df_melted, x='time', y='value', hue='date', style='variable')\nax.tick_params(rotation=90)\n\n\n\npng\n\n\nIt is possible to build a plot by using the full datetime object, rather than splitting by day, and the result is shown below:\ndf_melted = df_resampled.reset_index().melt(id_vars=['datetime'], value_vars=['temperature'])\ndisplay(df_melted)\n\nfig = plt.figure(figsize=(12,4))\nax = sns.lineplot(data = df_melted, x='datetime', y='value')\nax.tick_params(rotation=90)\n\n\n\n\n\n\n\n\ndatetime\n\n\nvariable\n\n\nvalue\n\n\n\n\n\n\n0\n\n\n2022-07-13 14:30:00\n\n\ntemperature\n\n\n31.266667\n\n\n\n\n1\n\n\n2022-07-13 15:00:00\n\n\ntemperature\n\n\n32.048214\n\n\n\n\n2\n\n\n2022-07-13 15:30:00\n\n\ntemperature\n\n\n32.547368\n\n\n\n\n3\n\n\n2022-07-13 16:00:00\n\n\ntemperature\n\n\n32.926316\n\n\n\n\n4\n\n\n2022-07-13 16:30:00\n\n\ntemperature\n\n\n33.198245\n\n\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n\n\n147\n\n\n2022-07-16 16:00:00\n\n\ntemperature\n\n\n35.548214\n\n\n\n\n148\n\n\n2022-07-16 16:30:00\n\n\ntemperature\n\n\n35.810526\n\n\n\n\n149\n\n\n2022-07-16 17:00:00\n\n\ntemperature\n\n\n35.903573\n\n\n\n\n150\n\n\n2022-07-16 17:30:00\n\n\ntemperature\n\n\n35.987719\n\n\n\n\n151\n\n\n2022-07-16 18:00:00\n\n\ntemperature\n\n\n36.000000\n\n\n\n\n\n152 rows × 3 columns\n\n\n\n\n\npng\n\n\nNow, this plot looks much clearer, however it’s not exactly what I wanted. For this reason, I decided to get rid of the offending data series (i.e. the incomplete series of readings from the first day) and plot the data for the remaining days. This isn’t really the best way of doing it, which is why I went for the solution in the first section of this post, but it gets the job done at least.\nSo, now my objective is selecting only the readings that start after the first day has ended, which can be done using either one of the two commands below, whose output is exactly the same.\ndf_resampled.index returns the index of the dataframe, which is in datetime format: this allows to compare it with a different datetime object that I am declaring (either fromisoformat(...) or date(...)). The comparison allows to find the datetime indices that occur after the given value, and keep only those.\ndf_1 =df_resampled.loc[df_resampled.index &gt;= datetime.datetime.fromisoformat('2022-07-14 00:00:00')]\n\ndf_2 =df_resampled.loc[df_resampled.index.date &gt; datetime.date(2022, 7,13)]\n\n# Checking that the two indices are equal\nall(df_1.index == df_2.index)\nTrue\nNow that there is a cleaner dataframe to plot, the usual lineplot function can be used to prepare the temperature and humidity comparisons. |\ndf_melted = df_1.melt(id_vars=['time', 'date'], value_vars=['temperature'], value_name='temp')\ndf_melted['time'] = df_melted['time'].apply(datetime.time.isoformat)\ndf_melted = df_melted.sort_values(['date', 'time'])\n\nfig = plt.figure(figsize=(10,4))\nax = sns.lineplot(data = df_melted, x='time', y='temp', hue='date')\nax.tick_params(rotation=90)\n\n\n\npng\n\n\ndf_melted = df_1.melt(id_vars=['time', 'date'], value_vars=['humidity'], value_name='hum')\ndf_melted['time'] = df_melted['time'].apply(datetime.time.isoformat)\ndf_melted = df_melted.sort_values(['date', 'time'])\n\nfig = plt.figure(figsize=(10,4))\nax = sns.lineplot(data = df_melted, x='time', y='hum', hue='date')\nax.tick_params(rotation=90)\n\n\n\npng\n\n\n\n\nExtracting extremes\nAfter splitting the readings on a day-by-day basis using pivot, I looked for a way of getting the extremes using the same table. Turns out, this was far more complex than I thought, especially for printing them in an easy-to-read way.\nLet’s start by taking a look at df_pivot again:\ndisplay(df_pivot.head())\n\n\n\n\n\n\n\n\ntemperature\n\n\nhumidity\n\n\n\n\ndate\n\n\n2022-07-13\n\n\n2022-07-14\n\n\n2022-07-15\n\n\n2022-07-16\n\n\n2022-07-13\n\n\n2022-07-14\n\n\n2022-07-15\n\n\n2022-07-16\n\n\n\n\ntime\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n00:00:00\n\n\nNaN\n\n\n29.640351\n\n\n31.483636\n\n\n31.187500\n\n\nNaN\n\n\n44.256141\n\n\n41.012727\n\n\n38.969643\n\n\n\n\n00:30:00\n\n\nNaN\n\n\n29.321428\n\n\n31.041071\n\n\n30.917544\n\n\nNaN\n\n\n44.341071\n\n\n40.332143\n\n\n38.903509\n\n\n\n\n01:00:00\n\n\nNaN\n\n\n29.025000\n\n\n30.600000\n\n\n30.540741\n\n\nNaN\n\n\n44.517857\n\n\n40.024561\n\n\n39.196297\n\n\n\n\n01:30:00\n\n\nNaN\n\n\n28.775000\n\n\n30.238182\n\n\n30.076786\n\n\nNaN\n\n\n44.776786\n\n\n39.936363\n\n\n39.655357\n\n\n\n\n02:00:00\n\n\nNaN\n\n\n28.519298\n\n\n29.961403\n\n\n29.866071\n\n\nNaN\n\n\n44.966667\n\n\n40.045614\n\n\n39.594642\n\n\n\n\n\nThis dataframe has a two-level header, with one half of the column covering the temperature on different days, and the other half recording humidity in the same periods of time. The index is a timeseries (not a datetime anymore, since the date is recorded in the columns).\n# Extracting the extremes and saving them in dataframes\nval_idxmax=df_pivot[['temperature', 'humidity']].apply(pd.Series.idxmax)\nval_max=df_pivot[['temperature', 'humidity']].apply(pd.Series.max)\n# Concatenating dataframes\ndf_concat = pd.concat([val_idxmax, val_max],  axis=1).reset_index().pivot(index='date', columns='level_0')\ndf_concat\n\n\n\n\n\n\n\n\n0\n\n\n1\n\n\n\n\nlevel_0\n\n\nhumidity\n\n\ntemperature\n\n\nhumidity\n\n\ntemperature\n\n\n\n\ndate\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2022-07-13\n\n\n14:30:00\n\n\n18:00:00\n\n\n49.774510\n\n\n33.494737\n\n\n\n\n2022-07-14\n\n\n10:30:00\n\n\n18:00:00\n\n\n47.358182\n\n\n35.562499\n\n\n\n\n2022-07-15\n\n\n10:00:00\n\n\n16:00:00\n\n\n43.696364\n\n\n36.289285\n\n\n\n\n2022-07-16\n\n\n09:00:00\n\n\n18:00:00\n\n\n51.060715\n\n\n36.000000\n\n\n\n\n\nThe result above kind of works, but is not very understandable: on the left, we have the time at which the extreme was measured and on the right we have the actual measured value.\nIt would be nicer to have a format similar to the original df_pivot, with temperature and humidity on top and the two variables timeat and measured value below. This can be done using a MultiIndex, which in this case I am building with the function .from_product. This function executes the cartesian product of the iterables passed as inputs and produces a MultiIndex object where each iterable represents a level in the multiindex.\nmi = pd.MultiIndex.from_product([['humidity', 'temperature'], ['timeat', 'value']])\nmi\nMultiIndex([(   'humidity', 'timeat'),\n            (   'humidity',  'value'),\n            ('temperature', 'timeat'),\n            ('temperature',  'value')],\n           )\nAt this point, I was looking for a way of massaging the columns and the index in order to make them match the multiindex, and this is what I came up with. Pretty ugly, but it works.\nFor each row in df_concat, I select the two columns of the multiindex (timeat and value, with labels 0 and 1 respectively), and then the specific variable (either humidity or temperature), so that the content of the list is:\n[row[timeat][humidity], row[value, humidity], row[timeat][temperature], row[value][temperature]]\nFinally, I add the index (i.e. the date) to the list, then I build the dataframe starting from those records. In the new dataframe, the index is then set to use the date column added to the list.\nThis is one way of printing the values, however I later realized that this makes actual pretty printing far more difficult, and the entire process thus far was quite clunky and “unpythonic”, which is why I decided to something else.\nrows=[]\nfor idx, row in df_concat.iterrows():\n    t = [row[i_][variable]  for variable in ['humidity', 'temperature'] for i_ in [0,1] ]\n    rows.append([idx] + t)\ndf_new = pd.DataFrame.from_records(rows)\ndf_new=df_new.set_index(0)\ndf_new.index.rename('date', inplace=True)\ndf_new.columns = mi\ndf_new\n\n\n\n\n\n\n\n\nhumidity\n\n\ntemperature\n\n\n\n\n\n\ntimeat\n\n\nvalue\n\n\ntimeat\n\n\nvalue\n\n\n\n\ndate\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2022-07-13\n\n\n14:30:00\n\n\n49.774510\n\n\n18:00:00\n\n\n33.494737\n\n\n\n\n2022-07-14\n\n\n10:30:00\n\n\n47.358182\n\n\n18:00:00\n\n\n35.562499\n\n\n\n\n2022-07-15\n\n\n10:00:00\n\n\n43.696364\n\n\n16:00:00\n\n\n36.289285\n\n\n\n\n2022-07-16\n\n\n09:00:00\n\n\n51.060715\n\n\n18:00:00\n\n\n36.000000"
  },
  {
    "objectID": "posts/raspberry/coding-the-thermometer.html",
    "href": "posts/raspberry/coding-the-thermometer.html",
    "title": "Coding the Thermometer",
    "section": "",
    "text": "In this post, I will describe the code I have developed to run the thermometer, log the data and save it in such a way that it can be used to carry out some (limited) analytics. At this point, the thermometer has already been installed  \nPart of the code I am showing here was taken from the Ardumotive - How to use the DHT22 I have already referenced. I then expanded on it to add some simple logging.\n\nLibraries\nTo run the code, only three libraries are needed: the Adafruit_DHT library,  time library (for sleep), and the datetime library for better date/time utilities.\nsleep is needed to slow down the program, so that the sensor is able to gather the information (in fact, it cannot be accessed more than once every two seconds).\n# Library for reading from the DHT sensor\nimport Adafruit_DHT as dht\nfrom time import sleep\nfrom datetime import datetime as dt\n\n\nThe main loop\nThe main loop of the progam features a while True loop, which will let the program run until it gets interrupted by the user. As soon as the program runs, the log file is opened in append mode, so that it is possible to add more lines without deleting previous readings (this is pretty useful when debugging!). The value of DHT_data is set to be the pin that is connected to the DATA output of the DHT22 sensor.\nNext, the while loop is executed. In each iteration of the loop, the timestamp of the iteration is saved in now, then the sensor is read. The values of temperature and humidity are printed on screen, then put together with the timestamp in variable data_line and added to the log file.\nThe loop is wrapped in a try-except block, so that it is possible to interrupt it “gracefully” using Ctrl+C. When this happens, the log file is closed and the program terminates.\n#Set DATA pin (this is the pin I will be connecting the DATA output of the chip to)\nDHT_data = 4 \n\n# Open the log file in append mode, so that previous readings will not be deleted \n# when the program restarts.\nfw = open('log.txt', 'a')\n\ntry: \n    # Run forever, until the program is killed through Ctrl+C\n    while True:\n        # Getting timestamp for the current measurement\n        now = dt.now()\n        # Reading Humidity and Temperature from DHT22\n        h,t = dht.read_retry(dht.DHT22, DHT_data)\n\n        # Print Temperature and Humidity on Shell window\n        print('Time={0}\\tTemp={1:0.1f}*C\\tHumidity={2:0.1f}%'.format(now, t, h))\n\n        # Preparing the datetime object for logging, splitting date and time for convenience.\n        this_date = now.date()\n        this_time = now.time()\n        # Writing the data in CSV format.\n        data_line = \"{},{},{},{}\\n\".format(this_date, this_time, t,h)\n        fw.write(data_line)\n\n        sleep(30) #Wait 30 seconds and read again\n\nexcept KeyboardInterrupt:\n    fw.close()\n    print('Ending the program.')\nWhen all is said and done, this is a possible output of the program:\nTime=2022-07-12 00:57:15.430657 Temp=25.0*C     Humidity=58.2%\nTime=2022-07-12 00:57:56.118202 Temp=25.0*C     Humidity=58.3%\nTime=2022-07-12 00:58:29.214395 Temp=25.0*C     Humidity=58.3%\nTime=2022-07-12 00:58:59.776892 Temp=25.0*C     Humidity=58.5%\nTime=2022-07-12 00:59:30.340131 Temp=25.1*C     Humidity=58.5%\nTime=2022-07-12 01:00:13.552538 Temp=25.1*C     Humidity=58.3%\nTime=2022-07-12 01:00:46.646864 Temp=25.1*C     Humidity=58.4%\nTime=2022-07-12 01:01:17.210069 Temp=25.1*C     Humidity=58.4%\nTime=2022-07-12 01:01:52.816956 Temp=25.1*C     Humidity=58.5%\nNote that there is a bug in the line\nprint('Time={0}\\tTemp={1:0.1f}*C\\tHumidity={2:0.1f}%'.format(now, t, h))\nwhich is caused by the temperature sensor returning invalid data: this raises an exception in the print function and interrupts the program. I was not able to reproduce it (after admittedly not a lot of testing) to add a fix, but a simple solution to the problem is simply deleting the offending line. At least, until I figure out a fix.\nSo there we go! This is how it is possible to code a simple log of temperature and humidity by using a Raspberry Pi and a DHT22 sensor. In a future post, I will be play around with some data visualisation and plot the temperature and humidity over time.\nThanks for reading!"
  },
  {
    "objectID": "posts/about-me/pubs.html",
    "href": "posts/about-me/pubs.html",
    "title": "Publications",
    "section": "",
    "text": "Here are my main publications:\n\nRiccardo Cappuzzo, Gael Varoquaux, Aimee Coelho, Paolo Papotti: Retrieve, Merge, Predict: Augmenting Tables with Data Lakes Pre-print, submitted at VLDB2024, PDF\nRiccardo Cappuzzo, Paolo Papotti, and Saravanan Thirumuruganathan: Relational Data Imputation with Graph Neural Networks EDBT2024, PDF\nRiccardo Cappuzzo, Paolo Papotti, and Saravanan Thirumuruganathan: Creating Embeddings of Heterogeneous Relational Datasets for Data Integration Tasks. SIGMOD 2020, PDF\nRiccardo Cappuzzo, Paolo Papotti, Saravanan Thirumuruganathan: EmbDI: Generating Embeddings for Relational Data Integration (Discussion Paper). SEBD 2021 PDF\nRiccardo Cappuzzo, under the supervision of Paolo Papotti, Elena Baralis: Clustering of Categorical Data for Anonymization and Anomaly Detection. Master Thesis at Politecnico di Torino PDF\nRiccardo Cappuzzo, under the supervision of Paolo Papotti: Deep Learning Models for Tabular Data Curation. PhD Thesis at EURECOM/Sorbonne University PDF"
  },
  {
    "objectID": "posts/news/moving-up.html",
    "href": "posts/news/moving-up.html",
    "title": "Moving up… or sideways?",
    "section": "",
    "text": "So, my postdoc is about to end (just a few more weeks left), and it seems like my next position has been decided! I will be a research engineer at P16, a development unit within Inria where I will be working on the Skrub Python library.\nIt’s a short term contract (one year), which means that in a few months I’ll have to go back to looking for a new job. For the time being, however, I’ll enjoy spending more time at Inria.\nI’ll miss Dataiku though, it’s a great company and I met some really nice people there, despite the fact I was there only for one or two days a week."
  },
  {
    "objectID": "posts/news/soda-kickoff.html",
    "href": "posts/news/soda-kickoff.html",
    "title": "SODA Kickoff: a new slide deck for skrub",
    "section": "",
    "text": "Today we had the SODA Kickoff, an event organized by some of the PIs (I still don’t know what “PI” stands for, other than that it’s not “private investigator”) of the Inria team I am part of.\nIt was very interesting! I got to hear about what all the other post docs, PhD students, and researcher are working on, and I got to know some people I don’t often talk with.\nAs part of the event, I presented skrub (not too unexpected), and had to prepare a new slide deck to showcase the latest updates to the library. It was very well received, and a lot of people complimented me for the presentation, which is always great :D\nHere it is, together with the rest of the materials on our teaching website."
  },
  {
    "objectID": "posts/news/end-of-2024.html",
    "href": "posts/news/end-of-2024.html",
    "title": "2024 wrapped: ups, downs, and some changes",
    "section": "",
    "text": "It’s the end of the year, so I decided I had to do my semestral wrestling bout with Hugo to go over what happened in the past 12 months.\nTo start with, as I haven’t updated the website in months, I have been welcomed by an error message, which further complicates my relationship with this platform. Seriously though, every time I try to write something here I spend more time debugging Hugo than I do writing.\nOverall, most of the year was devoted to “the paper”, that one article I have been working on for the past two years. Said paper has been rejected three times (so far), and we are still working on it. Supposedly, we should be done with it this January, but who knows what the future actually holds.\nWhat else? Well, my contract as a post-doc ended, and now I am working as a research engineer at Inria, working on Skrub. Overall, I am interacting with a lot of the same people while working on a library that I find interesting and that still lets me run experiments.\nI should start looking for a new position soon enough, but first I’d like to get the paper done with.\nI went to PyData Paris 2024, which was a very interesting experience (though quite tiring, as I went there in the same period I was moving to a different place), meeting a lot of brilliant people in the field I would like to keep on working in.\nI would say that the year has been very stressful, with burnout waiting for me after pretty much every paper deadline, and some extremely disappointing experience in Academia. On the other hand, it was also a very instructive year as I feel like I learned a lot about data science, machine learning, note taking, proper software development, and some more. So, swings and roundabouts?\nIf I have to come up with a theme for 2025, I think it will be something along the lines of “read more sources”. I have been saving all sorts of scholarly articles, blog posts, guides, essays, books, and proceeded to read maybe 1% of the collection. My hope is that by the time this day rolls around next year, I’ll be able to look back and think that I was able to read what I save, rather than just hoard articles.\nThat said, see you around in the next update (whenever that may be, and if Hugo allows…). Happy 2025!"
  },
  {
    "objectID": "posts/news/retrieve-merge-predict-short.html",
    "href": "posts/news/retrieve-merge-predict-short.html",
    "title": "New publication: Retrieve, Merge, Predict",
    "section": "",
    "text": "Almost exactly two years in the making, our paper “Retrieve, Merge, Predict - Augmenting tables with data lakes” has finally been published in TMLR: you may find it here.\nI am planning to write a proper blog post to summarize the results, as well as a retrospective on the paper, just to reminisce on the effort it took to finally publish it."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Riccardo Cappuzzo",
    "section": "",
    "text": "Data Science, Machine Learning, Python"
  }
]